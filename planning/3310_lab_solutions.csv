lab_sol_id,lab_group,lab_sol_pub_date,lab_sol_title,lab_sol_author,lab_sol_filename,lab_sol_pdf_url,lab_sol_markdown
101,INTRO_LAB,2018-09-05,Lab #1 Answers,,answers,/files/lab_solutions/lab_01/lab_01_answers.pdf,"```{r setup, include=FALSE}
# This section sets up some options on knitr for processing the RMarkdown file
knitr::knit_hooks$set(inline = function(x) { knitr:::format_sci(x, 'md')})
knitr::opts_chunk$set(echo = TRUE)
```
```{r initialize, include=FALSE}
# This section loads necessary R libraries and sources scripts that define 
# useful functions format_md.
# 
library(tidyverse)
library(stringr)

source(""_scripts/format_md.R"")
```
# Instructions:

* Clone a local copy of the file repository from github.
* In your local repository, answer the exercises in the template
  `Lab_01_Activities.Rmd`.
* The report, which you will be graded on, will be a knitted Word or PDF file
  based on the `Lab_01_Activities.Rmd` template, in which you have added 
  answers to the exercises.
  
    When you knit the documnet, it will carry out the calculations you have
    programmed and it will also show the R code you used to do the calculations.
    Thus, it will automatically show your work and your results.
    
    As we move along in the semester, you will get experience using R to make
    graphs, tables, and other output, which you will be able to use to 
    produce integrated lab reports that show all the steps of your work
    along with the results and your disucssion and illustration of those
    results.
    
    You will turn in your knitted lab report along with the 
    Lab_01_Activities.Rmd file you used to produce it, and the rest of the project,
    by committing your work to the Git repository on your computer and then 
    pushing the commits to your account on GitHub.
* You should frequently knit your file to make sure it runs without errors and
  produces appropriate output. Knitting a file to PDF format can be slow, so I 
  recommend knitting to HTML or Word format while you're working on your project
  and then knitting to PDF when you get to a good stopping point and want to see
  what the finished document will look like.
  
    Please don't wait until the last minute, just before you submit your project,
    before knitting because a big piece of your grade is producing a finished 
    knitted document and if you wait too late and then can't knit because 
    there are errors in your file, you will be unhappy.
    
    One nice thing about Git is that if you commit your edits frequently to your
    repository, then if things are going nicely and your document is knitting,
    and then you do something new and it breaks, you will be able to review your
    changes and figure out what you did that broke your document, and you will
    always be able to go back to the earlier versions that knit properly.
* Use git to commit your changes (including the edits to `Lab_01_Activities.Rmd`
  and the new PDF file) to your local git repository.
* Push the changes from your local git repository to github.

    The last changes that you push before the due date (Monday 
    Sept. 3 at 9:00 am) will be graded.

I advise using git to commit changes frequently as you work and push those commits
to GitHub so that if something happens to your computer, your work will be saved in the
cloud.



# Exercises from Chapter 2

**_I have worked Exercise 2.1 as an example to show you how you can use 
RMarkdown to solve problems and write up the answers. 
You will solve the other exercises for the lab using the same methods._**

## Exercise 2.1 (worked example)


Consider exercise 1 in Chapter 2, on p. 17 of our textbook, 
_Global Warming: Understanding the Forecast_:

> A joule (J) is an amount of energy, and a watt (W) is a rate of using energy, 
> defined as 1 W = 1 J/s. How many Joules of energy are required to run a 100-W 
> light bulb for one day?

```{r ex_2_1_a}
seconds_per_hour = 60 * 60 # number of seconds in one hour
hours_per_day = 24 # number of seconds in one day
light_bulb_power = 100
joules_per_day = light_bulb_power * seconds_per_hour * hours_per_day

joules_per_day
```

It can be hard to read a long number like that, so we can use R's format command to add commas:

```{r formatting_example}
format_md(joules_per_day, comma = TRUE)
```

Or if you want to express that in scientific notation, you can do this:
```{r scientific_formatting_example, results=""asis""}
format_md(joules_per_day, digits = 3, format = ""scientific"")
```

**Answer:** A `r light_bulb_power` watt light bulb burns `r light_bulb_power` Joules per second, 
so the light bulb burns `r light_bulb_power * seconds_per_hour` Joules per hour and
`r joules_per_day` Joules per day.

> Burning coal yields about 
> `r format_md(30.E+6, digits = 1, format = ""engineering"")` J of energy per 
> kilogram of coal burned. 
> Assuming that the coal power plant is 30% efficient,
> how much coal has to be burned to light that light bulb for one day?

```{r ex_2_1_b}
joules_heat_per_kg_coal = 30.E+6
efficiency = 0.30
joules_electricity_per_kg_coal = joules_heat_per_kg_coal * efficiency
kg_coal_per_joule_electricity = 1 / joules_electricity_per_kg_coal
kg_coal_per_day = joules_per_day * kg_coal_per_joule_electricity

kg_coal_per_day
```

**Answer:** 1 kilogram of coal produces `r format_md(joules_heat_per_kg_coal, digits = 1, comma = TRUE)` joules of heat, which is converted into `r format_md(joules_electricity_per_kg_coal, digits = 1, comma = TRUE)` joules of electricity.
We can turn this around and figure that to get 1 joule of electricity takes 1 / (`r format_md(joules_electricity_per_kg_coal, digits = 1, comma = TRUE)`) = `r format_md(kg_coal_per_joule_electricity, digits = 1, format = ""scientific"")` kg of coal,
so to get `r format_md(joules_per_day, digits = 1, comma = TRUE)` joules of electricity to power the light bulb for one day takes `r format_md(kg_coal_per_day, digits = 2, comma = TRUE)` kg of coal.

## Exercise 2.2

> A gallon of gasoline carries with it about 1.3 &times; 10^8^ J of energy. 
> Given a price of $3 per callon, how many Joules can you get for a dollar?

```{r ex_2_2_a}
gasoline_energy = 1.3E8 # Joules per gallon
gasoline_price = 3.0 # dollars per gallon
gasoline_joules_per_dollar = gasoline_energy / gasoline_price
```

**Answer:** You can get 
`r format_md(gasoline_joules_per_dollar, digits = 2, format = ""scientific"")` Joules 
of gasoline for a dollar.

> Electricity goes for about $0.05 per kilowatt hour. A kilowatt hour is just a 
> weird way to write Joules because a watt is a joule per second, and a kilowatt 
> hour is the number of Joules one would get from running 1000 W time one hour
> (3,600 seconds). In the form of electricity, how many Joules can you get for a 
> dollar?

```{r ex_2_2_b}
kwh_price = 0.05
joules_per_kwh = 1000 * 3600
electricity_joules_per_dollar = joules_per_kwh / kwh_price
```

**Answer:** One kilowatt hour is `r format_md(joules_per_kwh, digits = 2, format = ""scientific"")` Joules.
At a price of `r format_md(kwh_price, digits = 1)` dollars per kwh, you can buy 
`r format_md(electricity_joules_per_dollar, digits = 2, format=""scientific"")` Joules of electricity for a dollar.

> A standard cubic foot of natural gas carries with it about 1.1 &times; 10^6^ Joules
> of energy. You can get about 5 &times; 10^5^ British Thermal Units (BTUs) of gas
> for a dollar, and there are about 1,030 BTUs in a standard cubic foot. 
> How many Joules of energy in the form of natural gas can you get for a dollar?

**Answer:**
This is a litle complicated. Let's start by entering the information we have:

```{r ex_2_2_c_info}
joules_per_scf = 1.1E6   # Joules per scf
btu_per_dollar = 5E5 # BTU per dollar
btu_per_scf = 1030   # BTU per scf
```

We know how many BTUs we can buy for a dollar, and we also know how many 
BTUs are in an SCF, so we can calculate the number of SCF we can buy with a
dollar:
```{r ex_2_2_c_calc}
scf_per_dollar = btu_per_dollar / btu_per_scf
```
We can buy `r format_md(scf_per_dollar, digits = 2)` standard cubic feet for a
dollar.
Now we can calculate the number of Joules we can get for a dollar:
 ```{r ex_2_2_c_calc_2}
natural_gas_joules_per_dollar = joules_per_scf * scf_per_dollar
 ```
 
 You can buy 
 `r format_md(natural_gas_joules_per_dollar, digits = 2, format=""scientific"")` Joules 
 of natural gas for a dollar.

> A ton of coal holds about 3.2 &times; 10^10^ J of energy and costs about $40.
> How many Joules of energy in the form of coal can you get for a dollar?

```{r ex_2_2_d}
coal_joules_per_ton = 3.2E10
coal_cost_per_ton = 40
coal_joules_per_dollar = coal_joules_per_ton / coal_cost_per_ton
```

**Answer:** You can get `r format_md(coal_joules_per_dollar, digits = 2, format=""scientific"")` Joules
of coal for a dollar.

> Corn oil costs about $0.10 per fluid ounce wholesale. A fluid ounce carries
> about 240 dietary Calories (which a scientist would call kilocalories).
> A dietary Calorie is about 4200 J. How many Joules of energy in the form of
> corn oil can you get for a dollar?

```{r ex_2_2_e}
corn_oil_price_per_ounce = 0.10
corn_oil_calorie_per_ounce = 240
joules_per_calorie = 4200
corn_oil_joules_per_ounce = joules_per_calorie * corn_oil_calorie_per_ounce
corn_oil_joules_per_dollar = corn_oil_joules_per_ounce / corn_oil_price_per_ounce
```

**Answer:** You can get 
`r format_md(corn_oil_joules_per_dollar, digits = 2, format=""scientific"")` Joules 
of corn oil for a dollar.

> Rank these five energy sources from cheap to expensive. 
> What is the range of prices?

**Answer:** 

1. Coal is the cheapest (most Joules per dollar) at 
   `r format_md(coal_joules_per_dollar, digits = 1, format = ""scientific"")`
   Joules per dollar.
2. Natural gas is the second cheapest at 
   `r format_md(natural_gas_joules_per_dollar, digits = 1, format = ""scientific"")`
   Joules per dollar.
5. Electricity is the third cheapest at 
   `r format_md(electricity_joules_per_dollar, digits = 1, format = ""scientific"")`
   Joules per dollar.
3. Gasoline is the fourth cheapest at 
   `r format_md(gasoline_joules_per_dollar, digits = 1, format = ""scientific"")`
   Joules per dollar.
4. Corn oil is the most expensive at 
   `r format_md(corn_oil_joules_per_dollar, digits = 1, format = ""scientific"")`
   Joules per dollar.


## Exercise 2.3 (Grad students only)

> This is one of those job-interview questions to see how creative you are, 
> analogous to one I heard: ""How many airplanes are over Chicago at any given time?"" 
> You need to make stuff up to get an estimate and demonstrate your management 
> potential. 
> The question is: _What is the efficiency of energy production from growing corn?_
> 
> Assume that sunlight deposits 250 W/m^2^ of energy on a corn field, averaging 
> over the day-night cycle. 
> There are approximately 4,200 J per dietary Calorie. 
> How many Calories of energy are deposited on a square
> meter of field over the growing season? 
>
> (Note: the word ""calorie"" has two different meanings. Physicists and chemists, 
> use ""calorie"" (with a lower-case ""c"") to refer to a thermodynamic unit of heat, 
> but nutritionists use the word Calorie (with a capital 'C')
> to mean 1 kilocalorie (1000 thermodynamic calories), so when you see ""Calories"" 
> on a food label, it means kilocalories. To keep this exercise simple, I have 
> edited the textbook version so we only need to think in terms of dietary Calories.)


**Answer:**

Let's estimate a growing season at about 3 months
(actual time depends on the kind of corn: sweet corn for corn on the cob takes
2-3 months to grow, and corn for cattle feed or making corn meal takes closer
to 4 months, so 3 months is kind of in the middle).

```{r ex_2_3_a}
I_sun = 250 # Watts per square meter
joules_per_calorie = 4200 # Joules
growing_season = 90 # days
# Convert days to seconds: 24 hours per day, 3600 seconds per hour
growing_season_seconds = growing_season * 24 * 3600

joules_per_season = I_sun * growing_season_seconds
```

The average growing season is about `r growing_season` days, or 
`r format_md(growing_season_seconds, digits = 2, format=""scientific"")` seconds.
During that time, an average of `I_sun` W/m^2^ of sunlight is deposited on the 
ground, so a square meter of field receives a total of 
`r format_md(joules_per_season, digits = 2, format = ""scientific"")` Joules.

> Now guess how many ears of corn grow per square
> meter, and guess what the number of dietary Calories is that you get for eating 
> an ear of corn. Compare the sunlight energy with the corn energy to get the 
> efficiency.

**Answer:**
There is no single right answer here. There are different approaches, and what you get out depends on the assumptions and approximations you make. The basic approach is to figure out how many dietary Calories are in an ear of corn, how many ears of corn grow on a plant, and how much area the leaves of the plant constitute. 

```{r corn_data, include=TRUE}
corn_leaf_area = 5500 / 100^2 # Square meters of leaf area per plant
corn_ear_calories <- 80
corn_ear_joules <- corn_ear_calories * joules_per_calorie
```

When I (Prof. Gilligan) first went to do this problem, I googled dietary 
information about corn, and found that the Department of Agriculture estimates 
that an ear of corn has `r round(corn_ear_calories)` Calories. (<http://www.fns.usda.gov/fdd/facts/hhpfacts/New_HHPFacts/Veges/HHFS_CORN_FRESH_F210_Final.pdf>). 
The University of Iowa Agronomy Extension reports that a typical corn plant produces one ear (<http://www.agronext.iastate.edu/corn/corn-qna.html>) 
and Utah State University reports one to two ears per plant 
(<http://extension.usu.edu/htm/faq/faq_q=96>). 
A paper from the journal Agricultural and Forest Meteorology reports that the 
total leaf area for a corn plant is typically between 
5000 and 6000 cm^2^ (0.5--0.6 square meters) 
(<http://www.sciencedirect.com/science/article/pii/S0168192309000410>)

Putting these data together, I calculated the energy absorbed by the leaves of
the corn plant and used that to calculate the efficiency with which corn 
converts sunlight to food calories:

```{r corn-efficiency, include=TRUE}
corn_leaf_energy = corn_leaf_area * joules_per_season
corn_efficiency = corn_ear_joules / corn_leaf_energy
```

This gives me an estimated efficiency of `r format_md(corn_efficiency, digits = 2)`,
or `r format_md(corn_efficiency * 100, digits = 2)` percent.

On the other hand, David Archer's solution manual for our textbook makes different
assumptions. Archer assumes that an ear of corn has 500~dietary calories, 
that each plant produces an average of 4~ears, and that there are 
400 cm^2^ of leaves per plant, which would make the plant 2% efficient.  

You can see that Archer and I come up with answers that are different by a 
factor of 100, so this question is not looking for a precise answer. 

To do a reality check, I looked up the efficiency of photosynthesis and found 
that typical plants are between 0.2 and 2.0% efficient at photosynthesis. 
If we consider that lots of the energy a corn plant takes from the sun goes 
into making leaves, stems, and roots, not just into producing the edible corn 
kernels on the ear, I think my estimates are closer to the mark than Archer's, 
but the point of this exercise is not to be exactly right, but to get a feel 
for making reasonable approximations to solve in real-world problems.

Whether Archer is correct or I am, either way a corn plant is much less 
efficient than a typical automobile (around 15% efficient at converting 
gasoline to forward motion) or an electrical generation plant 
(typically 30--45% efficient at converting coal or natural gas energy to
electricity). 
Photovoltaic solar electricity is typically 15--20% efficient, which makes it 
between 10 and 400 times more efficient than photosynthesis, depending on 
whether you take my estimated efficiency or Archer's for photosynthesis.

## Exercise 2.4

> The Hoover Dam produces $2 \times 10^{9}$ W of electricity. It is composed of 
> $7 \times 10^{9}$ kg of concrete.
> Concrete requires 1 MJ of energy (1 megajoule, 1,000,000 Joules) to produce 
> per kilogram. 
> How much energy did it take to produce the dam? How long is the ""energy payback 
> time"" for the dam?

**Answer:** 
```{r ex_2_4_a}
concrete_energy = 1E6 # joules
dam_concrete = 7E9 # kg concrete
dam_concrete_energy = concrete_energy * dam_concrete
```
I took `r format_md(dam_concrete_energy, digits=2, format=""scientific"")` Joules
to make the concrete for the Hoover dam.
```{r ex_2_4_a_payback}
hoover_output = 2E9 # watts
payback_seconds = dam_concrete_energy / hoover_output
payback_days = payback_seconds / (24 * 3600)
payback_years = payback_days / (265.25) # account for leap years
```

It would take approximately 
`r format_md(payback_seconds, digits=1, format=""scientific"")` seconds, 
or `r round(payback_days)` days for the Hoover
Dam to generate as much electricity as it took to build it.

> The area of Lake Mead, formed by Hoover Dam, is 247 mi^2^. Assuming 250 W/m^2^ 
> of sunlight falls on Lake Mead, how much energy could you produce if instead of 
> the lake you installed solar cells that were 12% efficient? 
> (1 mile is 1609 meters; how many square meters are in a square mile?)

**Answer:** 
```{r ex_2_4_b}
I_sun = 250
efficiency = 0.12
lake_mead_area_sq_miles = 247
sq_meters_per_sq_mile = 1609^2
lake_mead_area = lake_mead_area_sq_miles * sq_meters_per_sq_mile # square meters
lake_mead_solar_power = I_sun * efficiency * lake_mead_area
```
The area of Lake Mead is 
`r format_md(lake_mead_area, digits=2, format=""scientific"")` square meters.
If 12% of the sunlight falling on this area were converted to electricity, 
it would produce 
`r format_md(lake_mead_solar_power, digits=1, format=""scientific"")` Watts,
which would be about `r round(lake_mead_solar_power / hoover_output)`
times as great as the output of the Hoover Dam.

## Exercise 2.5

> It takes approximately $2 \times 10^{9}$ J of energy to manufacture 1 m^2^ of 
> crystalline-silicon photovoltaic cell. (Actually, the number quoted was 
> 600 kilowatt hours. Can you figure out how to convert kilowatt hours into 
> Joules?) Assume that the solar cell is 12% efficient, and calculate how long it
> would take, given 250 W/m^2^ of sunlight, for the solar cell to repay the energy 
> it cost for its manufacture.

**Answer:** put your answer here ...
```{r ex_2_5}
mfg_energy_kwh = 600 # Kilowatt hours
joules_per_kwh = 1000 * 3600
mfg_energy = 2E9 # Joules
efficiency = 0.12
I_sun = 250

mfg_energy_check = mfg_energy_kwh * joules_per_kwh

solar_power = I_sun * efficiency
payback_seconds = mfg_energy / solar_power
payback_days = payback_seconds / (24 * 3600)
payback_years = payback_days / 365.25
lifetime = 10 # years
lifetime_seconds = lifetime * 365.25 * 24 * 3600 # seconds per year
lifetime_output = lifetime_seconds * solar_power
```
One kwh is `r format_md(joules_per_kwh, digits=1, format=""scientific"")` Joules
(1000 Watts &times; 3600 seconds/hour),
so 600 kWh  = `r format_md(mfg_energy_check, digits=1, format=""scientific"")` Joules.

At 12% efficiency, 1 m^2^ of solar panel would produce 
`r format_md(solar_power, digits = 3)` Watts, so it would take
`r format_md(payback_seconds, digits=2, format=""scientific"")` seconds, or
`r round(payback_years, digits=1)` years for the panel to produce as much energy
as it took to make it.

Since the average solar panel has a useful lifetime of about 10 years, it would
generate around `r round(lifetime_output / mfg_energy)` times as
much energy during its lifetime as it took to manufacture it.


## Exercise 2.7

> Infrared light has a wavelength of about 10 $\mu$m. What is its wave number in
> cm^-1^?

**Answer:** 
```{r ex_2_7_a}
wavelength = 10E-6 # meters
centimeter = 0.01 # meter
wavenumbers = centimeter / wavelength
```
10 $\mu$m radiation would have a wavenumber of 
`r format_md(wavenumbers, digits=1, format=""scientific"")` cm^-1^.

> Visible light has a wavelength of about 0.5 $\mu$m. What is its frequency in 
> Hz (cycles per second)?

**Answer:** 
```{r ex_2_7_b}
speed_of_light = 3E8 # meters per second
wavelength = 0.5E-6
frequency = speed_of_light / wavelength
```
The frequency of visible light is about 
`r format_md(frequency, digits=1, format=""scientific"")` Hz.

> FM radio operates at a frequency of about 40 kHz. What is its wavelength?

**Answer:** 
```{r ex_2_7_c}
frequency = 40E3
wavelength = speed_of_light / frequency
```
The wavelength of FM radio waves would be about 
`r format_md(wavelength, digits=1, format=""scientific"")` meters,
or `r format_md(wavelength / 1000, digits=2)` kilometers.

"
201,DATA_LAB,2018-09-15,Lab #2 Answers,,answers,/files/lab_solutions/lab_02/lab_02_answers.pdf,"```{r setup, include=FALSE}
knitr::knit_hooks$set(inline = function(x) { knitr:::format_sci(x, ""md"")})
knitr::opts_chunk$set(echo = TRUE)

# This section loads necessary R libraries and sources scripts that define 
# useful functions format_md.
# 
data_dir = ""_data""
script_dir = ""_scripts""

library(pacman)
p_load(zoo, xml2, tidyverse, stringr)

theme_set(theme_bw(base_size = 15))

source(file.path(script_dir, ""utils.R""), chdir = T)
source(file.path(script_dir, ""modtran.R""), chdir = T)

mlo_url = paste0(""http://scrippsco2.ucsd.edu/assets/data/atmospheric/stations/"",
                ""in_situ_co2/monthly/monthly_in_situ_co2_mlo.csv"")

giss_url = ""https://data.giss.nasa.gov/gistemp/tabledata_v3/GLB.Ts+dSST.csv""

# Create a data directory if one does not exist.
if (!dir.exists(data_dir)) dir.create(data_dir)
```
# Worked Example

## Downloading CO~2~ Data from Mauna Loa Observatory

In 1957, Charles David Keeling established a permanent observatory on Mauna Loa,
Hawaii to make continuous measurements of atmospheric carbon dioxide. The 
observatory has been running ever since, and has the longest record of direct 
measurements of atmospheric carbon dioxide levels. The location was chosen 
because the winds blow over thousands of miles of open ocean before reaching
Mauna Loa, and this means the CO~2~ measurements are very pure and uncontaminated
by any local sources of pollution.

We can download the data from <`r mlo_url`>. We can download the file and save 
it to the local computer using the R function `download.file`

```{r download_mlo_data, include=TRUE, message=FALSE}
mlo_url = ""http://scrippsco2.ucsd.edu/assets/data/atmospheric/stations/in_situ_co2/monthly/monthly_in_situ_co2_mlo.csv""
download.file(mlo_url, file.path(data_dir, ""mlo_data.csv""))
```

Try opening the data file in Excel or a text editor.

The first 54 lines of the data file are comments describing the data. 
These comments describe the contents of each column of data
and explain that this data file uses the special value `-99.99` to
indicate a missing value. The Mauna Loa Observatory started recording data
in March 1958, so the monthly averages for January and February are missing.
Other months are missing for some months in the record when the instruments 
that monitor CO~2~ concentrations were not working properly.

The `read_csv` function from the `tidyverse` package can read the data into R 
and convert it into a `tibble` data structure (like a fancy data table).
However, when R reads in `.csv` files, it expects column names to be on a 
single row, 
and lines 55--57 of the data file are column headings that are split across 
multiple rows, so R will get confused if we tell it to use those rows as column 
names.

Thus, we will tell `read_csv` to read this data file, but skip the first 57 lines.
We will also tell it not to look for column names in the data file, so we will
supply the column names, and we will tell it that whenever it sees `-99.99`,
it should interpret that as indicating a missing value, rather than a measurement.

Finally, R can guess what kinds of data each column contains, but for this file,
things work a bit more smoothly if we provide this information explicitly.

`read_csv` lets us specify the data type for each column by providing a string
with one letter for each column. The letters are `i` for integer numbers,
`d` for real numbers (i.e., numbers with a decimal point and 
fractional parts), `n` for an unspecified number, 
`c` for character (text) data, `l` for logical (`TRUE` or `FALSE`), 
`D` for calendar dates, `t` for time of day, and `T` for combined date and time.


```{r import_mlo_data, include=TRUE, message=FALSE}
mlo_data = read_csv(file.path(data_dir, ""mlo_data.csv""), 
                    skip = 57,  # skip the first 57 rows
                    col_names = c(""year"", ""month"", ""date.excel"", ""date"",
                                   ""co2.raw"", ""co2.raw.seas"", 
                                   ""co2.fit"", ""co2.fit.seas"",
                                   ""co2.filled"", ""co2.filled.seas""),
                     col_types = ""iiiddddddd"", # the first three columns are integers
                                               # and the next 7 are real numbers
                     na = ""-99.99"" # interpret -99.99 as a missing value
                    )
```

Let's look at the first few rows of the data:

Here is how it looks in R:

```{r show_data_r, include = TRUE}
head(mlo_data)
```

And here is how we can use the `kable` function to format the data
nicely as a table in an RMarkdown document:

```{r show_data, include=TRUE, results = ""asis""}
head(mlo_data) %>% knitr::kable()
```

There are six different columns for the CO~2~ measurements: 

* `co2.raw` is the 
  raw measurement from the instrument. The measurements began in March 1958, so
  there are `NA` values for January and February. In addition, there are missing
  values for some months when the instrument was not working well.

* `co2.fit` is a smoothed version of the data, which we will not use in this lab.

* `co2.filled` is the same as `co2.raw`, except that where there are missing
  values in the middle of the data, they have been filled in with interpolated
  estimates based on measurements before and after the gap.

For each of these three data series, there is also a _seasonally adjusted_
version, which attempts to remove the effects of seasonal variation in order
to make it easier to observe the trends.

For this lab, we will focus on the `co2.filled` data series. To keep things simple,
we can use the `select` function from `tidyverse` to keep only certain columns
in the tibble and get rid of the ones we don't want.

```{r simplify_mlo_data, include = TRUE}
mlo_simple = mlo_data %>% select(year, month, date, co2 = co2.filled)

head(mlo_simple)
```

Note how we renamed the `co2.filled` column to just plain `co2` in the select
function.

Now, let's plot this data:

```{r plot_mlo, include = TRUE}
ggplot(mlo_simple, 
       aes(x = date, y = co2)) + # This line specifies the data to plot and
                                 # the aesthetics that define which variables to 
                                 # use for the x and y axes
  geom_line() +   # This line says to plot lines between the points
  labs(x = ""Year"", y = ""CO2 concentration (ppm)"",
       title = ""Measured CO2 from Mauna Loa Observatory"") # This line gives the 
                                                          # names of the axes
  # Earlier in this .Rmd file, I called set_theme(theme_bw(base_size = 15)) to 
  # set the default plot style. If you call ggplot() without this,
  # you will get a different style, but you can either call theme_set
  # or you can add a theme specification (such as ""+ theme_bw(base_size = 15)"")
  # end of the sequence of plotting commands in order to apply a specific style
  # to an individual plot.
```

Notice the seasonal variation in CO~2~. Every year, there is a large cycle of
CO~2~, but underneath is a gradual and steady increase from year to year.
If we wanted to look at the trend without the seasonal variation, we could use
the `co2.filled.seas` column of the original tibble, but instead, let's look at
how we might estimate this ourselves.

The seasonal cycle is 12 months long and it repeats every year. This means that
if we average the values in our table over a whole year, this cycle should average
out. We can do this by creating a new column `trend` where every row represents
the average over a year centered at that row (technically, all the months from
5 months before through six months after that date):

```{r plot_mlo_trend, include = TRUE, warning=FALSE}
mlo_simple %>% mutate(trend = rollapply(data = co2, width = 12, FUN = mean,
                                      fill = NA, align = ""center"")) %>%
  ggplot(aes(x = date)) + 
  geom_line(aes(y = co2), color = ""dark blue"") +
  geom_line(aes(y = trend), color = ""black"", size = 2) +
  labs(x = ""Year"", y = ""CO2 concentration (ppm)"", 
       title = ""Measured and Seasonally Adjusted CO2"")
```

But wait: we might want a legend to tell the reader what each colored line 
represents. We can create new aesthetics for the graph mapping to do this:

```{r plot_mlo_trend_2, include = TRUE, warning=FALSE}
mlo_simple %>% mutate(trend = rollapply(data = co2, width = 12, FUN = mean,
                                      fill = NA, align = ""center"")) %>%
  ggplot(aes(x = date)) + 
  geom_line(aes(y = co2, color = ""Raw"")) +
  geom_line(aes(y = trend, color = ""12-month average""), size = 2) +
  scale_color_manual(values = c(""Raw"" = ""dark blue"", ""12-month average"" = ""black""),
                     name = ""Smoothing"") +
  labs(x = ""Year"", y = ""CO2 concentration (ppm)"", 
       title = ""Measured and Seasonally Adjusted CO2"")
```

We can also anlyze this data to estimate the average trend in CO~2~.
We use the `lm` function in R to fit a straight line to the data,
and we use the `tidy` function from the `broom` package to
print the results of the fit nicely.

R has many powerful functions to fit data, but here we will just use a very 
simple one. We specify the linear relationship to fit using R's formula
language. If we want to tell R that we think `co2` is related to `date`
by the linear relationship $co2 = a + b \times \text{date}$, then we write
the formula `co2 ~ date`. The intercept is implicit, so we don't have to spell
it out.

```{r calc_mlo_trend, include = TRUE}
trend = lm(co2 ~ date, data = mlo_simple)

library(broom)

tidy(trend)
```

This shows us that the trend is for CO~2~ to rise by 
`r round(summary(trend)$coefficients[""date"",""Estimate""],2)` ppm per year, 
with an uncertainty of plus or minus
`r signif(summary(trend)$coefficients[""date"",""Std. Error""], 1)`.

We can also plot a linear trend together with the data:

```{r plot_mlo_with_fitted_trend, include = TRUE, warning=FALSE}
mlo_simple %>% mutate(trend = rollapply(data = co2, width = 12, FUN = mean,
                                      fill = NA, align = ""center"")) %>%
  ggplot(aes(x = date, y = co2)) + 
  geom_line() +
  geom_smooth(method = ""lm"") +
  labs(x = ""Year"", y = ""CO2 concentration (ppm)"", 
       title = ""Measured CO2 and Linear Fit"")
```

# Exercises

## Exercises with CO~2~ Data from the Mauna Loa Observatory

Using the `select` function, make a new data tibble called `mlo_seas`, from 
the original `mlo_data`, which only has two columns: `date` and 
`co2.seas`, where `co2.seas` is a renamed version of `co2.filled.seas` from the 
original tibble.

```{r make_mlo_seas, include=TRUE}
# We only need to load the libraries once and they will be loaded for all 
# subsequent code chunks
library(tidyverse)
library(zoo)

mlo_seas = select(mlo_data, date, co2.filled.seas)
mlo_seas = rename(mlo_seas, co2.seas = co2.filled.seas)

# Alternately, you can simplify with the pipe operator:
# 
# mlo_seas = select(mlo_data, date, co2.filled.seas) %>% rename(co2.seas = co2.filled.seas)
# 
# or you can rename as part of the select operation:
# 
# mlo_seas = select(mlo_data, date, co2.seas = co2.filled.seas)

# Display the first few rows:
head(mlo_seas)
```

Now plot this with `co2.seas` on the _y_ axis and `date` on the _x_ axis,
and a linear fit:

```{r plot_mlo_seas, include = TRUE}
ggplot(mlo_seas, aes(x = date, y = co2.seas)) +
  geom_line() +
  geom_smooth(method=""lm"") +
  labs(x = ""Year"", y = ""CO2 concentration (ppm)"")
```

Now fit a linear function to find the annual trend of `co2.seas`. Save
the results of your fit in a variable called `trend.seas`.

```{r fit_mlo_sease, include=TRUE}
trend.seas = lm(co2.seas ~ date, data = mlo_seas)

tidy(trend.seas)
```

Compare the trend you fit to the raw `co2.filled` data to the trend you fit
to the seasonally adjusted data.

**Note:** I just intend students to informally look at the trend in the graph 
and estimate its slope by eye to compare to the results in `trend.seas`.

## Exercises with Global Temperature Data from NASA

We can also download a data set from NASA's Goddard Institute for Space Studies
(GISS), which contains the average global temperature from 1880 through the 
present.

The URL for the data file is 
<`r giss_url`>

Download this file and save it in the directory `_data/global_temp_land_sea.csv`.

```{r download_giss_temp, include=TRUE}
giss_url = ""https://data.giss.nasa.gov/gistemp/tabledata_v3/GLB.Ts+dSST.csv""

download.file(giss_url, file.path(data_dir, ""global_temp_land_sea.csv""))
```

* Open the file in Excel or a text editor and look at it.

* Unlike the CO~2~ data file, this one has a single line with the 
data column names, so you can specify `col_names=TRUE` in `read_csv`
instead of having to write the column names manually.

* How many lines do you have to tell `read_csv` to skip?

**Answer:** 1 line: the first line is ""Land-Ocean: Global Means"" and we want to 
skip it.

* `read_csv` can automatically figure out the data types for each column,
  so you don't have to specify `col_types` when you call `read_csv`

* This file uses `***` to indicate missing values instead of `-99.99`, so you
  will need to specify `na=""***""` in `read_csv`.  
  
    For future reference,
    if you have a file that uses multiple different values to indicate missing
    values, you can give a vector of values to `na` in `read_csv`:
    `na = c(""***"",""-99.99"", ""NA"", """")` would tell `read_csv` that if it finds 
    any of the values ""***"", ""-99.99"", ""NA"", or just a blank with nothing in it,
    any of those would correspond to a missing value, and should be indicated by
    `NA` in R.
  
Now read the file into R, using the `read_csv` function, and assign
the resulting tibble to a variable `giss_temp`

```{r read_giss_temp, include=TRUE}
giss_temp = read_csv(file.path(data_dir, ""global_temp_land_sea.csv""), skip = 1, na = ""***"",
                     col_names = TRUE)

# show the first 5 lines of giss_temp
head(giss_temp, 5)
```

Something is funny here: Each row corresponds to a year, but there are columns
for each month, and some extra columns called ""J-D"", ""D-N"", ""DJF"", ""MAM"", ""JJA"",
and ""SON"". These stand for average values for the year from January through 
December, the year from the previous December through November, and the seasonal
averages for Winter (December, January, and February), 
Spring (March, April, and May), Summer (June, July, and August), and Fall 
(September, October, and November).

The temperatures are recorded not as the thermometer reading, but as _anomalies_.
If we want to compare how temperatures are changing in different seasons and at
different parts of the world, raw temperature measurements are hard to work with
because summer is hotter than winter and Texas is hotter than Alaska, so it 
becomes difficult to compare temperatures in August to temperatures in January,
or temperatures in Texas to temperatures in Alaska
and tell whether there was warming.

To make it easier and more reliable to compare temperatures at different times
and places, we define anomalies: The temperature anomaly is the difference between
the temperature recorded at a certain location during a certain month and
a baseline reference value, which is the average temperature for that month
and location over a period that is typically 30 years.

The GISS temperature data uses a baseline reference period of 1951--1980, so 
for instance, the temperature anomaly for Nashville in July 2017 would be
the monthly average temperature measured in Nashville during July 2017 minus
the average of all July temperatures measured in Nashville from 1951--1980.

The GISS temperature data file then averages the temperature anomalies over all
the temperature-measuring stations around the world and reports a global average
anomaly for every month from January 1880 through the latest measurements
available (currently, July 2017).

Let's focus on the months only. Use `select` to select just the columns for 
""Year"" and January through December (if you are selecting a consecutive range
of columns between ""Foo"" and ""Bar"", you can call `select(Foo:Bar)`).
Save the result in a variable called `giss_monthly`

```{r make_giss_monthly, include=TRUE}
giss_monthly = select(giss_temp, Year:Dec)
#
# alternately, you could remove unwanted columns:
# 
# giss_monthly = select(giss_temp, -(`J-D`:SON))
# 
# You have to use back-quotes for the column `J-D` because its name includes
# characters other than ""a""-""z"", ""A""-""Z"", ""0""-""9"", ""."", and ""_"".
# You can give columns names with other characters than these, but it becomes
# more complicated to indicate them to R.

head(giss_monthly)
```

Next, it will be difficult to plot all of the data if the months are organized as
columns. What we want is to transform the data tibble into one with three columns:
""year"", ""month"", and ""anomaly"". We can do this easily using the `gather` function
from the `tidyverse` package: `gather(df, key = month, value = anomaly, -Year)`
or `df %>% gather(key = month, value = anomaly, -Year)` will gather all of the 
columns except `Year` (the minus sign in `select` or `gather` means to include
all columns except the ones indicated with a minus sign) and:

* Make a new tibble with three columns: ""Year"", ""month"", and ""anomaly""
* For each row in the original tibble, make rows in the new tibble for each of 
  the columns ""Jan"" through ""Dec"", putting the name of the column in ""month"" 
  and the anomaly in ""anomaly"".

Here is an example of using `gather`, using the built-in data set `presidents`,
which lists the quarterly approval ratings for U.S. presidents from 1945--1974:

```{r gather_example_prep, include=TRUE}

df = presidents@.Data %>% matrix(ncol=4, byrow = TRUE) %>%
  as_tibble() %>% set_names(paste0(""Q"", 1:4)) %>% mutate(year = 1944 + seq(n()))


print(""First 10 rows of df are"")
print(head(df, 10))
```

For each year, the table has a column for the year and 
four columns (Q1 ... Q4) that hold the quarterly 
approval ratings for the president in that quarter.
Now we want to gather these data into three columns:
one column for the year, one column to indicate the quarter,
and one column to indicate the approval rating.

We do this with the `gather` function from the `tidyverse` package.

```{ r gather_example}
dfg <- df %>% gather(key = quarter, # create a column called ""quarter"" to store
                                    # the names of the columns that are gathered
                     value = approval, # create a column called ""approval"" to
                                       # store the values from those columns
                                       # (i.e., the approval ratings in that quarter)
                     -year # the minus sign means gather all columns EXCEPT year.
                     ) %>%
  arrange(year, quarter) # sort the rows of the resulting tibble to put
                         # the years in ascending order, from 1945 to 1971
                         # and within each year, sort the quarters from Q1
                         # to Q4

head(dfg) # print the first few rows of the tibble.
```

Now you try to do the same thing to:

* First select just the columns of `giss_monthly` for the year and the 
  individual months.

* Next, gather all the months togeher, so there will be three columns:
  one for the year, one for the name of the month, and one for the 
  temperature anomaly in that month.

* Store the result in a new variable called `giss_g`

```{r gather_giss, include=TRUE}
giss_g = gather(giss_monthly, key = month, value = anomaly, -Year)
```

Remember how the CO~2~ data had a column `date` that had a year plus a fraction
that corresponded to the month, so June 1960 was 1960.4548?

Here is a trick that lets us do the same for the `giss_g` data set.
R has a data type called `factor` that it uses for managing categorical data,
such as male versus female, Democrat versus Republican, and so on.
Categorical factors have a textual label, but are silently represented as integer 
numbers. Normal factors don't have a special order, so R sorts the values alphabetically.
However, there is another kind of factor called an ordered factor, which allows
us to specify the order of the values.

We can use a built-in R variable called `month.abb`, which is a vector of
abbreviations for months.

The following command will convert the `month`  column in `giss_g` into an
ordered factor that uses the integer values 1, 2, ..., 12 to stand for
""Jan"", ""Feb"", ..., ""Dec"", and then uses those integer values to create a new
column, `date` that holds the fractional year, just as the `date` column in
`mlo_data` did:

```
giss_g = giss_g %>% 
  mutate(month = ordered(month, levels = month.abb),
         date = Year + (as.integer(month) - 0.5) / 12) %>% 
  arrange(date)`
```

In the code above, `ordered(month, levels = month.abb)` converts the variable
`month` from a character (text) variable that contains the name of the month
to an ordered factor that associates a number with each month name, such that
""Jan"" = 1 and ""Dec"" = 12.

Then we create a new column called `date` to get the fractional year corresponding
to that month. We have to explicitly convert the ordered factor into a number
using the function `as.integer()`, and we subtract 0.5 because the time that
corresponds to the average temperature for the month is the middle of the month.

Below, use code similar to what I put above to add a new `date` column to
`giss_g`.

```{r add_date_to_giss_g, include=TRUE}
# Here, you just copy the code from above and run it.
# 
giss_g = giss_g %>% 
  mutate(month = ordered(month, levels = month.abb),
         date = Year + (as.integer(month) - 0.5) / 12) %>%
  arrange(date)
```

Now plot the monthly temperature anomalies versus date:

```{r plot_giss, include=TRUE}
ggplot(giss_g, aes(x = date, y = anomaly)) +
  geom_line() +
  geom_point() +
  labs(x = ""Year"", y = ""Temperature Anomaly (Celsius)"")
```

That plot probably doesn't look like much, because it's very noisy.
Use the function `rollapply` from the package `zoo` to create 
new columns in `giss_g` with
12-month and 10-year (i.e., 120-month) rolling averages of the 
anomalies.

Make a new plot in which you plot a thin blue line for the monthly anomaly
(use `geom_line(aes(y = anomaly), color = ""blue"", alpha = 0.3, size = 0.1)`;
alpha is an optional specification for transparency where 0 means invisible
(completely transparent) and 1 means opaque),
a medium dark green line for the one-year rolling average,
and a thick dark blue line for the ten-year rolling average.

```{r plot_giss_with_smoothing, include=TRUE, warning=FALSE}
giss_g %>% 
   mutate( smooth.1 = rollapply(data = anomaly, width = 12, FUN = mean,
                                      fill = NA, align = ""center""),
           smooth.10 = rollapply(data = anomaly, width = 120, FUN = mean,
                                      fill = NA, align = ""center"")) %>% 
  ggplot(aes(x = date)) + # Put code here to map variables to aesthetics
  geom_line(aes(y = anomaly), alpha = 0.3, size = 0.1, color = ""blue"") +
  geom_line(aes(y = smooth.1), color = ""dark green"", size = 0.5) + 
  geom_line(aes(y = smooth.10), color = ""dark blue"", size = 1) +
  labs(x = ""Year"", y = ""Temperature Anomaly (Celsius)"")
```

Alternately, we could do this fancier version:
```{r alt_plot_giss_with_smoothing, include=TRUE, warning=FALSE}
giss_g %>% 
   mutate( smooth.1 = rollapply(data = anomaly, width = 12, FUN = mean,
                                      fill = NA, align = ""center""),
           smooth.10 = rollapply(data = anomaly, width = 120, FUN = mean,
                                      fill = NA, align = ""center"")) %>% 
  ggplot(aes(x = date)) + # Put code here to map variables to aesthetics
  geom_line(aes(y = anomaly, size = ""None"", color = ""None""), alpha = 0.3) +
  geom_line(aes(y = smooth.1, size = ""1-year"", color = ""1-year"")) + 
  geom_line(aes(y = smooth.10, size = ""10-year"", color = ""10-year"")) +
  scale_color_manual(values = c(""None"" = ""blue"", ""1-year"" = ""dark green"",
                                ""10-year"" = ""dark blue""), name = ""Averaging"") +
  scale_size_manual(values = c(""None"" = 0.1, ""1-year"" = 0.5,
                               ""10-year"" = 1.0), name = ""Averaging"") +
  labs(x = ""Year"", y = ""Temperature Anomaly (Celsius)"")
```


The graph shows that temperature didn't show a steady trend until starting around
1970, so we want to isolate the data starting in 1970 and fit a linear trend
to it.

To select only rows of a tibble that match a condition, we use the function 
`filter` from the `tidyverse` package:

`data_subset = df %>% filter( conditions )`, where `df` is your original tibble
and `conditions` stands for whatever conditions you want to apply.
You can make a simple condition using equalities or inequalities:

* `data_subset = df %>% filter( month == ""Jan"")` to select all rows where the 
  month is ""Jan""
  
* `data_subset = df %>% filter( month != ""Aug"")` to select all rows where the 
  month is not August.

* `data_subset = df %>% filter( month %in% c(""Sep"", ""Oct"", ""Nov"")` to select all 
  rows where the month is one of ""Sep"", ""Oct"", or ""Nov"".

* `data_subset = df %>% filter(year >= 1945)` to select all rows where the year 
  is greater than or equal to 1945.

* `data_subset = df %>% filter(year >= 1951 & year <= 1980 )` to select all rows
  where the year is between 1951 and 1980, inclusive.

* `data_subset = df %>% filter(year >= 1951 | month == ""Mar"")` to select all rows
  where the year is greater than or equal to 1951 or the month is ""Mar"".
  this will give all rows from January 1951 onward, plus all rows before 1951
  where the month is March.

Below, create a new variable `giss_recent` and assign it a subset of `giss_g` that 
has all the data from January 1970 through the present. Fit a linear trend to
the monthly anomaly and report it.

What is the average change in temperature from one year to the next?

```{r giss_trend, include=TRUE}
giss_recent = filter(giss_g, date >= 1970)

recent_trend = lm(anomaly ~ date, data = giss_recent)

tidy(recent_trend)
```

### Did Global Warming Stop after 1998?

It is a common skeptic talking point that global warming stopped in 1998.
In years with strong El Ni&ntilde;os, global temperatures tend to be higher
and in years with strong La Ni&ntilde;as, global temperatures tend to be lower.
We will discuss why later in the semester.

The year 1998 had a particularly strong El Ni&ntilde;o, and the year set a record
for global temperature that was not exceeded for several years. Indeed, compared
to 1998, it might look as though global warming paused for many years.

We will examine whether this apparent pause has scientific validity.

To begin with, we will take the monthly GISS temperature data and convert it to
annual average temperatures, so we can deal with discrete years, rather than 
separate temperatures for each month.

We do this with the `group_by` and `summarize` functions.

We also want to select only recent data, so we arbitrarily say we will look at
temperatures starting in 1979, which gives us 19 years before the 1998 
El Ni&tilde;o.

We don't have a full year of data for 2017, so we want to discard that because we
won't get a full year average from it.

If we go back to the original `giss_g` data tibble, run the following code:

```{r summarize_giss, include=TRUE}
giss_annual = giss_g %>% 
  filter(Year >= 1979 & Year < 2017) %>%
  group_by(Year) %>% 
  summarize(anomaly = mean(anomaly)) %>%
  ungroup() %>%
  mutate(date = Year + 0.5)

head(giss_annual)
```

This code groups the giss data by the year, so that one group will have
January--December 1979, another will have January--December 1980, and
so forth.

Then we replace the groups of 12 rows for each year (each row represents one month)
with a single row that represents the average of those 12 months.

It is important to tell R to `ungroup` the data after we're done working with
the groups.

Finally, we set `date` to `year + 0.5` because the average of a year corresponds 
to the middle of the year, not the beginning.

Now, let's introduce a new column `after`, which indicates whether the data is 
after the 1998 El Ni&ntilde;o:

Now plot the data and color the points for 1998 and afterward dark red to
help us compare before and after 1998.

```{r plot_recent_giss, include=TRUE}
ggplot(giss_annual, aes(x = date, y = anomaly)) +
  geom_line(size = 1) +
  # I didn't include it in the original instructions, but the
  # following version of geom_line is nicer than what's above:
  # 
  # geom_line(aes(color = Year >= 1998), size = 1) +
  # 
  geom_point(aes(color = Year >= 1998), size = 2) +
  scale_color_manual(values = c(""TRUE"" = ""dark red"", ""FALSE"" = ""dark blue""),
                     guide = ""none"") + # color ""before"" points dark blue, 
                                       # ""after"" points dark red
                                       # don't use a legend
  labs(x = ""Year"", y = ""Temperature Anomaly (Celsius)"")
```

Does it look as though the red points are not rising as fast as the blue points?

Let's just plot the data from 1998 on:

```{r plot_pause, include=TRUE, warning=FALSE}
ggplot(giss_annual, aes(x = date, y = anomaly)) +
  geom_line(size = 1) +
  # I didn't include it in the original instructions, but the
  # following version of geom_line is nicer than what's above:
  # 
  # geom_line(aes(color = Year >= 1998), size = 1) +
  # 
  geom_point(aes(color = Year >= 1998), size = 2) +
  scale_color_manual(values = c(""TRUE"" = ""dark red"", ""FALSE"" = ""dark blue""),
                     guide = ""none"") + # color ""before"" points dark blue, 
                                       # ""after"" points dark red
                                       # don't use a legend
  xlim(1998,2016) +
  labs(x = ""Year"", y = ""Temperature Anomaly (Celsius)"")
```

Now how does it look?

Let's use the `filter` function to break the data into two different tibbles: 
`giss_before` will have the data from 1979--1998 and the other, `giss_after` 
will have the data from 1998 onward (note that the year 1998 appears in both
tibbles).

```{r split_giss_data, include=TRUE}
giss_before = filter(giss_annual, Year <= 1998)
giss_after = filter(giss_annual, Year >= 1998)
```

Now use `lm` to fit a linear trend to the temperature data in `giss_before`
(from 1979--1998) and assign it to a variable `giss_trend`.

Next, add a column `timing` to each of the split data sets and set
the value of this column to ""Before"" for `giss_before` and ""After"" for
`giss_after`.

```{r add_timing, include=TRUE}
giss_before = mutate(giss_before, timing = ""Before"")
giss_after = mutate(giss_after, timing = ""After"")

giss_trend = lm(anomaly ~ date, data = giss_before)
tidy(giss_trend)
```

Now, combine the two tibbles into one tibble:

```{r combine_giss, include=TRUE}
giss_combined <- bind_rows(giss_before, giss_after)
```

Now let's use ggplot to plot `giss_combined`:

* Aesthetic mapping:
    * Use the `date` column for the _x_ variable.
    * Use the `anomaly` column for the _y_ variable.
    * Use the `timing` column to set the color of plot elements
* Plot both lines and points.
    * Set the `size` of the lines to 1
    * Set the `size` of the points to 2
* Use the `scale_color_manual` function to set the color of
  ""Before"" to ""dark blue"" and ""After"" to ""dark red""
* Use `geom_smooth(data = giss_before, method=""lm"", color = ""blue"", fill = ""blue"", alpha = 0.2, fullrange = TRUE)` to show a linear trend that is fit just to the `giss_before` data.

```{r plot_combined, include = TRUE}
ggplot(giss_combined, aes(x = date, y = anomaly, color = timing)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  geom_smooth(data = giss_before, method = ""lm"", color = ""blue"", fill = ""blue"",
              alpha = 0.2, fullrange = TRUE) +
  scale_color_manual(values = c(Before = ""dark blue"", After = ""dark red"")) +
  labs(x = ""Year"", y = ""Temperature Anomaly (Celsius)"")  
```

Try this with the parameter `fullrange` set to `TRUE` and `FALSE` in the 
`geom_smooth` function. What is the difference?

```{r alt_plot_combined_false, include = TRUE}
ggplot(giss_combined, aes(x = date, y = anomaly, color = timing)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  geom_smooth(data = giss_before, method = ""lm"", color = ""blue"", fill = ""blue"",
              alpha = 0.2, fullrange = FALSE) +
  scale_color_manual(values = c(Before = ""dark blue"", After = ""dark red"")) +
  labs(x = ""Year"", y = ""Temperature Anomaly (Celsius)"")  
```

```{r alt_plot_combined_true, include = TRUE}
ggplot(giss_combined, aes(x = date, y = anomaly, color = timing)) +
  geom_line(size = 1) +
  geom_point(size = 2) +
  geom_smooth(data = giss_before, method = ""lm"", color = ""blue"", fill = ""blue"",
              alpha = 0.2, fullrange = TRUE) +
  scale_color_manual(values = c(Before = ""dark blue"", After = ""dark red"")) +
  labs(x = ""Year"", y = ""Temperature Anomaly (Celsius)"")  
```

**Answer:** 
Both plots show the full data set, and a linear trend that is fit just 
to the ""before"" data. The trend line shows both the best fit for a trend
(that's the solid line) and the range of uncertainty in the fit (that's the 
light blue shaded area around the line).

But, when `fullrange = FALSE`, the line is only drawn for the data to which the
trend was fit, whereas when `fullrange = TRUE`, the trend line is drawn for the full 
range of the graph, even though the trend was only fit to the data in part of
the graph.

If the temperature trend changed after 1998 (e.g., if the warming paused, or if
it reversed and started cooling) then we would expect the temperature measurements
after 1998 to fall predominantly below the extrapolated trend line, and our 
confidence that the trend had changed would depend on the number of points that
fall below the shaded uncertainty range.

How many of the red points fall below the trend line?

**Answer:** 6 points: 1999, 2000, 2008, 2011, 2012, and 2013.

How many of the red points fall above the trend line?

**Answer:** Not counting 1998, 12 points: 2001, 2002, 2003, 2004 (barely), 
2005, 2006, 2007, 2009, 2010, 2014, 2015, and 2016.

What do you conclude about whether global warming paused or stopped after 1998?

**Answer:** Most of the years after 1998 were warmer than we would have
predicted if temperatures had continued to follow the warming trends of 1979--1998,
so this is not evidence of any slow-down or pause in the warming.
"
301,MODTRAN_LAB,2018-09-21,Lab #3 Answers,,answers,/files/lab_solutions/lab_03/lab_03_answers.pdf,"```{r setup, include=FALSE}
knitr::knit_hooks$set(inline = function(x) { knitr:::format_sci(x, 'md')})
knitr::opts_chunk$set(echo = TRUE, cache=TRUE, message=FALSE, warning=FALSE,
                      fig.height=4, fig.width=6,
                      fig.pos = 'htbp')

# This section loads necessary R libraries and sources scripts that define 
# useful functions format_md.
# 
data_dir = ""_data""
script_dir = ""_scripts""

if (!dir.exists(data_dir)) dir.create(data_dir)

library(pacman)
p_load(zoo, xml2, tidyverse, stringr)

theme_set(theme_bw(base_size = 15))

source(file.path(script_dir, ""utils.R""), chdir = T)
source(file.path(script_dir, ""format_md.R""), chdir = T)
source(file.path(script_dir, ""modtran.R""), chdir = T)

```
# Chapter 4 Exercises

## Exercise 4.1: Methane

Methane has a current concentration of 1.7 ppm in the atmosphere and
is doubling at a faster rate than CO~2~.

------

a) Would an additional 10 ppm of methane in the atmosphere have a larger or smaller
  impact on the outgoing IR flux than an additional 10 ppm of CO~2~ at current
  concentrations?

------

**Answer:**
Run MODTRAN in three configurations: 
400 ppm CO~2~ and 1.7 ppm methane,
400 ppm CO~2~ and 11.7 ppm methane,
and
410 ppm CO~2~ and 1.7 ppm methane,
and compare I~out~ for each configuration:

```{r ex_4_1_a_modtran, include=TRUE, message=FALSE}
run_modtran(file.path(data_dir, ""modtran_baseline.txt""), 
            atmosphere = 'tropical', co2 = 400, ch4 = 1.7)
run_modtran(file.path(data_dir, ""modtran_plus_10_ppm_methane.txt""), 
            atmosphere = 'tropical', co2 = 400, ch4 = 11.7 )
run_modtran(file.path(data_dir, ""modtran_plus_10_ppm_co2.txt""), 
            atmosphere = 'tropical', co2 = 410, ch4 = 1.7 )

baseline = read_modtran(file.path(data_dir, ""modtran_baseline.txt""))
plus_10_methane = read_modtran(file.path(data_dir, 
                                         ""modtran_plus_10_ppm_methane.txt""))
plus_10_co2 = read_modtran(file.path(data_dir, ""modtran_plus_10_ppm_co2.txt""))

i_baseline = baseline$i_out
i_methane = plus_10_methane$i_out
i_co2 = plus_10_co2$i_out
```

Now, we calculate the change in I~out~ corresponding to adding 10 ppm of methane
and adding 10 ppm CO~2~:

```{r ex_4_1_a_delta_i_out, include=TRUE}
delta_i_methane = i_baseline - i_methane
delta_i_co2 = i_baseline - i_co2
ratio_10_ppm = delta_i_methane / delta_i_co2
```

At the default settings, 
I~out~ = `r format_md(i_baseline, digits = 2)` W/m^2^. 
If we add 10 ppm methane, I~out~ becomes
`r format_md(i_methane, digits = 2)` W/m^2^
(a decrease of `r format_md(delta_i_methane, digits = 2)` 
W/m^2^)
and if we add 10 ppm CO~2~, I~out~ becomes
`r format_md(i_co2, digits = 2)` W/m^2^
(a decrease of `r format_md(delta_i_co2, digits = 2)` 
W/m^2^). 

Thus, the impact of adding 10 ppm of methane on I~out~ is 
`r format_md(ratio_10_ppm, digits = 2)` 
times greater than the impact of adding 10 ppm of CO~2~,
which means methane would also have a much greater impact on the temperature.

------

b) Where in the spectrum does methane absorb? What concentration does it take to
  begin to saturate the absorption in this band? Explain what you are looking 
  at to judge when the gas is saturated.
  
    **Note:** See the suggestions in the instructions for this lab.

------

**Answer:**
We can run MODTRAN with no greenhouse gases except methane to find where it
absorbs. Setting methane to 10 ppm will give a good indication of where it
absorbs:

```{r ex_4_1_b_spectrum, include=TRUE, message=FALSE, warning=FALSE}
# file.path combines one or more directories with a filename.
filename = file.path(data_dir, ""modtran_10_ppm_methane.txt"")

run_modtran(filename, co2_ppm = 0, ch4_ppm = 10, 
            trop_o3_ppb = 0, strat_o3_scale = 0, h2o_scale = 0, 
            freon_scale = 0, 
            delta_t = 0, h2o_fixed = ""vapor pressure"",
            atmosphere = ""tropical"", clouds = ""none"", 
            altitude_km = 70, looking = ""down"")

plot_modtran(filename)
```

Methane absorbs in the range 1200--1400 cm^-1^ (we can look at the wavelength
scale on the top of the plot and see that this corresponds to roughly 
7--8 micron wavelength).

Now let's find out what concentration of methane band corresponds to the onset
of band-saturation. Start with 0.4 ppm methane and keep doubling for 11 
doublings (until we get to 2048 times the original concentration).

We create an empty `tibble`, and then every time we run a new
MODTRAN simulation, we create a tibble with one row and two columns:
methane concentration and I~out~. Then we use `bind_rows` to add this
tibble to the bottom of `methane_data`. This produces a tibble with
columns for methane concentration and I~out~, and a row for each run
of MODTRAN.

This is an example of using `bind_rows` to combine tibbles together
row by row when they have the same columns.

```{r ex_4_1_b_saturation, include=TRUE, message=FALSE, warning=FALSE}
        methane_data = tibble() # create a blank data tibble

        # The for command repeats everything between the braces ""{...}""
        # for each value of x in the sequence 0, 1, 2, ..., 10, 11.    
        for (x in 0:11) {
          # Set the methane concentration to 0.4 times 2 to the power of x,
          # In other words 0.4, 0.8, 1.6, 3.2, ...
          p_methane = 0.4 * (2^x) 
          
          # Create a character variable that will be a file name of the form
          # ""_data/methane_xx_x.txt"", where xx_x is the methane concentration,
          # with an underscore for the decimal point.
          file_name = formatC(p_methane, digits = 1, decimal.mark = ""_"", 
                              format = ""f"") %>%
                      str_c('methane_', ., "".txt"") %>%
                      file.path(data_dir, .)
    
          # Now run MODTRAN
          run_modtran(file_name, co2_ppm = 0, ch4_ppm = p_methane, 
                      trop_o3_ppb = 0, strat_o3_scale = 0, h2o_scale = 0, 
                      freon_scale = 0, 
                      delta_t = 0, h2o_fixed = ""vapor pressure"",
                      atmosphere = ""tropical"", clouds = ""none"", 
                      altitude_km = 70, looking = ""down"")
    
          # Read the MODTRAN results into R
          results = read_modtran(file_name)

          p = plot_modtran(file_name, 
                           descr = str_c(p_methane, "" ppm methane""))
          print(p)
          # Create a data tibble with columns for the methane concentration
          # and I out, and append it to the end of the tibble methane_data
          df = tibble(methane = results$ch4, i_out = results$i_out)
          methane_data = bind_rows(methane_data, df)
        }
```

Now that we have completed all the model runs, we can analyze the data.
First, plot I~out~ for each concentration of methane:

```{r plot_methane_saturation, include=TRUE}
  plot_0 = ggplot(methane_data, aes(x = methane, y = i_out)) +
  geom_line(size = 1) + 
  geom_point(size = 2) +
  labs(x = ""Methane concentration (ppm)"", 
       y = expression(I[out]~(W/m^2)),
       title = expression(paste(I[out], "" versus methane concentration"")))

  print(plot_0)
```

This plot shows saturation in action: At small concentrations of methane,
a small amount of added methane produces a large decrease in I~out~, but
when methane concentrations are larger, even large additions of methane
have a much smaller effect on I~out~ than the small additions did when 
concentrations were low.

However, to examine saturation, it's often much more useful to plot
I~out~ against the logarithm of the concentration, as we show below
by plotting the concentrations on a logarithmic axis. 

A logarithmic axis represents the intervals for doublings as the same 
size on the axis, whether they represent a doubling from 1 to 2 ppm or 
from 1000 to 2000 ppm.

```{r plot_methane_saturation_log_scale, include=TRUE}
  plot_1 = ggplot(methane_data, aes(x = methane, y = i_out)) +
  geom_line(size = 1) + 
  geom_point(size = 2) +
  scale_x_log10(breaks = c(1, 2, 5, 10, 20, 50, 100, 200, 500, 1000)) +
  labs(x = ""Methane concentration (ppm)"", 
       y = expression(I[out]~(W/m^2)),
       title = expression(paste(I[out], 
                          "" versus methane concentration (log scale)"")))

  print(plot_1)
```

When we look at I~out~ versus the logarithm of the concentration, we see
that at small concentrations, successive doublings produce steeper and
steeper slopes in the change of I~out~, but as concentrations get large
the slopes change by less and less from one doubling to the next and 
I~out~ begins to approach a straight line instead of a curve.

In order to better estimate where I~out~ stops curving and becomes straight,
we plot the change in I~out~ from one
concentration to the next (remember that we're doubling the concentration
each time). 

Use the `lag` function to calculate how `i_out` changes from one row
to the next:

```{r calc_i_out_change, include = TRUE}
methane_data = methane_data %>% mutate(change = i_out - lag(i_out))
```


When I~out~ starts to follow a straight line when it's plotted
against the logarithm of the concentration:

```{r plot_methane_intensity_change, include=TRUE}
  plot_2 = ggplot(methane_data, aes(x = methane, y = change)) +
  geom_line(size = 1) + 
  geom_point(size = 2) +
  scale_x_log10(breaks = c(1, 2, 5, 10, 20, 50, 100, 200, 500, 1000)) +
  labs(x = ""Methane concentration (ppm)"", 
       y = expression(Delta * I[out]~(W/m^2)),
       title = expression(paste(""Change in "", I[out], 
                                "" between successive doublings"")))

  print(plot_2)
```

At small concentrations, we see the change in I~out~ getting more and 
more negative, but at larger concentrations, as saturation kicks in, 
we start to see the change in I~out~ flattening out. 

We can identify band saturation with the first concentration at which the 
change in I~out~ is the same for that doubling and for the next.
This would be around 200 ppm.

You notice that the flattening isn't perfect or absolute: the changes
flatten out from 200--400 ppm, but then become even smaller at 800 ppm.
This is because many molecules, such as methane, have multiple absorption
bands, and the different bands saturate at different concentrations.

We generally say that absorption begins to saturate when the successive
changes in I~out~ flatten out for the first time.

The graph of the change in I~out~ for successive doublings of methane concentration
flattens out at around 200 ppm, meaning that we can estimate that this is 
roughly where band saturation occurs.

------

c) Would a doubling of methane have as great an impact on the heat balance as a 
   doubling of CO~2~?

------

**Answer:**
We already have a baseline run of MODTRAN from part (a), 
with 400 ppm CO~2~ and 1.7 ppm methane, so now we need to run MODTRAN
for 800 ppm CO~2~ and 1.7 ppm methane, and for 
400 ppm CO~2~ and 3.4 ppm methane.

```{r ex_4_1_c, include=TRUE, message=FALSE}
run_modtran(file.path(data_dir, ""modtran_double_co2.txt""), 
            co2 = 800, ch4 = 1.7, 
            atmosphere = 'tropical')
run_modtran(file.path(data_dir, ""modtran_double_ch4.txt""), 
            co2 = 400, ch4 = 2 * 1.7, 
            atmosphere = 'tropical')
double_co2 = read_modtran(file.path(data_dir, ""modtran_double_co2.txt""))
double_methane = read_modtran(file.path(data_dir, 
                                        ""modtran_double_ch4.txt""))
```

Now compare I~out~ and calculate the change of I~out~ from the baseline
when we double CO~2~ and when we double methane:

```{r ex_4_1_c_calc_i_out, include=TRUE, message=FALSE}
i_2x_co2 = double_co2$i_out
i_2x_methane = double_methane$i_out

delta_co2_2x = i_baseline - i_2x_co2
delta_methane_2x = i_baseline - i_2x_methane
```

Doubling CO~2~ reduces I~out~ by 
`r format_md(delta_co2_2x, digits = 2)` W/m^2^.
Doubling methane reduces I~out~ by 
`r format_md(delta_methane_2x, digits = 2)` W/m^2^, so 
the effect of doubling CO~2~ on I~out~ is
`r format_md(delta_co2_2x / delta_methane_2x, digits = 2)`
times greater than doubling methane, 
and doubling CO~2~ will have a much greater effect on temperature.

------

d) What is the ""equivalent CO~2~"" of doubling atmospheric methane? That is to say,
   how many ppm of CO~2~ would lead to the same change in outgoing IR radiation
   energy flux as doubling methane? What is the ratio of ppm CO~2~ change to 
   ppm methane change?

------

**Answer:**
From part (c), we know that doubling methane reduces I~out~ by
`r format_md(delta_methane_2x, digits = 2)` W/m^2^.

Part (a) showed us that the effect on I~out~ of adding 10 ppm of methane
is `r format_md(ratio_10_ppm, digits = 2)` times as great as adding 
10 ppm of CO~2~, so we might guess that doubling methane (adding 1.7 ppm) 
would be equivalent to adding 
`r format_md(ratio_10_ppm * 1.7, digits = 2)` ppm of CO~2~.

Let's test this guess with MODTRAN:

```{r ex_4_1_d_guess, include=TRUE, message=FALSE}
delta_co2_guess = ratio_10_ppm * 1.7

run_modtran(file.path(data_dir, ""modtran_guess.txt""), 
            atmosphere = 'tropical',
            co2 = 400 + delta_co2_guess, ch4 = 1.7)
modtran_guess_co2 = read_modtran(file.path(data_dir, ""modtran_guess.txt""))

i_guess = modtran_guess_co2$i_out

delta_i_guess = i_baseline - i_guess
```

So we see that our guess of increasing CO~2~ by 
`r format_md(delta_co2_guess, digits = 2)` ppm 
changed I~out~ by 
`r format_md(delta_i_guess, digits = 2)` W/m^2^, 
so this was not nearly enough of a change in CO~2~ 
to match a doubling of methane.

```{r ex_4_1_manual_equiv, include=TRUE, message=FALSE}
equiv_co2 = 85
```

Next, open up the web-based version of MODTRAN and manually adjust
the CO~2~ concentration until I~out~ matches I~out~ for doubled 
methane. If you do this, you will find that adding
`r format_md(equiv_co2, digits = 2)` ppm of CO~2~
matches doubling methane:

```{r ex_4_1_manual, include=TRUE, message=FALSE}
run_modtran(file.path(data_dir, ""modtran_equiv_co2.txt""), 
            atmosphere = 'tropical', co2 = 400 + equiv_co2, ch4 = 1.7)

modtran_equiv_co2 = read_modtran(file.path(data_dir, 
                                           ""modtran_equiv_co2.txt""))

i_equiv = modtran_equiv_co2$i_out

delta_equiv = i_baseline - i_equiv
```

Adding `r format_md(equiv_co2, digits = 2)` ppm CO~2~ 
changes I~out~ by
`r format_md(delta_equiv, digits = 2)` W/m^2^, which
matches what we measured for doubling methane.

Thus, doubling methane is equivalent to increasing CO~2~ by
`r equiv_co2` ppm.

## Exercise 4.2: CO~2~ (Graduate students only)

a) Is the direct effect of increasing CO~2~ on the energy output at the top of
   the atmosphere larger in high latitudes or in the tropics?

    For each atmosphere, first record $I_{\text{out}}$ with CO~2~ at 400 ppm
    and then record the change when you increase CO~2~ to 800 ppm.

------

**Answer:**
First, run MODTRAN for the different configurations:

```{r ex_4_2_a, include=TRUE, message=FALSE}
# We have already calculated these numbers for tropical atmosphere.
# Now let's do the same for midlatitude summer and subarctic summer:
run_modtran(file.path(data_dir, ""midlat_summer_baseline.txt""), 
            co2 = 400, atmosphere = 'midlatitude summer') 
run_modtran(file.path(data_dir, ""midlat_summer_2x_co2.txt""), 
            co2 = 800, atmosphere = 'midlatitude summer') 
run_modtran(file.path(data_dir, ""subarctic_summer_baseline.txt""), 
            co2 = 400, atmosphere = 'subarctic summer') 
run_modtran(file.path(data_dir, ""subarctic_summer_2x_co2.txt""), 
            co2 = 800, atmosphere = 'subarctic summer')

midlat_baseline = read_modtran(file.path(data_dir, 
                                         ""midlat_summer_baseline.txt""))
midlat_double = read_modtran(file.path(data_dir, 
                                       ""midlat_summer_2x_co2.txt""))

i_midlat_baseline = midlat_baseline$i_out
i_midlat_double = midlat_double$i_out

delta_midlat =  i_midlat_baseline - i_midlat_double

subarctic_baseline = read_modtran(file.path(data_dir, 
                                  ""subarctic_summer_baseline.txt""))
subarctic_double = read_modtran(file.path(data_dir, 
                                ""subarctic_summer_2x_co2.txt""))

i_subarctic_baseline = subarctic_baseline$i_out
i_subarctic_double = subarctic_double$i_out

delta_subarctic = i_subarctic_baseline - i_subarctic_double
```

Here are the results:

* Tropical: Change in I~out~ from doubling CO~2~ is 
  `r format_md(delta_co2_2x, digits = 2)` W/m^2^.

* Midlatitude: Change in I~out~ from doubling CO~2~ is 
  `r format_md(delta_midlat, digits = 2)` W/m^2^.

* Subarctic: Change in I~out~ from doubling CO~2~ is 
  `r format_md(delta_subarctic, digits = 2)` W/m^2^.

So the effect of doubling CO~2~ is strongest in the tropics and gets weaker the
farther toward the poles you go. 

**Note:** This is what happens without feedbacks. 
If we include the important feedbacks in the climate system,
the effect of doubling CO~2~ is much greater near the poles
than in the tropics.

------

b) Set pCO~2~ to an absurdly high value of 10,000 ppm. You will see a spike
   in the CO~2~ absorption band. What temperature is this light coming from? 
   Where in the atmosphere do you think this comes from?

    Now turn on clouds and run the model again. Explain what you see.
    Why are night-time temperatures warmer when there are clouds?

------

**Answer:** As we add CO~2~, the skin height rises. 

When the emission at a
certain wavelength flattens out, that corresponds to the skin height for
that wavelength reaching the tropopause, so as the skin height rises
the temperature at the skin height remains constant because it's in the 
region of the lower stratosphere where the environmental lapse rate is zero.

Eventually, the skin height rises above that part of the stratosphere into the
region where the temperature starts rising with increasing altitude (i.e.,
where the environmental lapse rate is negative). 

For wavelengths where the 
skin height is in this part of the stratosphere, there will be a spike of 
increased longwave emissions.

**Details:** 

Students don't need to do this, but this makes a good illustration of exactly
where in the atmosphere, the spike comes from:

```{r ex_4_2_spectra, include=TRUE, message=FALSE}
pco2 = 1.0E+4 # 10,000 ppm

for (alt in c(10, 20, 30, 40, 50, 60, 70)) {
  fname = str_c(file.path(data_dir, ""10k_co2_alt_""), alt, '.txt')
  run_modtran(fname, co2 = pco2, altitude = alt)
  p = plot_modtran(fname)
  print(p)
}
```

Now let's look at the temperature profile:

```{r ex_4_2_profile, include=TRUE, message=FALSE, warning = FALSE}
profile = read_modtran_profile(file.path(data_dir, ""10k_co2_alt_70.txt""))

ggplot(profile, aes(x = T, y = Z)) +
  geom_path(size = 1) +
  scale_y_continuous(breaks = seq(0,70,10), limits = c(0,70)) +
  labs(x = ""Temperature (Kelvin)"", y = ""Altitude (km)"")
```


Now let's run the model with the altitude set to 70 km and stratus clouds
turned on:

```{r ex_4_2_b_clouds, include=TRUE, message=FALSE}
pco2 = 1.0E+4 # 10,000 ppm

run_modtran(file.path(data_dir, ""modtran_10k_co2_no_clouds.txt""), 
            co2 = pco2, clouds = 'none')

run_modtran(file.path(data_dir, ""modtran_10k_co2_clouds.txt""), 
            co2 = pco2, clouds = 'altostratus')

no_clouds = read_modtran(file.path(data_dir, 
                                   ""modtran_10k_co2_no_clouds.txt""))
clouds = read_modtran(file.path(data_dir, 
                                ""modtran_10k_co2_clouds.txt""))

i_no_clouds = no_clouds$i_out
i_clouds = clouds$i_out

p_no_clouds = plot_modtran(file.path(data_dir, 
                                     ""modtran_10k_co2_no_clouds.txt""))
print(p_no_clouds)

p_clouds = plot_modtran(file.path(data_dir, 
                                  ""modtran_10k_co2_clouds.txt""))
print(p_clouds)
```

Without clouds, the outgoing heat from longwave radiation is 
I~out~ = `r format_md(i_no_clouds, digits = 2)` W/m^2^ and with clouds, it's 
I~out~ = `r format_md(i_clouds, digits = 2)` W/m^2^.

In the plots of the spectra, notice how the emissions in the infrared window
from 800--1200 cm^-1^ decrease when middle-level clouds (altostratus) are
added.

## Exercise 4.3: Water vapor

Our theory of climate presumes that an increase in the temperature at ground
level will lead to an increase in the outgoing IR energy flux at the top of the
atmosphere.


a) How much extra outgoing IR would you get by raising the temperature of the 
   ground by 5&deg;C? What effect does the ground temperature have on the 
   shape of the outgoing IR spectrum and why?

------

**Answer:**
Run MODTRAN with `delta_t` set to 5 Kelvin:

```{r ex_4_3_a, include=TRUE, message=FALSE}
run_modtran(file.path(data_dir, ""modtran_plus_5k.txt""), 
            delta_t = 5, h2o_fixed = 'vapor pressure')

modtran_5k = read_modtran(file.path(data_dir, ""modtran_plus_5k.txt""))

i_5k = modtran_5k$i_out

delta_5k =  i_5k - i_baseline
```

Raising the ground temperature by 5K raises I~out~ by 
`r format_md(delta_5k, digits = 2)` W/m^2^.

Next, plot the baseline spectrum and the spectrum for the
warmer surface so we can compare them:

```{r plot_delta_5k, include=TRUE, message=FALSE}
p_baseline = plot_modtran(file.path(data_dir, ""modtran_baseline.txt""), 
                          descr = ""Baseline"")
print(p_baseline)

p_5k = plot_modtran(file.path(data_dir, ""modtran_plus_5k.txt""), 
                    descr = ""Temperature increased by 5K"")
print(p_5k)
```

The whole spectrum becomes brighter (warmer), but you can see a 
greater increase in the wavelengths corresponding to the
infrared window (800--1200 cm^-1^).

You can also see changes in the main CO~2~ emissions peak around 650 cm^-1^,
but not as much as in the window region.

------

b) More water can evaporate into warm air than into cool air. Change the
   model settings to hold the water vapor at constant relative humidity 
   rather than constant vapor pressure (the default), calculate the change
   in outgoing IR energy flux for a 5&deg;C temperature increase.
   Is it higher or lower? Does water vapor make the Earth more sensitive to
   CO~2~ increases or less sensitive?

------

**Answer:**
Run MODTRAN with relative humidity fixed and compare I~out~ to
what we saw in part (a) when we held vapor pressure fixed.

```{r ex_4_3_b, include=TRUE, message=FALSE}
run_modtran(file.path(data_dir, ""modtran_plus_5k_humidity.txt""), 
            delta_t = 5, h2o_fixed = 'relative humidity')

modtran_5k_humidity = 
  read_modtran(file.path(data_dir, ""modtran_plus_5k_humidity.txt""))

i_5k_humidity = modtran_5k_humidity$i_out
delta_5k_humidity = i_5k_humidity - i_baseline
```

When we raised the surface temperature by 5K with 
vapor pressure constant, I~out~ changed by 
`r format_md(delta_5k, digits=2)` W/m^2^.
When we raise the surface temperature by 5K with 
relative humidity constant, I~out changes by
`r format_md(delta_5k_humidity, digits=2)` W/m^2^, which is
about 
`r format_md(100 * delta_5k_humidity / delta_5k, digits = 2)` 
percent of what it was with the water vapor pressure held constant.

Constant relative humidity reduces the change in outgoing longwave radiation
for the same in temperature, so the temperature would have to rise higher to
compensate for the decrease in I~out~ when we increase CO~2~. 

A given change in temperature produces a smaller change in I~out~ when 
relative humidity is constant (i.e., when the water vapor feedback is 
active) than when vapor pressure is held constant (i.e., when the
water vapor feedback is disabled).

When a forcing is applied (e.g., increasing greenhouse gas concentrations),
it changes I~out~, and the surface temperature must change enough to 
bring I~out~ back to its original value to balance the heat flow.

This means
that water vapor makes the earth _more_ sensitive to changes in CO~2~.

Now, let's plot the spectrum for a 5K increase in surface temperature
with constant relative humidity.

```{r humidity_plot, include=TRUE, message=FALSE}
p_humidity = plot_modtran(file.path(data_dir, 
                          ""modtran_plus_5k_humidity.txt""),
                descr = ""Temp. increased by 5K (const. rel. hum.)"")
print(p_humidity)
```

------

c) Now see this effect in another way. 

    * Starting from the default base case, record the total outgoing 
      IR flux. 

    * Now double pCO2. The temperature in the model stays the
      same (that's how the model is written), but the outgoing IR flux
      goes down.

    * Using constant water vapor pressure, adjust the temperature offset
      until you get the original IR flux back again. Record the change in
      temperature
    
    * Now repeat the exercise, but holding the relative humidity fixed
      instead of the water vapor pressure.
    
    * The ratio of the warming when you hold relative humidity fixed
      to the warming when you hold water vapor pressure fixed is the 
      feedback factor for water vapor. What is it?

------

**Answer:** 
Under baseline conditions (400 ppm CO~2~), 
I~out~ = `r format_md(i_baseline, digits = 2)` W/m^2^.

```{r ex_4_3_c_interactive, include=TRUE, message=FALSE}
#
# Figure out the right delta_t to use by interactively playing 
# with the web-based interface to MODTRAN. Then insert the values 
# here.
#
delta_t_vapor_pressure = 0.76
delta_t_humidity = 1.21
```

After playing with the interactive web-based MODTRAN, we find that
after doubling CO~2~ with constant water vapor pressure,
a warming of
`r delta_t_vapor_pressure` Kelvin restores I~out~ to
its original value for 400 ppm.

After doubling CO~2~ with constant relative humidity, it takes a warming of
`r delta_t_humidity` Kelvin to restore I~out~ to its original value.

The ratio of warming for constant relative humidity versus constant water vapor
pressure is 
`r format_md(delta_t_humidity / delta_t_vapor_pressure, digits = 2)`,
so the feedback factor for water vapor in the tropics is 
_f_ = `r format_md(delta_t_humidity / delta_t_vapor_pressure, digits = 2)`.

Below, we show that these changes in temperature restore the original
(baseline) I~out~ with CO~2~ at 800 ppm:

```{r ex_4_3_c, include=TRUE, message=FALSE}
#
# Figure out the right delta_t to use by interactively playing with the
# web-based interface to MODTRAN. Then insert the values here.
#
run_modtran(file.path(data_dir, ""warming_vapor_pressure.txt""), 
            co2 = 800, delta_t = delta_t_vapor_pressure, 
            h2o_fixed = 'vapor pressure')

run_modtran(file.path(data_dir, ""warming_humidity.txt""), 
            co2 = 800, delta_t = delta_t_humidity, 
            h2o_fixed = 'relative humidity')

warming_vapor_pressure = read_modtran(file.path(data_dir, 
                                  ""warming_vapor_pressure.txt""))
warming_humidity = read_modtran(file.path(data_dir, 
                                  ""warming_humidity.txt""))

i_vapor_pressure = warming_vapor_pressure$i_out
i_humnidity = warming_humidity$i_out
```

Under default conditions, 
I~out~ = `r format_md(i_baseline, digits = 2)` W/m^2^.

With doubled CO~2~ and constant water vapor pressure,
raising the surface temperature by 
`r delta_t_vapor_pressure` Kelvin restores I~out~ to
`r format_md(i_vapor_pressure, digits = 2)` W/m^2^.

With doubled CO~2~ and constant relative humidity, 
raising the surface temperature by 
`r delta_t_humidity` Kelvin restores I~out~ to
`r format_md(i_humnidity, digits = 2)` W/m^2^.

# Chapter 5 Exercise

## Exercise 5.2: Skin Height

a) Run the MODTRAN model in using the ""Tropical"" atmosphere, without clouds, and with
   present-day pCO~2~ (400 ppm). Use the ground temperature reported by the model to calculate
   $\varepsilon \sigma T_{\text{ground}}^4$, the heat flux emitted by the ground.
   Assume $\varepsilon = 1$, and I have already provided the value of the 
   Stefan-Boltzmann constant $\sigma$, as the R variable `sigma_sb`,
   which equals `r format_md(sigma_sb, digits = 3, format = ""scientific"")`.
   (I defined it in the script ""utils.R"", which I loaded in the ""setup"" chunk
   in the RMarkdown document).

    Next, look at the outgoing heat flux at the top of the atmosphere (70 km) 
    reported by the MODTRAN model. Is it greater or less than the heat flux
    that you calculated was emitted by the ground?

------

**Answer:** 
Use the Stefan-Boltzmann law to calculate I~up,ground~:

```{r ex_5_2_a, include=TRUE, message=FALSE}
T_ground = baseline$t_ground

i_up_ground = sigma_sb * T_ground^4
i_up_skin = baseline$i_out
```

The ground temperature is `r format_md(T_ground, digits = 3)` Kelvin,
so the Stefan-Boltzmann equation tells us that the ground emits
I~up,ground~ = `r format_md(i_up_ground, digits = 2)` W/m^2^ of longwave
radiation.

MODTRAN calculates that at the top of the atmosphere, there is
I~out~ = `r format_md(i_up_skin, digits = 2)` W/m^2^ of longwave radiation
going out to space, which is considerably less than what we calculated was
emitted by the ground.

------

b) Use the outgoing heat flux at the top of the atmosphere to calcuate the
   skin temperature (use the equation 
   $I_{\text{out}} = \varepsilon \sigma T_{\text{skin}}^4)$).
   What is the skin temperature, and how does it compare to the ground 
   temperature and the temperature at the tropopause, as reported by the 
   MODTRAN model?
   
    Assuming an environmental lapse rate of 6K/km, and using the 
    skin temperature that you calculated above, and the ground temperature
    from the model, what altitude would you expect the skin height to be?

------

**Answer:**
The Stefan-Boltzmann law tells us that
$$I = \varepsilon\sigma T^4,$$
so we can do a little algebra to figure out that
$$T = \sqrt[4]{\frac{I}{\varepsilon\sigma}}.$$
We can use this equation to calcualte the effective
skin temperature from I~out~ at the top of the atmosphere.

```{r ex_5_2_b_skin_temp, include=TRUE, message=FALSE}
T_skin = (i_up_skin / sigma_sb)^0.25 # from Stefan-Boltzmann law
```

T~skin~ = `r format_md(T_skin, digits = 3)` K.

Next, use the lapse rate and the difference between the
skin temperature and the ground temperature in order
to calculate the skin height:
$$h_{\text{skin}} = 
\frac{T_{\text{ground}} - T_{\text{skin}}}{\text{environmental lapse}}$$

```{r ex_5_2_b, include=TRUE, message=FALSE}
env_lapse = 6 # Kelvin per kilometer

h_skin = (T_ground - T_skin) / env_lapse

T_tropopause = baseline$t_tropo
h_tropopause = baseline$h_tropo
```

We find that h~skin~ = `r format_md(h_skin, digits = 2)` km.

According to MODTRAN, 
the ground temperature is `r format_md(T_ground, digits = 3)` K,
the temperature at the tropopause is 
`r format_md(T_tropopause, digits = 3)` K, 
and the height of the 
tropopause is 
`r format_md(h_tropopause, digits = 2)` km, 
so our estimate suggests that the skin height in the tropics
is well below the tropopause, and the skin temperature is considerably
warmer than the tropopause, but considerably colder than the ground
temperature.

------

c) Double the CO~2~ concentration and run MODTRAN again. Do not adjust the
   ground temperature. Repeat the calculations from (b) of the skin
   temperature and the estimated skin height.

    What is the new skin temperature? What is the new skin height?

------

**Answer:**
In part exercise 4.1 (c) we measured I~out~ for doubled CO~2~ 
with no surface temperature change, so we can use this for 
I~out~ from the skin-height with doubled CO~2~:

```{r ex_5_2_c, include=TRUE}
i_skin_2 = i_2x_co2
```

I~skin~ = `r format_md(i_skin_2, digits = 2)` W/^2^

Next, we use the Stefan-Boltzmann law to calculate the skin temperature, 
just as we did in part (b):

```{r ex_5_2_c_t_skin, include=TRUE}
T_skin_2 = (i_skin_2 / sigma_sb)^0.25
```

T~skin~ = `r format_md(T_skin_2, digits = 3)` K.
Now we can use the lapse rate to calculate the skin height:

```{r ex_5_2_c_h_skin, include=TRUE}
h_skin_2 = (T_ground - T_skin_2) / env_lapse
```

The skin height is 
`r format_md(h_skin_2, digits = 1)` km,
which is
`r format_md(h_skin_2 - h_skin, digits = 1)` km 
higher with doubled CO~2~ than for the baseline (current conditions).

d) Put the CO~2~ back to today's value, but add cirrus clouds, using the
   ""standard cirrus"" value for the clouds. Repeat the calculations from (b) of
   the skin temperature and the skin height.
   
    What is the new skin temperature? What is the new skin height?
    Did the clouds or the doubled CO~2~ have a greater effect on the
    skin height?

**Answer:**
Run MODTRAN with cirrus clouds and compare to the baseline conditions:

```{r ex_5_2_d, include=TRUE, message=FALSE}
run_modtran(file.path(data_dir, ""modtran_cirrus.txt""), 
            clouds = 'standard cirrus')

cirrus = read_modtran(file.path(data_dir, ""modtran_cirrus.txt""))

i_cirrus = cirrus$i_out

T_skin_cirrus = (i_cirrus / sigma_sb)^0.25

h_skin_cirrus = (T_ground - T_skin_cirrus) / env_lapse
```

With cirrus clouds (using the standard cirrus model),
the skin temperature is
`r format_md(T_skin_cirrus, digits = 3)` Kelvin 
and the skin height is
`r format_md(h_skin_cirrus, digits = 2)` kilometers.
which is
`r format_md(h_skin_cirrus - h_skin, digits = 1)` km higher than for 
the baseline (current conditions) and
`r format_md(h_skin_cirrus - h_skin_2, digits = 1)` km higher than for 
doubled-CO~2~ conditions.

Cirrus clouds had much bigger effect than doubling CO~2~.
"
