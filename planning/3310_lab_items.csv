lab_item_id,lab_group,lab_document_title,doc_author,doc_filename,lab_document_pdf_url,bibliography,lab_document_markdown
101,INTRO_LAB,Introduction to EES 3310/5310 Labs,,overview,/files/lab_docs/lab_01/lab_01_overview.pdf,,"# Overview of EES 3310/5310 Labs

The laboratories in this course are computational. My goals for the laboratory section are:

1. Learn about best practices for _reproducible research_ and get experience applying tools and methods for making 
   sure that your research is reliable, reproducible, and trustworthy.
   We will focus on research about climate science and climate and energy policy, but the methods and tools we will
   use are widely used in all kinds of research in natural and social sciences and also in the private sector.
2. Get experience working with real data: download and analyze data and report the results of your analysis.
3. Get experience working with computer models of different aspects of the climate system. Learn how to use models
   to do science, how to analyze and interpret the results of model simulations, and how to write reports about
   research using computational models.

## General Policies

* There will be **two** web resources for each lab: 
    * **Documentation** that describes the lab and tells you what you need to do to prepare and what you will do in the 
      lab class.
    * **An assignment** that provides a template you will use in carrying out the lab and writing it up. The assignment
      will consist of a web link for you to click on to accept the assignment in GitHub Classroom. After you accept
      the assignment, GitHub Classroom will copy the assignment into your own GitHub Classroom account.
      You will then clone the assignment from GitHub to your own computer or a computer in the lab classroom to work
      on it.

    Both the documentation and the assignment will be posted to the course web site at 
    <https://ees3310.jgilligan.org/schedule/> at least one week before the lab class.
* **Before** coming to lab:
    * Be sure to read the documentation for that week's lab.
    * Accept the assignment on GitHub Classroom (click on the Assignment link in the course web site). If you will be
      bringing your own computer to the lab, you may want to clone the assignment onto your own computer before you
      come to lab, but that's not strictly necessary. If you will be using one of the computers in the lab classroom,
      you can clone the assignment when you log in at the beginning of class.
      
* In the first lab, on September 27th, Ms. Best will explain all of the different software we will be using and will
  walk you through all the steps of using Git to work with your assignments.
   
## Schedule for the Semester

The semester is divided roughly in half. 

### First Half of the Semester:

In the first half, the readings, class sessions, and laboratories will focus 
on understanding the science of how the earth's climate system works and how human perturbations to the environment may
affect the climate. 

The weekly labs during the first half of the semester will initiially focus on exercises from the book 
_Global Warming: Understanding the Forecast_ 
and then on Sept. 24--Oct. 15 you will work in pairs to develop your own project to investigate a
question about the climate using the computer models and/or data from the major climatic data archives.

#### Extended Lab Project

The project has the following important dates:

* You and your partner will get your research topic approved by Prof. Gilligan or Ms. Best by 
  **Wednesday September 26**.
* You and your partner will present the results of your research project in the lab on **Monday, October 15th**.
* You and your partner will turn in a written report on **Wednesday October 17th**.

### Second Half of the Semester

In the second half of the semester, you will analyze data on the economies and energy use of different countries around
the world and use these to analyze different policy options for reducing greenhouse gas emissions. The labs will begin
with exercises that follow the analyses you will be reading about in the book, _The Climate Fix_, and then 
you and a partner will conduct a detailed analysis of policy options for a country of your choice to make a transition
to a cleaner energy supply.

#### Policy Analysis for a Country

The policy analysis project has the following important dates:

* You and your partner will present the results of your policy analysis in the lab on **Monday December 3rd**.
* You and your partner will turn in a written policy analysis report on **Wednesday December 5th**.


# Laboratory Classroom

The laboratory will meet in [Stevenson 2200](https://as.vanderbilt.edu/vuit/computer_services/facilities/Stevenson.php) 
(right next to the Science and Engineering Library). This classroom is
equipped with computers that have all the software you need for this class. You are also free to bring your own
laptop to the lab. Detailed instructions for installing R, RStudio, git, and (optionally) the LaTeX typesetting
system on your computer (for Windows, MacOs, and Linux) are available on the course web site at 
<https://ees3310.jgilligan.org/tools/>.

The computer laboratory is accessible using your Vanderbilt ID card 24/7 except when other classes are meeting there.
You can check the availability of the computer lab using Vanderbilt's 
[VirtualEMS web app](https://emscampus.app.vanderbilt.edu/VirtualEms/LocationDetails.aspx?data=7cZwpiiplNtjoymq%2bO2RAGKuISCBHNPXxVY8ryMN4zGmaUtIip9OXVNLod0VXhr6)
at <https://emscampus.app.vanderbilt.edu/VirtualEms/>.
Be aware that the classroom is used Monday through Thursday evening from 7:00 until 10:00 or 10:30 for Astronomy labs.

# Software Tools

We will use four principal software tools for this class. All four are free and available for Windows, MacOS, and 
Linux. Detailed instructions for downloading and installing them are available on the course web site at
<https://ees3310.jgilligan.org/tools/>:

1. R: statistical analysis software
2. RStudio: a user-friendly interface to R, which makes it much easier to use.
3. Git: a tool for keeping track of revisions in computer code and documents and coordinating working together with
   other people on a project.
4. LaTeX: This is strictly optional. It is a typesetting package that allows you to produce PDF (acrobat) documents
   from RStudio.  If you choose not to install LaTeX, you can still produce HTML (web) and DOCX (Microsoft Word) 
   documents. The downside is that a full LaTeX distribution can be quite large (several hundred megabytes), so if you
   are running low on disk space, you might not want to install it. 
   
     LaTeX is installed on the lab computers, so you will always have the option of producing PDF documents there, even
     if you don't install it on your own computer.

All of this software is installed on the lab computers in Stevenson 2200, so you do not need to install it on your own
computer, but you will need it to do the data analysis and write up your lab reports, so it will probably be convenient
for you to install your own copies.
"
102,INTRO_LAB,Introduction to Reproducible Research,,reproducible_research,/files/lab_docs/lab_01/reproducible_research.pdf,reproducible_research.bib,"# Why Do We Need Reproducible Research?

In the spring of 2012, Bruno Iksil, a securities trader at 
the investment bank JPMorgan, Chase, & Company who was also known by the
nickname ""The London Whale"" for his aggressive trades, made a series of
costly mistakes that cost JPM-Chase $6.2 billion. [@hurtado_london_2015]
Iksil was attempting to manage the financial risk of a portfolio of investments.
However, an analyst on Iksil's team had calculated the volatility (a measure of 
financial riskiness) of his portfolio using an Excel spreadsheet and made a subtle
error in a mathematical formula, dividing by the sum of two numbers instead of 
the average. That error caused Iksill to underestimate risk by a factor of 2,
and thus to expose JPM-Chase to far more risk than he realized with his enormous 
trades. [@kwak_importance_2013]

Two years earlier, 
Carmen Reinhart and Kenneth Rogoff,
two highly respected academic economists published an influential 
research paper on the effect of government debt on economic growth. 
[@reinhart_growth_2010] This paper 
concluded that when government debt exceeds 90% of GDP, the country's economic
growth is likely to abruptly come to a halt, and even slide into recession.
This paper was used to justify harsh austerity measures throughout Europe, where
nations were struggling to recover from the 2008 global economic meltdown,
and was cited by Paul Ryan as justification for his proposals to dramatically 
cut federal spending in the U.S.

Thomas Herndon, a graduate student in economics at the University of Massachusetts,
was skeptical about this research, but Reinhart and Rogoff's paper did not 
explain all the details of their data and analysis. Finally, in 2013, Reinhart
and Rogoff gave Herndon copies of the spreadsheets they had used in their analysis.
Herndon found three glaring errors in the spreadsheet, and after he fixed the
errors, there was no sudden slowdown of economic growth. 
[@krudy_how_2013; @bailey_reinhart-rogoff_2013; @kwak_importance_2013]

Such errors are not unique to economic research. In the past two years, errors 
in spreadsheet formulas led to the retraction of papers in prominent journals
of environmental science, medicine, and biology. 
[@stern_think_2017; @palus_doing_2016; @ferguson_teflon_2015]

In 2015, a major study of air pollution associated with ""fracking"" in natural
gas wells was retracted when the authors discovered a major error in an Excel
spreadsheet that they used for data analysis. [@chawla_authors_2016]

Spreadsheets are not the only source of major errors in scientific publications.
Poor statistical practices have led to what has come to be called a ""crisis of
replication"" in psychology and medicine and concern that many 
published scientific results are incorrect.

Of particular concern is the fact that major problems are being discovered in
clinical medical research. One recent review of 5,000 papers in eight top medical
journals found that almost 100 had major inaccuracies.

## Scientific Errors in Climate Science

Scientific errors have had significant impact in climate science
In the 1990s, John Christy and Roy Spencer, 
a pair of prominent climate scientists at the University of Alabama at Huntsville
(UAH),
were analyzing satellite measurements of microwave emissions from the earth's 
atmosphere and using them to calculate the temperature of different layers of
the atmosphere. They reported the surprising result that whereas measurements of
temperatures at the earth's surface, made with thermometers at meteorological
monitoring stations, consistently showed a large warming trend, the satellite
measurements found that the lower troposphere was cooling off. Christy and Spencer
claimed that their satellite measurements were more accurate than the thermometer
measurements taken at the surface and challenged scientific findings that global
warming was taking place. Controversy about the disagreement between the satellite
measurements and the surface measurements raged for years, and was the subject of
a book-length report from the National Academy of Sciences. [@nrc_reconciling_2000]

A rival team of scientists at the company Remote Sensing Systems (RSS) looked into 
the matter and conducted its own analysis of the satellite temperature records.
Christy and Spencer would not release the computer code they used to analyze
the satellite record to the public, so other scientists could not check it for
errors. However, the independent analysis by scientists at RSS revealed a number
of serious errors in the UAH analysis, including a place where the UAH team
mistakenly added two numbers instead of subtracting one from the other (an easy
mistake to make in programming, and one that might have been caught years earlier
if the code had been available for public inspection). 
[@wentz_effects_1998; @mears_effect_2005; @christy_tropospheric_2007] In the end, 
the UAH team released corrections to their satellite temperature measurements, 
and the corrected record agreed well with other measurements, including those by 
thermometers at the surface and weather balloons. The years of controversy over 
whether the lower troposphere was cooling turned out to be mostly the result of 
computer programming errors.

### The Move toward Open Climate Science

In 2009, a leak of thousands of emails by a number of climate scientists led to
a scandal known as ""Climategate,"" in which prominent climate scientists were 
accused of doctoring their data and analysis. Investigations conclusively cleared 
them of any misdeeds, and it turned out that the things they were accused of doing 
in secret had in fact been clearly reported in published papers years before the 
scandal. However, the damage to the reputation of the scientists and the public's
trust in climate science taught the climate science community the importance of
being completely open with data, methods, and computer code. 

Since then, climate scientists have moved significantly toward adopting principles
of openness. Today, pretty much all major climate data sets are available for free
on the internet. Computer code used for important analysis, including the source
code to many of the major global climate models, is publicly available (although
much of it is not much use unless you have a supercomputer to run it on).

Making all the data and code available helps win trust by convincing the public that
climate scientists do not have anything to hide. It also facilitates faster scientific
progress by allowing scientists more easily to build on one another's work, 
and it makes it easier to find and correct errors. A good list of major sources
of climate data and computer code is available at 
<http://www.realclimate.org/index.php/data-sources/>.
The R Open Science Project (<https://ropensci.org/>) maintains a number of sophisticated open-source 
scientific projects that cover many fields, such as biology and climate science.
A retired engineer and amateur climate scientist, D. Kelly O'Day, maintains 
a blog (<https://rclimate.wordpress.com/>) where he shares R scripts to download, 
analyze, and graph climate data.

## The Big Picture

For the most part, science works. Advances in all fields of science have led to 
deep understanding of nature, and have led to technological breakthroughs that
drive our economy, enable us to live much longer and healthier lives, and 
otherwise improve the quality of our lives.

Nonetheless, even if only a few percent of major scientific research papers are
wrong, this has potential to mislead us about which medicines or medical procedures
are safe and which are dangerous, about which government policies are likely to 
be effective, and in the private sector, can lead companies to make 
financially disastrous mistakes.

### Reproducible Research

Two important principles in science, which should prevent these errors,
are that research should be _transparent_
and _reproducible_: Research reports should describe the procedures clearly and
in enough detail that other scientists know exactly what was done. And scientists
who repeat the research procedures, as described in the reports, should find
similar results, within the limits of experimental uncertainty.

However, as the anecdotes above, and hundreds of similar reports of problems in
research reveal, too often even well-meaning scientists fall short of providing
enough detail about their methods for other scientists to understand their work
and catch errors, and it is often difficult to truly reproduce previously 
published research.

To address these problems, the scientific community is increasingly embracing the
principles of what has come to be called **reproducible research**.

Federal funding agencies, scientific journals, and scientific societies now call
for authors to reveal all the details of their experiments and analysis, and
must share the data and computer codes they used to perform the analyses described
in their publications.

Whether you are doing research in basic science, such as quantum physics, 
conducting clinical trials to assess the effectiveness and safety of new drugs
and medical procedures, investigating climate change, analyzing economic policy,
or working for a private company to study financial risks and opportunities, 
it will be important for you to be able to do your research accurately, to 
communicate the details clearly with your co-workers and your bosses, and
to be able to return to your old research reports and vouch for all the details of
what you did.

Whether you find yourself working in pure academic research, in public policy, 
or for private industry, the tools of reproducible research will help you do these
things effectively.

# What is Reproducible Research?

_Reproducible research_ seeks to make scientific research completely reproducible
by documenting every decision a researcher made in the course of collecting and
analyzing data. At the simplest level, this would mean that when a scientist
submits a paper to a research journal, she would include all the data and a clear
description of the analysis.

However, a written description of the analysis process might inadvertently omit crucial 
steps, or the researcher might describe what she thought she did, but might have
made errors in her actual analysis.

In the example of Reinhart and Rogoff's paper on debt and economic growth, the two
economists described what they thought they had done, but they were unaware that
their spreadsheet contained errors. For three years after they published the paper,
the errors remained buried in their spreadsheets but other economists only knew the
written descriptions of the analysis that appeared in their paper and could not
examine the spreadsheet for themselves.

Thus, reproducible research calls for researchers to share not only their data, 
but also any spreadsheets, computer programs, or scripts they used to perform
the analysis. This will allow other researchers to catch errors where the actual
analysis procedure does not match the description in the published report, just
as Thomas Herndon was able to do when he obtained Reinhart and Rogoff's spreadsheets.

## Scripts versus Spreadsheets

In principle, this should suffice, but in practice it turns out that auditing a
spreadsheet is very difficult. When you open a spreadsheet in Excel, you see
a grid of text and numbers, but the formulas used in calculating the values of
certain cells from other cells are largely invisible and it is difficult to 
read and audit every formula in a spreadsheet that contains thousands of cells.

Thus, the scientific community has become increasingly mistrustful of spreadsheets
and prefers data analysis tools that use scripts to conduct the analysis.
Scripts (basically, short computer programs) are written in a textual form that 
is straightforward for a knowledgeable person to read and understand.
Consistency checks to catch errors are much easier to implement in scripts than
in spreadsheets.

In the labs for EES 3310 and 5310, we will be using scripts for the R statistical
analysis program to analyze climate data and the output of climate models.
In the course of the semester, you will become increasingly familiar with the ideas
of using analysis scripts to promote reproducible research.

## From the Analysis to the Manuscript

Even when analysis is performed correctly, it can be difficult to transcribe
every number correctly into the manuscript of a research report. When I (Professor Gilligan)
was in graduate school, one of my professors told me a cautionary tale from early in 
his career: Three prominent physicists were attempting a very difficult calculation in
quantum electrodynamics. To guard against errors, each of the three performed the 
calculation independently and then they compared their results. They were very excited
to discover that their calculations agreed perfectly. One of them went to 
add up the different terms from the calculations and type the result into the manuscript of a
paper they rushed into print to announce and share their accomplishment. 
Somewhere in the process, he transcribed a number incorrectly, so the
published result was incorrect and the embarrassing error was not discovered until some time
after it appeared in print.

Consider, too, what happens if a research report is almost complete and the researchers
discover an error in their analysis scripts or in their raw data. After they make the
correction, they must adjust every number in their final manuscript. This introduces
additional risks of either mistyping numbers or of missing a number that needs to 
be changed.

With modern computing tools, it has become easy to integrate the analysis with
the final report.

In the EES 3310 and 5310 laboratories, we will use a tool called RMarkdown, which
allows you to combine the text of your lab reports with the scripts that perform 
your data analysis and generate the figures and tables for your report.
Thus, any time you change a number in your data or change a line of code in your
analysis script, the computer can automatically regenerate your report to 
update all of the numbers and figures accordingly.


# Elements of Reproducible Research

This section is adapted from the ""Introduction to Reproducible Research""
by the R Open Science Project, <http://ropensci.github.io/reproducibility-guide/sections/introduction/>

## Kinds of Reproducibility

Reproducibility means different things in different scientific fields.
One big distinction is between computational versus observational or empirical 
aspects of research.

* Computational reproducibility provides detailed information on exactly how computations
    (either calculations or simulations using computer models) were performed, 
    and making it possible for others to exactly reproduce those computations or calculations.
    This includes providing source code for programs and scripts written by the researcher, 
    together with detailed specifications of the software (including the specific versions used), 
    and the hardware used (running the same software on different computer hardware can 
    sometimes give different results).
    R has a function that will automatically report the computing hardware and 
    software, so it will be trivial to add this to the end of all your laboratory
    reports.
* Empirical reproducibility provides detailed information about laboratory or field
    procedures that were used to acquire empirical data used in the analysis.
    In practice, this is often accomplished by providing the raw data together with
    details about how it was collected.

In this laboratory, we will focus entirely on _computational reproducibility_.
We will not address all the details of computational reproducibility in this
laboratory, but we will focus on three important aspects:

* **Literate computing and authoring:** 
    Literate computing refers to mixing computer code with narrative description
    of what the code is doing. Stanford computer science professor Donald Knuth 
    invented literate programming based on his experience with large software 
    projects, where he found that if he wrote clear narrative descriptions of 
    what his programming code was doing, he made fewer errors, and could find
    and correct those errors more quickly. 
    
    In this laboratory, we will use RMarkdown and RStudio to apply literate
    computing and authoring in laboratory activities and writing reports.
    
* **Automation:** 
    Many of you may be used to so-called ""point-and-click"" software
    tools for statistical analysis. Examples include Excel, SPSS, and Stata.
    These tools let you perform analysis by reading in your data and then highlighting
    data with a mouse and selecting menu options.
    This approach makes it easy to get started with these software packages 
    when you are a beginner, but make it difficult to track exactly what you did
    in your analysis, so when it comes time to write up your report, you may not
    remember exactly what you did, and in what order you did it.
    Automating your analysis using scripts means that your script contains the
    complete information about everything you did.
    
    Some programs, such as Stata, allow you to record your analysis and export a 
    script (a `.do` file) that will allow you or others to reproduce your analysis.
    This is a valid form of reproducible research, but it is not the one we will
    use in this course.
    
    Automation is also important if you have to do the same operation on many different data sets.
    What seemed easy when you just had to create one graph or analysis can quickly
    become tedious as you have to drag the mouse and click on the same menu entries 
    over and over again on a dozen different data sets.
    Automating your analysis with scripts makes it easy to run the same script on 
    each of your different data sets.

* **Revision Control:**
    As you edit both your text and the R scripts you use for your analysis, 
    it is valuable to be able to keep track of changes. For instance, if your
    analysis is working well, and then you edit something and it stops working
    it is useful to be able to go back and look at what changed between the
    time when it was working and when it stopped working.
    
    Revision control systems allow you to easily keep track of changes you make
    to your files. 
    
    Another important use of revision control is not relevant to this laboratory
    section,
    but applies to larger research projects. I have computational models that are
    constantly under development and I publish papers based on them. Suppose that
    another scientist has a question about a paper that I wrote two years ago, but
    I have made many changes to the model since then. How can I go back and 
    recreate the version of the model that I used for that paper?
    Revision control systems make this very easy.
    
    Finally, revision control systems are very useful for team projects because they
    allow a team of many researchers to coordinate their activities when they are all
    editing files (computer code and text) for a project at the same time.
    
    We will be using a revision control tool called `git`. There is a web site
    called [github.com](https://github.com), which allows people to share projects.
    My students and I use github to release software that we develop in our research,
    and there is an educational site connected to github, which we will use for 
    laboratory assignments in EES 3310 and 5310.
    
    Everyone should sign up for a free student account on github.

# Software Tools

All the software you will need is installed on the computers in our laboratory classroom (SC 2200, right next to the Science and Engineering Library).
If you want to install it on your personal computer, all the software we use is free and open source.

* R <https://cran.rstudio.com/> Available for Windows, Mac OS X, and Linux

* RStudio is available in many different options. What you want is the Open Source version of RStudio Desktop, which
    you can download from <https://www.rstudio.com/products/rstudio/download/#download>. Be sure to
    install R before you install RStudio.
    
    After you install RStudio, you will almost certainly want to install several optional packages for R
    that we will use extensively for the labs. From the ""packages"" window in RStudio, click on ""Install""
    and install the following packages: ""tidyverse"", ""knitr"", ""rmarkdown"", ""xts"", ""lme4"",
    ""ggalt"", ""GGAlly"", ""ggExtra"", ""ggspectra"", ""ggspectra"", 
    ""ggstance"", ""gtable"", ""hrbrthemes"", and ""pacman""

* Git for revision control. You have three options:

    * You can download Git from <https://git-scm.com/>
    
    * There is a very popular free git client called ""Source Tree"" that has some very nice 
      graphical utilities that let you use git from Windows Explorer or Mac Finder.
      You can get Source Tree from https://www.sourcetreeapp.com/ for Mac or Windows.
      You will need to sign up for a free account at Atlassian to install it.
      
    * A third option is Git Kraken, which you can get from <https://www.gitkraken.com/>
      Git Kraken is very popular, and is free for educational, personal, and other non-commercial use.
      Git Kraken is available for Windows, Mac, and Linux.
    
    After you install Git, it is important to run two commands:
    
    * Open a git command line:

        * On Windows, open the program menu, go to ""Git"" and click on ""Git Bash""
        * On a Mac, open a terminal window
        
    * Run the two following commands:
    
        * `git config --global user.name ""Your Name""` (using your own name instead of ""Your Name"")
        * `git config --global user.email ""your.email.address@vanderbilt.edu""` (using your own email address)
        
    Git uses information to keep track of who makes changes to a file. If you are editing a file on your 
    computer and a friend is editing it on her computer, git uses this user information to keep track of 
    who made each change. Then when you and your friend merge your changes, git will be able to tell you
    which of you edited what.

## Optional Tools

If you want to produce PDF files from RMarkdown, you will need to install the LaTeX software.

If you have a Windows computer, I recommend MikTeX, which you can get from <https://miktex.org/download>

If you have a Mac, I recommend the MacTeX distribution, which you can get from <http://www.tug.org/mactex/>

On Linux, you should be able to install LaTeX using the package manager for your Linux distribution (e.g., `sudo apt-get install texlive-full` on Debian, Ubuntu or Mint; and `sudo yum install texlive` or `sudo dnf install texlive` on Fedora, CentOS, or other RedHat-based distributions)

# Walking the Walk

Over the last five years I have become increasingly convinced that reproducible
research methods are both more efficient and also lead to higher quality research.
In my own research, I use the methods and many of the tools that we will use
in this laboratory.

# Further Reading

If you are interested in learning more about reproducible research, I would recommend
the following:

* Christopher Gandrud, _Reproducible Research with R and RStudio_ (Second Edition) (CRC Press/Chapman & Hall, 2015).
  Gandrud is an economist and political scientist, who pioneered a lot of the methods that I use for 
  reproducible research as part of his Ph.D. dissertation. This book is a comprehensive how-to guide
  to reproducible research, and all of the files necessary to reproduce the book are available online at
  <https://github.com/christophergandrud/Rep-Res-Book>

* The R Open Science Project, _Reproducibility in Science: A Guide to Enhancing Reproducibility in Scientific Results and Writing_
  <http://ropensci.github.io/reproducibility-guide/>

# References
"
103,INTRO_LAB,Introduction to Git and GitHub Classroom,,git_intro,/files/lab_docs/lab_01/git_intro.pdf,,"# Revision Control for Reproducible Research

A very important part of reproducible research is using revision control.

For an excellent introduction to using git with R, I recommend 
Professor Jenny Bryan's lecture, ""Happy Git and GitHub for the useR.""
Her slides for the lecture are available at <http://happygitwithr.com/>,
and you can watch the lecture online at 
<https://www.rstudio.com/resources/videos/happy-git-and-gihub-for-the-user-tutorial/>
(It's two hours long and walks you through using git with RStudio in detail and
gives lots of practical advice)

## Tools for Revision Control

This document describes two related tools:

* Git is a program that helps you manage revisions in files that you edit (computer code, text documents, etc.), and coordinate sharing documents and working on them with other people.
* GitHub is a web site that allows people to share projects using Git. It is free for educational users and for open-source projects that anyone can see and copy. 
    GitHub also allows paying customers to have private projects. Educational users can set up private projects for free.

    **Before lab on Monday** please sign up for a free account on GitHub at <https://github.com>. I also recommend that after you sign up, you then request a student account 
    at <https://education.github.com/students>. A student account gives you access to a bunch of free extras, such as the ability to set up private projects on GitHub.

    We will use an educational feature of GitHub called GitHub Classroom to distribute the lab assignments and for you to turn them in when you have completed them.


# Installing Git on Your own Computer

Git is installed on the lab computers, but since we will use git to submit lab reports, 
it will probably be a good idea for you to install git on your personal computer
as well. There are three good options for git that work 

* If your computer is a Mac or runs Linux, Git may already be installed. You can check 
  by opening a terminal window and typing `which git`. If the computer responds with something
  like `/usr/bin/git`, then Git is already installed. Otherwise, follow the instructions below.
* For Windows and MacOS, you can download Git from <https://git-scm.com/> and install it on your computer.
* For Linux computers, you can install Git from a terminal window as follows:
    * For Debian or Ubuntu, `sudo apt-get install git`
    * For Fedora or other RedHat-type distributions, `sudo yum install git` or `sudo dnf install git`.

After you install Git, it is important to run two commands:

* Open a git command window:

    * On Windows, open the Start menu, go to ""Git"" and click on ""Git Bash""
    * On a Mac, open a terminal window
    
* Run the two following commands:

    * `git config --global user.name ""Your Name""` (using your own name instead of ""Your Name"")
    * `git config --global user.email ""your.email.address@vanderbilt.edu""` (using your own email address)
    
Git uses information to keep track of who makes changes to a file. If you are editing a file on your 
computer and a friend is editing it on her computer, git uses this user information to keep track of 
who made each change. Then when you and your friend merge your changes, git will be able to tell you
which of you edited what.

# What Git Does

Git is a very powerful tool that can do many things, and can become very confusing, even to experts.
Fortunately, we can ignore most of what git can do, and focus on a few simple things:

* Cloning a project from a remote server (e.g., github.com) to make a local copy of the file repository.
* Using file differencing to see what you changed since the last time you committed changes to your local repository
* Staging and committing changes that you made on your local computer to your local file repository
* Synchronizing Pushing changes between your local computer and a remote server.

Pretty much everything you might want to do with git, you can do from inside RStudio.

## Git Vocabulary:

* **Repository** is where git stores the history of an entire project. Git tracks every change that you make to every file in a project.

    Git can synchronize repositories in different computers. We will use github a lot for the labs in this course.
    At the beginning of a lab session, you will clone the repository for the lab from github.
    Cloning makes a copy of the repository from the remote computer to your local computer.
    After you have cloned a repository, you will have not only the current version of all the files
    in the project, but you will have the entire history of each of those files.
    
    If you are collaborating with other people, you can both edit files and then synchronize your repository
    with your partner's repository and this will let each of you see all of the changes that each of you made
    to the files.
    
    An easy way to clone remote repositories from RStudio is to go to the ""File"" menu and choose ""New Project"".
    Then choose the option, ""Version Control"". Then select ""Git"" and enter the URL for the remote repository.

* **Commit** A commit is a snapshot of all the files in a repository at some point in time. If you edit some files,
    create some new files, and delete some files, then you can commit all of those changes (edits, new files, and deleted files).
    The git repository will add the new files to the repository, note that the deleted files have been deleted, 
    and note the changes in the edited files between the latest version in the repository and the edited version that you are
    committing. Then it will then note the current state of all the files in the repository, so the commit represents a snapshot of the 
    current state of all the files in the repository when you make the commit.

* **Clone** When you start a new lab project, you will **clone** a template that I post on GitHub classroom. Cloning a project
  not only gives you a copy of the current project, but it also gives you a copy of the entire history of the project, so if you are
  working with a partner and one of you clones a project from the other, you will be able to use Git to coordinate your work and 
  each of you will be able to see everything the other one has done on the project. It is like ""track changes"" on steroids.

## Using Git with RStudio

* **File Differencing** When you are working in an RStudio project that has a git repository, 
    if you edit a file, RStudio notices that it has changed and the file appears in the ""Git"" window in RStudio.
    If you highlight that file in the ""Git"" window and click on the ""Diff"" button, RStudio will open a window where
    it will show you what changed in the file, compared to the latest version in the repository.
    
    RStudio shows the changes by identifying which lines in the file have changed, and showing the old version
    of those lines in red, and the new version in green. If you delete a line, you will just see a red line,
    and if you add a new line, you will just see a green line.
    
    If you decide that you are not happy with the changes and want to restore the file to the version that 
    existed in the repository, you can right click on the file and select ""Revert."" Be careful with this,
    because **if you revert a file, you will lose _all_ the changes you made!**
    
    
* **Staging and Committing Changes** is where you tell git to save the changes that you made to your local files into your
    local repository. Git will only record changes in files when you commit changes. Committing is a two-step process:
    first, you tell git which files you want to commit (that is, which files you want to record changes for), and then
    once you have selected the files, you tell git to commit the changes on those files to its memory in the repository.
    
    In RStudio, any file you change will show up in the ""Git"" window. Changes can be editing a file, creating a new file, 
    or deleting a file. You stage a file by checking the box next to the file name in the ""Git"" window in RStudio.
    Then you tell git to permanently remember those changes by Committing: Click on the ""Commit"" button in the ""Git"" window.
    
    Committing stores changes. If you delete a file, and then stage and commit it, the repository will note that the file is 
    now deleted. However, all of the previous versions of the file, before you committed the delete, will be stored in the
    repository. This is very useful. If you have ever been working on a project and accidentally deleted an important file,
    that can be very painful. With git, **if you committed that file to the repository, then even if you delete the file and commit
    the delete, you will be able to recover the file by checking a previous version out of the repository.**
    
    Note that the repository **only remembers the changes that you tell it to commit.** Specifically, it records the differences
    between the last version of the file that you committed and the new version that you are committing. It does not know anything about
    anything that happened between the two commits. Thus, if you edit a file but do not commit it,
    and then delete the file and commit the file as a deleted file, git will only record the fact that you deleted the file. It will
    not remember any edits that you made before you deleted it.
    
    It is a good idea to commit changes pretty frequently. Any time you hve something that is working, it's a good idea to commit.
    For instance, if you are working on a lab project that has many parts to it, as soon as you have answered one part you should
    stage and commit the changes. That way, if something goes wrong, you can recover your work from the repository.
    
    When you commit changes to a repository, git asks you to enter a comment to describe the commit. You can give a brief description
    of what you changed in the commit, or remark on the state of the files (e.g., ""Answered exercises 1-5."" or 
    ""Finally, the scripts are working properly!""). Think about what would be useful to you in helping you understand the commit
    if you are looking back over your repository history at some time in the future.

* **Synchronizing Repositories**
    Git can synchronize multiple repositories. You can **push** the changes you have made on your local repository to a
    remote repository on a server, or you can **pull** changes in the remote repository to your local computer and merge
    them into your local repository.

    When you work on the computer in the computer classroom where our lab sessions meet, your edits are stored on the local repository on
    that computer. You turn in your lab work by pushing your local repository to the remote repository at github.com. 
    Only after you have pushed your work will I be able to see what you have done.
    
    If you have a copy of the lab project repository on the computer in the computer classroom and another copy on your personal computer,
    you might make some edits on the classroom computer and some edits on your personal computer. Now you want to synchronize
    the edits you made on the two computers. You do this as follows:
    
    1. **Commit** any changes that you want to keep on the lab computer. You will not be able to proceed if there are any uncommitted changes,
        so you will need to either commit or revert any changed files.
    2. **Commit** any changes that you want to keep on your personal computer.
    3. **Pull** any changes from the remote github repository to the lab computer. If nothing has changed in the remote repository since the
        last time you synchronized the repository on the lab computer, nothing will happen. However, if anything changed in the remote 
        repository, then this will merge the changes to the remote repository with the changes that you committed on your local computer.
    4. **Push** the changes from the lab computer to the remote github repository. This will make sure that the remote repository is identical
        to the repository on the lab computer.
    5. **Pull** the changes from the github repositiroy to your personal computer. 
        This will merge the changes from the lab computer's repository (which you pushed to the github repository) 
        with the changes that you have committed on your personal computer.
    6. **Push** the changes from your personal computer to the remote repository. This will send the changes on your 
        personal computer to the remote repository on github to make sure that the remote repository on github
        is now identical to the repository on your personal computer.
    7. **Pull** the changes from the remote github repository to the lab computer. This merges the changes from your personal
        computer into the repository on the lab computer.
        
    After the last step, the three repositories (personal computer, lab computer, and github) will all be identical, and will include
    all the changes that you made on your personal computer and the lab computer.

    This may seem complicated, but you can simplify it if you follow a basic practice:
    
    * Every time you start working on a project that has a remote repository, **pull** from the remote repository before you start working.
    * Every time you have committed work that you don't want to lose, **push** to the remote repository. 
    
        This is particularly important for the lab computers because if the computer you are working on crashes or reboots, or if you log out,
        you will lose all of your files on the lab computer. **If you push your commits to the remote repository on github, then your work is
        preserved.**
        
        This also means that **if you push projects from your personal computer to a remote repository (e.g., on github), then 
        even if your personal computer breaks or gets stolen, the remote github repository will have the whole history of the 
        project, up through the last time you pushed it.**

### Conflicts

If you edit the same file on two different computers, git will attempt to merge the two sets of edits automatically. Git does a good
job with this if you edit different lines on the two computers. However, if you edit the same lines on the two computers, git doesn't 
know which version of the changed lines you want to keep.

|  Original |  Computer 1 | Computer 2 |
|:----------|:------------|:-----------|
| Mary had a little lamb | Mary had a **great big lamb** | Mary had a little lamb |
| Its fleece was white as snow | Its fleece was white as **clouds** | Its fleece was white as **milk** |
| And everywhere that Mary went | And everywhere that Mary went | And everywhere that Mary **walked** |
| The lamb was sure to go | The lamb was sure to go | The lamb was sure to go |

If you try to merge these, git can deal with the edits to the first and third lines, but 
the two computers made incompatible edits to the second line and git does not know whether to go with ""clouds"" or ""milk"".

When you pull the changes from one computer onto the other, git will complain about a conflict, and the file will look like

```
Mary had a great big lamb
<<<<<<< HEAD
Its fleece was white as clouds
=======
Its fleece was white as milk
>>>>>>> change
And everywhere that Mary walked
The lamb was sure to go
```

Then you have to manually edit the file to resolve the conflict. 
There are graphical tools to help you manage merge conflicts (this is one of the 
reasons people like to use graphical git tools like Source Tree or Git Kraken).

If you have conflicts, you will need to edit the files to resolve the conflicts
and delete the lines git uses to mark conflicts 
(the ones beginning with `<<<<<<<`, `=======`, and `>>>>>>>`).
Then you will need to stage the files where you resolved the changes
and make a commit.

# GitHub and GitHub Classroom

GitHub is a web site devoted to sharing open-source Git repositories and allowing
paying customers to operate private git repositories.
You can get a free account at <https://github.com> and as a student, you can
get some free extra features if you request a student account at 
<https://education.github.com/students>.

GitHub classroom is an add-on service that github offers for teachers, which allows 
teachers to post assignments on GitHub and then invite students to clone the assignment
and then turn in the completed assignment via a private repository.

For each lab assignment, I will create a repository on GitHub Classroom
and invite you to accept the assignment. When you accept the assignment,
GitHub will clone the assignment into private repository just for you on GitHub.
Only you, I, and the teaching assistant will be able to see your private repository.

You can then clone the private repository to your personal computer or a computer in the laboratory classroom
and complete it. As you make commits, I encourage you to push the changes back up to GitHub.
"
104,INTRO_LAB,Introduction to R and RMarkdown,,rmarkdown_intro,/files/lab_docs/lab_01/rmarkdown_intro.pdf,,"
```{r setup, include=FALSE}
knitr::knit_hooks$set(inline = function(x) { knitr:::format_sci(x, 'md')})
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(stringr)

rinline = function(code) {
  sprintf('``` `r %s` ```', code)
}

```

# RMarkdown

RMarkdown is a combination of two tools: Markdown is a simple way of writing
plain text that can be formatted into fancy documents. Markdown originated
as a tool for easily writing blog posts and comments that include formatting,
such as section  headings, italic and bold-faced text, and so forth without
having to learn a technical formatting language, such as HTML. Markdown is 
fairly generic, so it is easy to translate a single Markdown document into
many output formats, such as HTML for web pages,  PDF, and Microsoft Word documents.

You can learn a lot about the details of Markdown at the RStudio web site 
<http://rmarkdown.rstudio.com>. RStudio has a handy cheat sheet for RMarkdown 
that you can see by opening the Help menu, going to ""Cheatsheets"" and
opening the RMarkdown Cheat Sheet or the more comprehensive RMarkdown Reference.

What makes RMarkdown different from ordinary Markdown is that it allows you to 
combine text, formatted in Markdown, with instructions to the R statistical 
software to analyze data and produce graphs, tables, and other useful output.

By integrating the data analysis with the text of a document, we can easily make our 
research reproducible. The RMarkdown file contains all the instructions to 
load the data, analyze it, and generate the final report. Then if your
data changes and you need to update your report, you can do so by knitting the
RMarkdown document with the updated data files.

To turn an RMarkdown document into an HTML, PDF, or Microsoft Word document, you 
just click on the ""Knit"" button in RStudio. If you click on the word ""Knit"" on the 
button, RStudio will turn the RMarkdown document into the default format (see 
below). To knit the document into a different output format, click on the arrow 
just to the right of the word ""Knit,"" and select the output format you want.

I use RMarkdown to produce all of the documents I produce for the labs in this course.
You can see my code at <https://github.com/gilligan-ees-3310-2018>.
The code for all of the Lab #1 documentation is at 
<https://github.com/gilligan-ees-3310-2018/lab_01_documentation>

# Document header

At the top of an RMarkdown document there is a section set off by lines of three
hyphens above it and below it.

Here is an example:

```
---
title: ""Your document's title""
subtitle: ""Your document's subtitle""
author: ""Your name goes here""
date: ""Aug 28, 2017""
output:
    pdf_document: default
    html_document: default
---
```

By default, when RStudio knits the document, it converts it into the format listed
first under `output_format`

When the output format lists, for instance, ""`pdf_format: default`"", that means
that when RStudio creates a pdf document, it uses the default options. 
If you want to customize the output, you can specify
options for RStudio to use in creating PDF, HTML, or Word output documents:

```
---
title: ""Your document's title""
subtitle: ""Your document's subtitle""
author: ""Your name goes here""
date: ""Aug 28, 2017""
output:
    pdf_document:
      toc: ""true"" 
      number_sections: ""false""
    html_document:
      self_contained: ""false""
---
```



# Markdown 

Some of the basic elements of Markdown are:

## Paragraphs

Any block of one or more lines of text, with a blank line before and a blank line after
is treated as a single paragraph.  To separate paragraphs, put a blank line between them:

### Markdown:

```
This is one paragraph.
It stretches over several consecutive lines,
but it will be formatted 
as a single paragraph.

This is another paragraph.
The blank line between the two
blocks of text tells Markdown
that they are separate paragraphs
```

### Formatted Output:

> This is one paragraph.
It stretches over several consecutive lines,
but it will be formatted 
as a single paragraph.
>
> This is another paragraph.
The blank line between the two
blocks of text tells Markdown
that they are separate paragraphs

## Section headers

Any line of text that begins with one or more hash symbols (""#"") and is preceded by 
a blank line is treated as a section header. Top-level section headers have a single
hash, and subsections, subsubsections, etc. use two, three, etc. hashes.

### Markdown:

```
# This is a top-level section header

## This is a subsection

### This is a subsubsection

# This is another section
```

### Formatted Output: 

```

```

> # This is a top-level section header
>
> ## This is a subsection
>
> ### This is a subsubsection
>
> # This is another section

## Formatting text

To make _italic text_ and **boldface text** you surround the text with underscores 
or asterisks. A single underscore or asterisk means italic, two means boldface, 
and three means both italic and boldface:

### Markdown:

```
This is _italic text_. This is *also italic text*. __This is boldface__ and 
**so is this**. ***This is bold italic***. This is ~~strikethrough~~, perhaps 
to indicate an error.
```

### Formatted Output:

> This is _italic text_. This is *also italic text*. __This is boldface__ and 
> **so is this**. ***This is bold italic***. This is ~~strikethrough~~, perhaps 
> to indicate an error.


## Lists

You can make bulleted or numbered lists easily in Markdown. Simply begin a line 
with an asterisk, hyphen, or plus sign.
To make a sub-list, just indent the lines of the sublist by four spaces.

### Markdown:

```
* This is a list

* This is the second item of the list.

    * This is a sub-list

    * This is another item in the sub-list
    
        A list item can have several paragraphs. Just ident the continuation 
        by four additional spaces and do not begin it with an asterisk.
        If you have multiple lines with no blank line separating them,
        Markdown treats them as a single paragraph.

        Here is a third paragraph of continuation.

* This is the main list again.
  Just as with other things, you can break a single list item into several lines,
  and as long as there is no blank line between them, Markdown knows to treat
  them as a single paragraph.
```

### Formatted Output:

* This is a list

* This is the second item of the list.

    * This is a sub-list

    * This is another item in the sub-list
    
        A list item can have several paragraphs. Just indent the continuation 
        by four additional spaces and do not begin it with an asterisk.
        If you have multiple lines with no blank line separating them,
        Markdown treats them as a single paragraph.
        
        Here is a third paragraph of continuation.

* This is the main list again.
  Just as with other things, you can break a single list item into several lines,
  and as long as there is no blank line between them, Markdown knows to treat
  them as a single paragraph.

## Numbered Lists

To make numbered lists, start them with a number and a period:

### Markdown

```
1. This is a list

1. This is the second item of the list. Notice that I keep using the 
   numeral ""1"", but Markdown automatically increments the numbers
   for the list items.

    a) This is a sub-list

    a) This is another item in the sub-list
    
        A list item can have several paragraphs.
        
        Here is a third paragraph of continuation.
        
        i) this is a sub-sub list numbered with Roman numerals

        i) this is a sub-sub list numbered with Roman numerals

        i) this is a sub-sub list numbered with Roman numerals

1. This is the main list again.

```

### Formatted Output:

1.  This is a list

1.  This is the second item of the list. Notice that I keep using the 
   numeral ""1"", but Markdown automatically increments the numbers
   for the list items.

    a)  This is a sub-list

    a)  This is another item in the sub-list
    
        A list item can have several paragraphs.
        
        Here is a third paragraph of continuation.
        
        i)  this is a sub-sub list numbered with Roman numerals

        i)  this is a sub-sub list numbered with Roman numerals

        i)  this is a sub-sub list numbered with Roman numerals

1.  This is the main list again.


## Mathematical expressions

For simple math, you can just type stuff in  RMarkdown: `1 + 2 * 3` comes out as 
1 + 2 * 3.  If you want subscripts or superscripts, you can get those by using 
the `~` and `^` characters, respectively: `I~out~ = sigma T^4^` appears as 
I~out~ = sigma T^4^.

If you want fancier formatting for mathematical expressions, RMarkdown has a way 
to do this.

If you put an expression between dollar signs, RMarkdown interprets it differently 
from regular RMarkdown, and uses a format called LaTeX that is used for typesetting
sophisticated mathematics. I won't try to present all of LaTeX here, but will give 
some common examples:

An expression between single dollar signs is interpreted as _in-line_ math that
appears in the middle of a line of text. LaTeX uses special terms that begin with
a backslash `\` to indicate special mathematical formatting or operators.
For example `$a + b \times x$` appears
as $a  + b \times x$. 

You can show Greek letters, like $\alpha$, $\pi$, and $\sigma$ by spelling them 
out like this: `$\alpha$`, `$\pi$`, and `$\sigma$`. The epsilon that we use to 
indicate emissivity is a variation on the ordinary Greek epsilon, so we spell it
`$\varepsilon$` to get $\varepsilon$.

LaTeX handles subscripts and superscripts differently from RMarkdown:
You use `_{}` and `^{}` with whatever you want to appear in the subscript 
or superscript inside the braces `{}`:
`$I_{out} = \sigma T^{4}$` appears as $I_{out} = \sigma T^{4}$

You can include square root signs using `\sqrt`: `$x = \sqrt{y \times z}$` appears
as $x = \sqrt{y \times z}$. You can do other roots like this: 
`$T = \sqrt[4]{I_{out} / \sigma}$` appears as $T = \sqrt[4]{I_{out} / \sigma}$.

To get display math, which appears on its own line, you use double dollar signs. 
This is useful when you want to write a mathematical expression that is much taller 
than a line of text. In display mode you can display fractions using 
`\frac{numerator expression}{denominator expression}`, as in this example:
`$$\varepsilon \sigma T^4 = \frac{(1 - \alpha)}{4} \times I_{solar}$$` appears as

$$ \varepsilon \sigma T^4 = \frac{(1 - \alpha)}{4} \times I_{solar}$$
`$$T_{bare rock} = \sqrt[4]{\frac{(1 - \alpha)}{4 \varepsilon \sigma} \times I_{solar}}$$`
appears as

$$T_{bare rock} = \sqrt[4]{\frac{(1 - \alpha)}{4 \varepsilon \sigma} \times I_{solar}}$$

There is a lot more to the LaTeX notation that you can use in 
RMarkdown, but what I have shown here is sufficient for everything you will 
do in this class.
You won't need to use this kind of mathematical notation very much, but it can 
be convenient, especially when you are working exercises on atmospheric layer 
models.

If you find it too difficult to use LaTeX mathematical notation, I will accept 
either hand-written supplements in which you write out the equations by hand, or
a textual description in your document where you use words instead of mathematical
symbols to explain what you are doing.

## Figures

You can include image files in your RMarkdown document:

### Markdown

```
![This is a tornado](images/tornado.jpg)
```

### Formatted Output:

![This is a tornado](images/tornado.jpg)

## Hyperlinks

You can include links to documents on the web in two ways. The simplest
is if you want the URL for the link to appear in your document you can just 
surround the URL with angle brackets:
`<https://www.vanderbilt.edu>` appears as <https://www.vanderbilt.edu>. 

If you want to add a hyperlink to some text in your document, then you would
do it like this:

```
[Vanderbilt University](https://www.vanderbilt.edu)
``` 

to get this: [Vanderbilt University](https://www.vanderbilt.edu).

Note the difference between the hyperlink specification and the image specification
is whether there is an exclamation point before the square brackets.

# Using R for calculations

RMarkdown combines the ""Markdown"" system for formatting text with the
R statistical system for performing calculations and making graphs.

We are using R for the laboratory section of this course because it allows you to 
easily analyze large amounts of data and produce high-quality graphs.

To enter R expressions in an RMarkdown document, we use ""code blocks"" and
""inline code"". Code blocks are useful if we are doing a calculation, and 
inline code is useful if you just want to insert a number (maybe one you have
calculated in a code block) into the middle of a line of text.

Code blocks begin and end with three consecutive ""back-tick"" characters:

<pre><code>```{r code_block_name, options}
   # code goes here
```</code></pre>

Inline code appears between single back-ticks: `r rinline(""code goes here"")`

We express basic mathematical operations in R similarly to the way we express them 
in Excel and most programming languages. Addition and subtraction use the
normal plus and minus signs. Multiplication uses an asterisk, division uses
a slash (""/""), and powers (exponentiation) uses ""^"".

R has a lot of mathematical functions you can use, such as `sqrt()`, `log()` 
(for the natural logarithm), `log10()` for the base-10 logarithm, and so forth.

We can assign numbers to variables using the equals sign:
```{r stefan_boltzmann}
sigma = 5.67E-8
I_solar = 1350 # watts per square meter
albedo = 0.3
I_absorbed = I_solar * (1 - albedo)
T = (I_absorbed / (4 * sigma))^0.25
```

If the last line of a code block is something with a value (e.g., the name of a 
variable or a mathematical expression, as opposed to an assignment with an equals 
sign), then R will show that value:

```{r surface_temperature}
T
```


We can also print the value of an R expression in the middle of a line of text 
using inline code: ` T = ` `r rinline(""T"")` will give T = `r T`.

Inline code is very useful because it lets us ask R to automatically insert a 
number into the text. This means that every time we knit the document, that number 
is generated by R. If you write a report and then realize that there was a problem 
with the data or the analysis, you can just fix the problem and re-knit the report 
using the corrected data and analysis code. You don't need to manually go through 
a separate document and edit the numbers to update them with the latest results 
from your analysis. RMarkdown will do that for you, if you used inline code to 
insert the numbers into your text.

Thus, a bit of extra work at the beginning to set up the document and analysis 
using RMarkdown saves lots of time later on by making it trivial to update the 
document.

Code blocks also allow us to include graphs in our document:


<pre><code>```{r graph_example, fig.cap = ""This is a plot of pressure versus temperature.""}
ggplot(pressure, aes(x = temperature, y = pressure)) +
  geom_line() +
  geom_point() +
  labs(x = ""Temperature"", y = ""Pressure"")
```</code></pre>


```{r graph_example, echo=FALSE, fig.cap = ""This is a plot of pressure versus temperature.""}
ggplot(pressure, aes(x = temperature, y = pressure)) +
  geom_line() +
  geom_point() +
  labs(x = ""Temperature"", y = ""Pressure"")
```

## Loading R Scripts

In this course, I am trying to teach you to use the basics of R. Sometimes we will 
need to do things for our analysis that require more complicated R programming,
and then I will write R scripts that contain this code so you don't need to write
it yourself from scratch, but can just call functions that I provide.

To load a script (a file containing R code), you use the R command 
`source(""script_file.R"""")`

For the exercises here, we will use two scripts, which are located in the 
""_scripts"" directory.

To load them into R so you can use them in your analysis, you would put the 
following code into your RMarkdown document:

```{r load_scripts}
source(""_scripts/format_md.R"")
source(""_scripts/layer_diagram.R"")
```

### Useful Scripts for the Labs:

These first script defines a function, `format_md`, which formats 
numbers nicely for markdown documents, allowing you to control how many 
significant digits it shows, and optionally using scientific notation or 
inserting commas to separate thousands, millions, etc.

`format_md(sigma, digits = 1)` will produce `r format_md(sigma, digits = 1)`

`format_md(pi * 1E6, digits = 3, format=""scientific"")` will produce 
`r format_md(pi * 1E6, digits = 3, format=""scientific"")`

`format_md(pi * 1E6, digits = 3, comma = TRUE)` will produce 
`r format_md(pi * 1E6, digits = 3, comma = TRUE)`

### Script for Layer Diagrams

The other script allows you to produce a layer diagram, similar to the ones in 
Chapter 3 of _Global Warming: Understanding the Forecast_.

```{r example_layer_diagram}
make_layer_diagram(2)
```
"
105,INTRO_LAB,No more excuses for non-reproducible methods (optional),"Lenny Teytelman
",no_more_excuses,,,"Just this week, a new article about big problems in cancer research due to researchers' failure to follow reproducible research methods. Consider this article optional reading.

Here is the abstract:

> Here's a one-two punch to spark camaraderie among scientists. First, ask: ""How long did it take to get your PhD?"" Then follow up with: ""How long would it have taken if all your experiments had worked the first or second time?""
>
> Part of the probable time difference is due to inexperience, but not all of it. News last month brought a powerful reminder that access to detailed methods can be essential for getting experiments to work. In 2013, the US\$1.6-million Reproducibility Project: Cancer Biology set out to repeat key experiments from 50 high-profile cancer papers, and so assess the extent to which published results can be replicated. Instead, the project has decided to stop at 18 papers. One big reason for this was the difficulty of working out what exactly was done in the original experiments. Protocols---precise step-by-step recipes for repeating experiments---are missing from published research more often than not, and even the original researchers can have trouble pinpointing particulars years later.

You can read the full paper at <https://www-nature-com.proxy.library.vanderbilt.edu/articles/d41586-018-06008-w>

L. Teytelman, ""No more excuses for non-reproducible methods,"" Nature **560**, 411 (2018) [doi:10.1038/d41586-018-06008-w](https://doi.org/10.1038/d41586-018-06008-w)
"
201,DATA_LAB,Data Wrangling with R,,overview,/files/lab_docs/lab_02/lab_02_overview.pdf,,"```{r setup, include=FALSE}
knitr::knit_hooks$set(inline = function(x) { knitr:::format_sci(x, 'md')})
knitr::opts_chunk$set(echo = TRUE)

# This section loads necessary R libraries and sources scripts that define 
# useful functions format_md.
# 
data_dir = ""_data""
script_dir = ""_scripts""

library(pacman)
p_load(zoo, xml2, tidyverse, stringr, lubridate)

theme_set(theme_bw(base_size = 15))

# source('_scripts/utils.R', chdir = T)

if (!dir.exists(data_dir)) dir.create(data_dir)

nashville_weather <- readRDS('_data/nashville_weather.Rds')
chicago_weather <- readRDS('_data/chicago_weather.Rds')
```
# Introduction

This is a brief introduction to data types, data structures, and 
some of the functions and packages that we will use to manipulate
data in the labs.

There is a lot more, and two particular resources that I would recommend to you
are available free on the web.


## R for Data Science 

The first is the book, _R for Data Science_, by Hadley Wickham who wrote most
of the packages in the `tidyverse` collection. You can buy a print version of
the book from all the usual online sources, but Wickham has also posted the 
full text on the web at <http://r4ds.had.co.nz/> to make it available for free.
(Also, he wrote the whole book in RMarkdown, and if you're curious you can get
the RMarkdown from <https://github.com/hadley/r4ds>).

The key parts of the book, from the perspective of the labs for this course,
are Chapter 4: ""Workflow Basics,"" which presents a brief overview of R and
how to program with it; Chapter 5: ""Data Transformation,"" which explains the
functions I discuss in the first part of this handout about tibbles and the 
manipulating them with functions like `select`, `filter`, `mutate`, and
`summarize`; and Chapter 3: ""Data Visualization,"" which describes using 
the `ggplot2` package to make graphs and charts of your data.

If you are interested in learning more about R, Section II of the book
discusses the different data types that R uses in detail (tibbles,
character data (or strings), factors, dates, and times). Section III discusses
programming, and section IV discusses statistical modeling (i.e., fitting 
functions to data). Section V discusses RMarkdown and all the different ways
you can use it to communicate about your analysis with other people. 

The book is an excellent introduction to data analysis with
R. I have recommended it to many people who did not previously have experience
working with programming or R and they found it a very accessible, useful,
and user-friendly introduction.

## Online documentation for the tidyverse

_R for Data Science_ is a great introduction to the concepts behind the
`tidyverse` collection of packages and functions for R, but what should you do
when you already understand that big picture and just want to know how to do 
a specific task? For that, the online documentation for the `tidyverse` is
very useful and you can find it at <http://www.tidyverse.org/packages/>.

This page has links to the documentation for all the major tidyverse packages:
`ggplot2` for making graphics, `dplyr` and `tidyr` for working with `data.frames`
and tibbles, `reader` for reading in data from text files on the disk and
`readxl` for reading data from Excel spreadsheets, and many more packages that
we will not be using in these labs.

The documents give lots of examples showing what the functions do and explanations
of how to do many common tasks. Especially for `ggplot2`, it can be very useful
to look at the graphs in the examples to find something that looks like what 
you're trying to do and then seeing the code that made that happen.


# Data in R

## Kinds of variables

R is capable of analyzing many different kinds of data. Some of the most
important kinds of data that we may work with are:

* **Integer data**, which represents discrete quantities, such as counting
  events or objects.

* **Real number data**, which represents quantities that can have fractional values.
  Most of the data we will work with in this course, such as temperatures, 
  altitudes, amounts of rainfall, and so forth, will be real-number data.
  This kind of data is also referred to frequently as ""floating point"" data
  or (for obscure reasons having to do with computer hardware) as ""double""
  or ""double-precision"" data.

* **Character data**, which represents text. Examples include names of months,
  or categories (such as the name of a city or country). This kind of data is
  also referred to as ""string data"".

* **factor data**, which represents variables that can only take on
  certain discrete values. R treats factor data as a kind of augmented character
  data. 
  
    The difference between character data and factor data is that factor
    data has an explicit set of allowed values and has an integer number associated
    with each of those values. 
    
    For instance, if I have a factor variable with the allowed values ""up"" and 
    ""down"", then I could not assign it a value ""left"" or ""right"", whereas a
    character variable can be assigned any arbitrary text, such as 
    ""second star to the right and straight on til morning.""
    
    There are two kinds of factors: ordered and unordered. The difference is that
    the legal values for ordered factors have a specific order, so you can say 
    that one comes before or after another (or is greater than or less than another),
    whereas unordered factors don't have any natural ordering.
    
    Examples of ordered variables might be the months of the year, or the days 
    of the week, or a grouping like ""small"", ""medium"", ""large"", or 
    ""bad"", ""fair"", ""good"".
    
    Examples of unordered variables might be lists of states, gender, religion, 
    cities, sports teams, or other descriptive characteristics that don't 
    have a natural order to them.

* **date and time data**, which represents calendar dates, times of day, or 
  a combination, such as 9:37 PM on January 17, 1984.

For the most part, R handles different data types sensibly so you don't need to
worry about them, but sometimes when R is reading data in from files, or when
you want to convert one kind of variable to another, you will need to think about
these.

The most common cases where you will need to think about this is when you are 
reading data in from files. Sometimes it is ambiguous whether to treat something
from a file as character data, numerical data, or a date. In such cases, you may
need to give R guidance about how to interpret data. The functions for reading
data in from files, such as `read_csv` and `read_table` allow you to specify 
whether a given column of data in a table is integer, double-precision 
(floating point), character, date, etc.

R also provides functions for converting data. The `as.character` function
takes data that might be character, factor, or numeric, and represent it as
text (characters).

`as.numeric` or `as.integer` will allow you to convert a character variable to 
a number. For instance, `as.numeric(""3.14"")` converts a text variable ""3.14""
into a numeric variable 3.14.

`as.integer` is very useful when we want to convert an ordered factor to an 
integer that corresponds to the order of that value. For instance, if I have
an ordered factor `f` with legal values corresponding to the months of the year 
(""Jan"", ""Feb"", ..., ""Dec""), if `f` has the value `Mar`, then
`as.integer(f)` will have the integer value 3.

## Vectors, Lists, Data Frames, and Tibbles.

In statistics, you generally don't just work with one number at a time, but with
collections of numbers. R provides many ways to work with collections of numbers.

### Vectors

The simplest is a **vector**. A vector is a collection of values that are all
of the same kind: a collection of integers, a collection of floating point values,
a collection of character values, a collection of factor values, etc.

You specify vectors like this: `x = c(1, 2, 5, 9, 3, 4, 2, 7, 5)`.
You can access elements of vectors by indexing their position within the
vector, so `x[3]` will be 5 and `x[4]` will be 9.

You can also give the elements of a vector names:
`ages = c(Sam = 27, Ben = 20, Sarah = 25, Deborah = 31)` allows you to use 
`ages[""Ben""]`, which will be 20.

All of the elements of a vector have to be the same kind, so
`x = c(1, 2, ""three"")` will not allow the vector to mix numbers and characters
and R will transform all of the values to character. The result is 
`""1"", ""2"", ""three""`, and `x[1] + x[2]` will give an error because
R doesn't know how to add two character variables. However, 
`as.numeric(x[1]) + as.numeric(x[2])` will yield 3.

### Lists

Lists are a lot like vectors, but they can contain different kinds of variables.
They can even contain lists and vectors.
`x = list(1, 2, ""three"", list(4, 5, 6))` has four elements. The first two are
the numbers 1 and 2; the third is the character string ""three"", and the fourth
is the list `(4,5,6)`.

Just as we can have named vectors, we can have named lists:
`ages = list(Sam = 27, Ben = 20, Sarah = 25, Deborah = 31)`.
There is a nice shortcut to getting the elements of a named list, using a 
dollar sign: `ages$Sam` is 27.

### Data frames and tibbles

We will not use lists very much in this class. We will use vectors a little bit,
but what we will use _a lot_ are tables of data. You have probably worked a
lot with spreadsheets and other data analysis tools that organize data in a
table with rows and columns. This is a very natural and common way to work.

R provides a structure called a `data.frame` for working with tabular data, but
the package `tidyverse` introduces an improved version of the `data.frame`
called a `tibble` (think of it as a kind of data table).

`data.frames` and tibbles have rows and columns. A row represents a set of 
quantities, such as measurements or observations, that go together in some way.
Different rows in a tibble represent different sets of these quantities.

For instance, if I am measuring the height and weight of a number of people, then 
I would have a row for each person and each row would have a column for the 
person's name or identity code, a column for their height, and a column for 
their weight. 

If I am measuring the average temperature and average precipitation
for a number of cities, then I would have a column for the city, a column for the
temperature, and a column for the precipitation.

Each column of a `data.frame` or tibble should correspond to a specific kind of 
data (integer, floating point, character, factor, date, etc.). A column is 
a kind of vector, so it has to obey the restrictions that apply to vectors.

To get some experience with tibbles, let's load a couple of data sets that
I have prepared. If you have cloned the directory for this document
(from <https://github.com/gilligan-ees-3310/lab_02_documentation>),
you can load the datasets, which contain daily weather summaries for 
Nashville and Chicago,
by running the code below:

```{r load_datasets, include=TRUE}
nashville_weather = readRDS('_data/nashville_weather.Rds')
chicago_weather = readRDS('_data/chicago_weather.Rds')
```

Here is an example of the first few rows of a tibble with weather data for 
Nashville from 
`r lubridate::year(min(nashville_weather$date))`--`r lubridate::year(max(nashville_weather$date))`:

```{r head_nashville_weather, include=TRUE}
head(nashville_weather)
```

There are 6 columns: the weather station ID for the Nashville Airport,
the date of the measurement, the daily precipitation (in millimeters),
the daily minimum and maximum temperatures (Celsius), and the name of the
location.

The tibble also shows the kind of variable that each column represents:
`id` and `location` are character data, `date` is Date data, and 
`prcp`, `tmin`, and `tmax` are double-precision floating point data 
(i.e., real numbers).

In some ways, a tibble or `data.frame` is like a names list of vectors, where each
vector is a column and its name is the name of the column. We can access individual 
columns using the dollar sign, just as with regular named lists:

```{r tibble_column, include=TRUE}
precipitation = nashville_weather$prcp

head(precipitation)
```

RStudio has a nice feature that lets you examine a tibble or `data.frame`
as though it were a spreadsheet. To examine one of R's built-in data sets,
which has data on hurricanes in the Atlantic from 1975--2015: 
`View(dplyr::storms)`

Some other useful functions: 

* You can get a list of the names of
  a named vector, a named list, or the columns of a tibble or `data.frame`
  with the `names` function: `names(x)`, where x is a vector, list,
  tibble, or `data.frame`.

* You can get the length of a vector or list with the `length` function,
  and you can get the number of rows and columns in a tibble using the
  `dim` function:
  
```{r dim_len, include=TRUE}
x = c(1, 2, 3, 4, 5)
print(""Length of x is"")
print(length(x))

print(""Dimensions of dplyr::storms is "")
dim(dplyr::storms)
```
That's `r nrow(dplyr::storms)` rows and `r ncol(dplyr::storms)` columns.
You can also get just the number of rows or the number of columns
with `nrow()` and `ncol()`.

# The Tidyverse 

The ""tidyverse"" is a collection of packages written by Hadley Wickham
to make it easy to work with data frames. Wickham developed an improved 
kind of data frame that has features that are lacking in the basic R
`data.frame`, and he developed a collection of tools for manipulating, 
analyzing, and graphing data from tibbles and regular `data.frames`.

To use the tidyverse, we need to load the package using R's `library` function.
If tidyverse is not installed on your computer, you will get an error message
and you will have to run `install.packages(""tidyverse"")` before you can 
proceed.

When you load `tidyverse`, it automatically loads a bunch of useful packages
for manipulating and analyzing data: `tibble`, `dplyr`, `tidyr`, `purrr`, 
`readr`, and `ggplot2`.

If you have the `pacman` package installed, it can help you avoid these error 
messages: after you load `pacman` with `library(pacman)`, then you can load
other packages using `p_load(tidyverse)` (you can substituate any other
package name for ""tidyverse""): pacman will first see whether you have that
package on your computer; if you do, pacman will load it, and if you don't
pacman will install the package from the Comprehensive R Archive Network (CRAN)
and then load it.

In the code below, I will also load the `lubridate` package, which is part of
`tidyverse` but is not loaded automatically when you load `tidyverse`. 
`lubridate` provides useful functions for working with dates, which will
come in handy as we work with the weather data.

```{r load_tidyverse, include=TRUE}
library(tidyverse)
library(lubridate)
# alternately, I could do the following:
# library(pacman)
# p_load(tidyverse, lubridate)
```

One part of the `tidyverse` is the package `dplyr`, which has many useful 
tools for modifying and manipulating tibbles:

* `select` lets you choose a subset of columns from a tibble
* `rename` lets you rename columns
* `filter` lets you choose a subset of rows from a tibble
* `arrange` lets you sort the rows with respect to the values of different columns
* `mutate` lets you modify the values of columns or add new columns
* `summarize` lets you generate summaries of columns (e.g., the mean, maximum, 
  or minimum value of that column)
* `group_by` and `ungroup` let you perform calculations with grouping (e.g., in 
  combination with summarize, you can group by year to produce separate summaries 
  for each year)
* `bind_rows` to combine multiple tibbles that have the same kinds of columns
   by stacking one above the other.

There is a lot more, but these functions will be enough to keep us busy for now
and they will allow us to do some powerful analysis.

Let's start with `select`: You can select columns to keep or columns to delete.

Here are the first few rows of `nashville_weather`
```{r nw_head, include=TRUE}
head(nashville_weather)
```

Let's get rid of the `id` column, since we don't really care about the ID number,
that meteorological agencies use to identify the weather station. 
and let's get rid of the `location` column because we know that the data set
is from Nashville, so having that information repeated on each row is a waste
of space. 
To do this, we just  call select, specifying the tibble or `data.frame` to 
operate on, and then give a list of columns to eliminate, with a minus sign
in front of each:

```{r nw_no_id, include=TRUE}
x = select(nashville_weather, -id, -location)
head(x)
```

Alternately, instead of telling select which columns to get rid of, we can tell
it which columns to keep:

```{r nw_keep, include=TRUE}
x = select(nashville_weather, date, prcp, tmin, tmax)
head(x)
```

We can specify a range of consecutive columns by giving the first and last with a 
colon between them:

```{r nw_keep_2, include=TRUE}
x = select(nashville_weather, date:tmax)
head(x)
```

This is a general R thing: we can specify a range of numbers in a similar way:

```{r sequence, include=TRUE}
1:10
```

`rename` lets us rename columns:

```{r rename, include=TRUE}
x = rename(nashville_weather, weather_station = id, city = location)
head(x)
```

`filter` lets us select only rows that match a condition:
```{r filter_nw, include=TRUE}
x = filter(nashville_weather, year(date) > 2015 & tmax < 0)
head(x)
```
In the code above, I used the `year` function from the `lubridate` package to extract just the year from a date.

One thing that is important to know about making comparisons in `filter` expressions: to specify that two things are
equal, you write `==` with two equal signs. A single equal sign is for assigning a value to a variable and two
equal signs are for comparisons. You can also use ""<="" for less than or equal to and "">="" for greter than or equal to.

```{r filter_nw_2, include=TRUE}
x = filter(nashville_weather, date == ymd(""2016-06-10""))
x
```

In the code above, I used the `ymd` function from `lubridate` to translate a character
value ""2016-06-10"" to the date value for June 10, 2016.

You can combine conditions in `filter` by using `&` to indicate
""and"" and `|` to indicate ""or"".


We can sort the rows of a tibble or `data.frame` with the `arrange` function:

```{r arrange, include=TRUE}
x = arrange(nashville_weather, desc(tmax), tmin)
head(x,10)
```

This sorts the rows in descending order of `tmax` (i.e., so the largest values are at the top),
and where multiple rows have the same value of `tmax`, then it sorts them in ascending order
of `tmin`. Observe the three rows with `tmax` = 41.7, the two rows where `tmax` = 41.1,
and the four rows where `tmax` = 40.6.

We can use `mutate` to modify the values of columns or to create new columns.
The temperatures in Nashville weather are in Celsius and the precipitation is
in millimeters. Let's convert these to Fahrenheit and inches, respectively, and
then let's create a `trange` column that will have the difference between the maximum
and minimum temperature:

```{r mutate, include=TRUE}
x = mutate(nashville_weather, prcp = prcp / 25.4, tmin = tmin * 9./5. + 32,
           tmax = tmax * 9./5. + 32, trange = tmax - tmin)
head(x)
```

Summaries are useful for finding averages and extreme values. Let's find the maximum and minimum temperatures 
and the most extreme rainfall in the whole data set:

```{r summarize_simple, include=TRUE}
x = summarize(nashville_weather, prcp.max = max(prcp), tmin.min = min(tmin), tmax.max = max(tmax))
x
```

`nashville_weather` has `r nrow(nashville_weather)` rows, but summarize reduces it to a single summary row.

You can use summary to generate multiple summary quantities from a column:

```{r summarize_simple_2, include=TRUE}
x = summarize(nashville_weather, prcp.max = max(prcp), prcp.min = min(prcp))
x
```

We can also generate grouped summaries:

```{r summarize_grouped_1, include=TRUE}
x = ungroup(summarize(group_by(nashville_weather, year(date)), prcp.max = max(prcp), prcp.tot = sum(prcp)))
head(x)
```
This provides the maximum one-day precipitation and the total annual precipitation for each year

Note how difficult it is to read that grouped summary expression: the `group_by` function is inside
`summarize`, which is inside `ungroup`. 

The `tidyverse` offers us a much nicer way to put these kind of complicated expressions together 
using what it calls the ""pipe"" operator, `%>%`.  The pipe operator chains operations together,
taking the result of the one on the left and inserting it into the one on the right.

We can use the pipe operator to rewrite the expression above as

```{r summarize_grouped_pipes, include=TRUE}
x = nashville_weather %>% group_by(year(date)) %>% summarize(prcp.max = max(prcp), prcp.tot = sum(prcp)) %>%
  ungroup()
head(x)
```
Now the expression is easier to read: First we group `nashville_weather` by year, then we summarize it by calculating the
maximum daily precipitation and the yearly total for each year, and finally, after we summarize we ungroup.

You can combine any set of the `tidyverse` functions using the pipe operator, so you could `select` columns, 
`filter` rows, `mutate` the values of columns, and `summarize`, using the `%>%` pipe operator to connect
all of the different operations in sequence.

The final `dplyr` command we're going to look at is `bind_rows`, which lets us combine tibbles:

```{r bind_rows, include=TRUE}
weather = bind_rows(nashville_weather, chicago_weather)
```
This creates a single tibble that has all of the rows from `nashville_weather` on 
top and all the rows from `chicago_weather` on the bottom. Because the two tibbles
have the same columns, the columns are matched up.

We can operate on this combined tibble:

```{r process_combined_data_frame}
weather_summary = weather %>% 
  mutate(year = year(date), t.range = tmax - tmin) %>%
  group_by(year, location) %>%   
  summarize(prcp.max = max(prcp), prcp.tot = sum(prcp), t.range.max = max(t.range)) %>%
  ungroup() %>%
  arrange(year, location)
tail(weather_summary)
```

## Re-shaping tibbles: `gather` and `spread`

Sometimes you want to gather many columns in a data table, data frame, or tibble
into a single column. For instance, consider this data frame, which was 
read in from a spreadsheet of global temperatures produced at NASA:

```{r giss_temp_example}
giss_zonal <- readRDS('_data/giss_zonal.Rds')
head(giss_zonal)
```

The tibble presents the average temperatures for different bands of latitude:
64%deg;N--90&deg;N, 44&deg;N==64&deg;N, 24&deg;N--44&deg;N, Equator--24&deg;N,
and the same for the Southern Hemisphere.

If we wanted to plot all of these, we could do something like this:

```{r bad_zonal_plot, include=TRUE}
ggplot(giss_zonal, aes(x = year)) + 
  geom_line(aes(y = x64n_90n, color = ""64N-90N"")) +
  geom_line(aes(y = x44n_64n, color = ""44N-64N"")) +
  geom_line(aes(y = x24n_44n, color = ""24N-44N"")) +
  geom_line(aes(y = equ_24n, color = ""EQU-24N"")) +
  geom_line(aes(y = x24s_equ, color = ""24S-EQU"")) +
  geom_line(aes(y = x44s_24s, color = ""44S-24S"")) +
  geom_line(aes(y = x64s_44s, color = ""64S-44S"")) +
  geom_line(aes(y = x90s_64s, color = ""90S-64S"")) +
  labs(x = ""Year"", y = ""Temperature anomaly"")
```

This is a big mess. It would be hard to clean up the appearance, and would require a lot of 
retyping if we decided to group the data into different bands of latitude.

We can do this much more easily with the `gather` function:

```{r tidy_zonal_tibble, include=TRUE}
bands = names(giss_zonal) # column names of the tibble
bands = bands[-1] # drop the first column (""year"")
labels = c(""64N-90N"", ""44N-64N"", ""24N-44N"", ""EQU-24N"", 
           ""24S-EQU"", ""44S-24S"", ""64S-44S"", ""90S-64S"")
tidy_zonal = giss_zonal %>%
  gather(key = latitude, value = anomaly, -year)

head(tidy_zonal)
```

Now we can clean up the `latitude` column a bit to make it more friendly for
human readers:

```{r clean_zonal_tibble, include=TRUE}
tidy_zonal = tidy_zonal %>%
  mutate(latitude = ordered(latitude, levels = bands, 
                            labels = labels)) %>%
  # ^^^ the previous line converts the latitude band into an ordered factor
  # where the order is the order of the original columns. This will prevent
  # R from sorting them alphabetically when it makes the legend for the plot.
  # The ""labels"" parameter then changes the names from the somewhat cryptic 
  # original column name to something a human can read easily.
  arrange(year, latitude)

head(tidy_zonal)
```

Now let's plot it:
```{r plot_zonal_tibble, include = TRUE}
ggplot(tidy_zonal, aes(x = year, y = anomaly, color = latitude)) +
  geom_line() +
  labs(x = ""Year"", y = ""Temperature anomaly"")
```

The code for making the plot was a lot simpler, and by using an ordered factor,
we could control the order of the latitude bands in the legend, which now
appear in a sensible order. It is much easier to look at this graph and quickly
recognize that the far northern latitudes (64N-90N, and to a lesser extent
44N-64N) are warming up much faster than the rest of the planet.

Back in 1967, one of the first global climate models predicted that 
global warming due to greenhouse gases would cause the far northern latitudes 
to warm up much faster than the rest of the planet. This data confirms that
prediction.

We also see that the Southern Hemisphere has warmed much less than the Northern.
Think about why that might be.

We can also do the inverse of `gather` and spread one column of data and one
""key"" column into many columns, whose names are taken from the ""key"" column.

Let's go back to the `weather_summary` tibble we made above:

```{r head_weather_summary, include=TRUE}
head(weather_summary)
```

Let's set it up to make it easy to compare the annual precipitation of 
Nashville and Chicago:

```{r spread_weather_summary, include=TRUE}
x = weather_summary %>% select(year, location, prcp.tot) %>%
  spread(key = location, value = prcp.tot)
tail(x)
```

# Graphing Data

Here, we will look at the `ggplot2` package for plotting data. This 
package is automatically loaded when you load the `tidyverse` collection
with `library(tidyverse)`. It follows a theory of making useful graphs 
of data called, ""The Grammar of Graphics"" (that's where the ""gg"" comes from).

The idea is that a graph has several distinct parts, which come together:

* One or more _layers_ of graphics. A layer consists of the following:
  * A _data table_ with one or more columns, each corresponding to a different variable,
  * A _mapping_ of different variables (columns) in the data table to different,
    _aesthetics_ of the plot. Aesthetics are things like 
      * the _x_ coordinate,
      * the _y_ coordinate, 
      * the _color_ of the point or line, 
      * the _fill_ color that is used to fill in areas, like the interior of a rectangle or circle.
      * the _shape_ of points (e.g., circle, square, triangle, cross, diamond, ...)
      * the _size_ of points and lines
      * the _linetype_ (e.g., solid, dashed, dotted, ...)
      * and so forth ...
  * A _geometry_ (point, line, box, etc.) that is used to draw the data
* A coordinate system (axes and legends)

There are some more aspects to the gramar of graphics, but we don't need them
for what we're going to do.

A simple graph has just one layer: 

```{r simple_plot, include=TRUE, warning=FALSE}
ggplot(data = tidy_zonal, # the data
       # the mapping of variables to aesthetics
       mapping = aes(x = year, y = anomaly, color = latitude, shape = latitude)
       ) +
  geom_point() + # the geometry
  labs(x = ""Year"", y = ""Temperature Anomaly"") # labels for the coordinates
```

We can also make a plot with the same data, but two layers:

```{r second_plot, include=TRUE, warning=FALSE}
ggplot(data = tidy_zonal, # the data
       # the mapping of variables to aesthetics
       mapping = aes(x = year, y = anomaly, color = latitude, shape = latitude)
       ) +
  geom_point() + # the geometry of the first layer
  geom_line() +  # the geometry of the second layer
  labs(x = ""Year"", y = ""Temperature Anomaly"") # labels for the coordinates
```


We can also use different mappings for different layers

```{r third_plot, include=TRUE, warning=FALSE}
annual_extremes = weather %>% mutate(year = year(date)) %>%
  group_by(location, year) %>% 
  summarize(tmin = min(tmin, na.rm = T),      # the na.rm = T means to ignore missing values
            tmax = max(tmax, na.rm = T)) %>%  # if we don't put that in, then if any year has
  ungroup()                                   # a missing value for even one day, the tmax 
                                              # or tmin for that year will be recorded as 
                                              # NA (missing)
ggplot(data = annual_extremes # the data
       ) +
  geom_point(aes(x = year, y = tmin, color = location, shape = ""min"")) +
  geom_point(aes(x = year, y = tmax, color = location, shape = ""max"")) +
  xlim(1990,2000) + # set the range of the x-axis, part of the coordinate specification
  labs(x = ""Year"", y = ""Temperature Anomaly"") # labels for the coordinates
```

Note that `ggplot` issued several harmless warnings to tell us that 
setting the limits of the  _x_-axis the way we did. We can tell RMarkdown not
to include those warnings in the document by adding ""warning=FALSE""
to the options for the chunk

If we want to specify aesthetics as having fixed values, we can specify them
outside of the mapping. Here I specify the size and color of lines:

```{r fourth_plot, include=TRUE, warning=FALSE}
ggplot(data = annual_extremes # the data
       ) +
  geom_line(aes(x = year, y = tmin, group = location), color = ""dark blue"", size = 1) +
  geom_line(aes(x = year, y = tmax, group = location), color = ""dark red"", size = 0.3) +
  geom_point(aes(x = year, y = tmin, color = location, shape = ""min""), size = 2) +
  geom_point(aes(x = year, y = tmax, color = location, shape = ""max""), size = 2) +
  xlim(1990,2000) + # set the range of the x-axis, part of the coordinate specification
  labs(x = ""Year"", y = ""Temperature Anomaly"") # labels for the coordinates
```

We can also take finer control of the axis formatting:

```{r fifth_plot, include=TRUE, warning=FALSE}
ggplot(data = annual_extremes # the data
       ) +
  geom_line(aes(x = year, y = tmin, group = location), color = ""dark blue"", size = 1) +
  geom_line(aes(x = year, y = tmax, group = location), color = ""dark red"", size = 0.3) +
  geom_point(aes(x = year, y = tmin, color = location, shape = ""min""), size = 2) +
  geom_point(aes(x = year, y = tmax, color = location, shape = ""max""), size = 2) +
  scale_x_continuous(limits=c(1990,2000), breaks = c(1990, 1992, 1994, 1996, 1998, 2000)) + 
  # ^^^ the ""breaks"" parameter for an axis tells R where to put the labels
  labs(x = ""Year"", y = ""Temperature Anomaly"") # labels for the coordinates
```

And, of course, we could use `gather` to simplify this graph:

```{r sixth_plot, include=TRUE, warning=FALSE}
annual_extremes_gathered = annual_extremes %>%
  gather(key = Temperature, value = value, -year, -location)

ggplot(data = annual_extremes_gathered # the data
       ) +
  geom_line( aes(x = year, y = value, color = location, size = Temperature)) +
  geom_point(aes(x = year, y = value, color = location, shape = Temperature), size = 2) +
  scale_size_manual(values = c(tmax = 0.5, tmin = 0.1)) + # set coordinates for ""size""
  scale_x_continuous(limits=c(1990,2000), breaks = c(1990, 1992, 1994, 1996, 1998, 2000)) + 
  # ^^^ Set coordinates for the x-axis.
  scale_color_brewer(palette = ""Set1"", name = ""City"") + # one of many options for setting the color palette
                                                        # the Brewer palettes are very good for people with
                                                        # color-blindness.
  labs(x = ""Year"", y = ""Temperature Anomaly"") # labels for the coordinates
```
"
301,MODTRAN_LAB,Instructions for Lab #3,,instructions,/files/lab_docs/lab_03/lab_03_instructions.pdf,,"```{r setup, include=FALSE}
knitr::knit_hooks$set(inline = function(x) { knitr:::format_sci(x, 'md')})
knitr::opts_chunk$set(echo = TRUE)

# This section loads necessary R libraries and sources scripts that define 
# useful functions format_md.
# 
data_dir = ""_data""
script_dir = ""_scripts""

library(pacman)
p_load(xml2, tidyverse, stringr)

theme_set(theme_bw(base_size = 15))

source(file.path(script_dir, 'utils.R'), chdir = T)
source(file.path(script_dir, 'format_md.R'), chdir = T)
source(file.path(script_dir, 'modtran.R'), chdir = T)

```
# Instructions

It would be good to print these instructions and bring them to lab on 
Monday, or else to have the PDF with the instructions handy during lab.

For these exercises, I recommend that you work on them with the interactive
web-based MODTRAN models to get a feel for how the models apply to the 
exercise.

Once you are clear what you are doing, you can use the R scripts and RMarkdown
to turn those insights into reproducible research.

## Using MODTRAN with RMarkdown.

This RMarkdown document includes the line `source(""scripts/modtran.R"")`,
which loads a script with the following functions:

* `run_modtran()` allows you to automatically download a file
  with the data from a MODTRAN run. You call it with the following arguments:
  
    * `filename` is the name of the file to save the data to. I recommend
      giving it a meaningful name: for instance, a run with 550 ppm CO2 and 3.4
      ppm methane might be called ""`modtran_440_34.txt`"". Make up your own file
      names, but think about how you will tell which is which.
      
    * `co2_ppm` is the amount of CO~2~ in parts per million. The default is 400.
    
    * `ch4_ppm` is the amount of methane in parts per million. The default is 1.7.
    
    * `trop_o3_ppb` is the amount of ozone in the troposphere, in parts per billion.
      The default is 28. You probably won't change this unless you're setting all
      greenhouse gases to zero.
      
    * `strat_o3_scale` is the amount of stratospheric ozone, relative
      to the naturally occurring levels in the ozone layer. 
      You probably won't change this unless you're setting all
      greenhouse gases to zero.
      
    * `h2o_scale` is the amount of water vapor, relative to the naturally
      occurring levels in the atmosphere. 
      You probably won't change this unless you're setting all
      greenhouse gases to zero.
      
    * `freon_scale` is the amount of freon chemicals (used for refrigerators
      and air conditioners), relative to the current amounts.
      You probably won't change this unless you're setting all
      greenhouse gases to zero.
      
    * `delta_t` is the temperature offset, in degrees C.
      You adjust this to restore radiative equilibrium after you change the
      amount of CO~2~ or other greenhouse gases.
      
    * `h2o_fixed` is what quantity to hold fixed for water vapor.
      Possible values are ""vapor pressure"" (the default), and ""relative humidity""
      
    * `atmosphere` is the locality in the MODTRAN model. 
      Possible values are: ""tropical"" (the default), 
      ""midlatitude summer"", ""midlatitude winter"", 
      ""subarctic summer"", ""subarctic winter"", and 
      ""standard"" for the 1976 U.S. standard atmosphere.
      
    * `clouds` is the specification of clouds and rain.
      Possible values are ""none"" (the default),
      ""cumulus"", ""altostratus"", ""stratus"", ""stratocumulus"", ""nimbostratus"",
      ""drizzle"", ""light rain"", ""medium rain"", ""heavy rain"", ""extreme rain"",
      ""standard cirrus"", ""subvisual cirrus"", and ""NOAA cirrus"".
      
        **Stratus clouds** are flat, opaque, and low-altitude. 
        **Altostratus clouds** are flat and medium altitude.
        **Cirrus clouds** are thin and high-altitude. They are hard to model, 
        so there are three different varieties.
        **Cumulus clouds** are thick and stretch from low altitudes to medium altitudes.
        **Stratocumulus clouds** are like thunder clouds. They are very tall and 
        reach from low altitudes to the top of the troposphere.
        **Nimbostratus clouds** are low and thick, like stratus, but produce rain.
        
    * `altitude_km` is the altitude, in kilometers above sea level, that you 
      put your virtual sensor in the model.
      The default is 70 km, which is above almost all of the atmosphere.
      
        For some exercises, you may experiment with putting the sensor somewhere
        around 8 to 12 km, which is the top of the troposphere, below the stratospheric
        ozone layer.
        
        For other exercises, you might want to put it at 0 km (ground level),
        and set it to look up instead of down, so you can see the IR radiation
        coming down to the ground from the atmosphere instead of looking at the 
        IR radiation going out to space.
        
    * `looking` is the direction the sensor is looking. The options are
      ""down"" (the default) or ""up"".
      
    Any arguments you don't specify explicitly take on their default value.
    Thus, `run_modtran(file.path(data_dir, ""modtran_experiment_1.txt""), co2_ppm = 800, delta_t = 1.0, h2o_fixed = ""relative humidity"")`
    would run with all the default values, except for 800 ppm CO~2~, 
    a temperature offset of 1&deg;C, and holding relative humidity fixed.

* `plot_modtran` reads a MODTRAN output file and generates a plot.
   There are many arguments, and I won't explain them all here, but
   the important ones are:
   
    * `filename` is the MODTRAN output file with the data to use for the plot.
     
    * `descr` is an optional string to use for the title of the plot.
      If you don't specify anything, the function will make a title
      that indicates the CO2 concentration and the altitude of the virtual
      sensor.
    
    * `i_out_ref` is a reference value for the outgoing infrared. If you don't
      specify it, it's ignored, but if you specify it, then the plotting
      function adds an annotation to indicate the difference in outgoing IR
      between the current run being plotted and the reference value.
      Typically, you'd run a baseline run of MODTRAN with default parameters
      and then use the upward IR flux from that run as `i_out_ref` when you
      change the CO~2~ concentration or other model parameters.
    
    * `delta_t` is the temperature offset for this model run. If you specify it,
      the plotting function adds an annotation to indicate it.
    
    * `text_size` allows you to adjust the size of the text used for axis labels
      and the plot title.

* `read_modtran(filename)` allows you to read in a MODTRAN output file and 
  examine the data. This function returns a list with 7 elements:
  
    * `spectrum` is a data tibble with the spectral information (wavelength `lambda`,
      wavenumber `k`, outgoing IR intensity `tk`, and a number of other variables.)
    
    * `profile` is the profile of the atmosphere: a tibble with three columns:
      `Z` is the altitude in km, 
      `P` is the atmospheric pressure, in millibars, and
      `T` is the temperature in Kelvin.

    * `co2` is the atmospheric CO~2~ concentration
    
    * `ch4` is the atmospheric methane concentration
    
    * `i_out` is the intensity of the outgoing IR radiation flux.
    
    * `t_ground` is the ground temperature (in Kelvin) used in the model run.
      (Remember that this is something you set when you run the model.
      MODTRAN cannot calculate the way ground temperature changes when
      you change greenhouse gases, clouds, or other characteristics of the
      atmosphere.)
    
    * `t_tropo` is the temperature at the tropopause (in Kelvin).
    
    * `h_tropo` is the height of the tropopause (in km).
    
    * `alt` is the altitude of the virtual sensor.
    
    * `sensor_direction` is the direction of the virtual sensor (""up"" or ""down"").
    
### Converting temperature units    
    
* Some handy functions for converting temperature measurements
  from one unit of measurement to another are:

    * `ktof(T)` converts `T` from Kelvin to Fahrenheit.
    
    * `ktoc(T)` converts `T` from Kelvin to Celsius.
    
    * `ftok(T)` converts `T` from Fahrenheit to Kelvin.
    
    * `ctok(T)` converts `T` from Celsius to Kelvin.
    
    * `ctof(T)` converts `T` from Celsius to Fahrenheit.
    
    * `ftoc(T)` converts `T` from Fahrenheit to Celsius.
    
    But be aware that if you want to convert the _difference between two
    temperatures_, you need to convert the temperatures and then take the
    difference:
    
```{r delta_temperature}
    t1_k = 254 # Kelvin temperature
    t2_k = 288 # Kelvin temperature
    delta_t_k = t2_k - t1_k # Difference in temeprature, in Kelvin

    delta_t_k
    
    t1_f = ktof(t1_k) # Fahrenheit temperatures
    t2_f = ktof(t2_k)
    
    t1_f
    t2_f
    
    delta_t_f = t2_f - t1_f # Difference in temperature, in Fahrenheit
    
    delta_t_f
    
    # This will give the wrong answer for the 
    # temperature difference in Fahrenheit!
    ktof(delta_t_k)
```
You see that $`r t2_f` - `r t1_f` \neq `r ktof(delta_t_k)`$.

* Some variables that I have defined for you are:

    * `sigma_sb` is the Stefan-Boltzmann constant.
    
    * `solar_constant` is the Solar Constant (the intensity of sunlight at the 
      top of the atmosphere).

## Examples:

```{r run_modtran_baseline, include=TRUE, message=FALSE, warning=FALSE}
run_modtran(filename = file.path(data_dir, ""modtran_baseline.txt""))

modtran_baseline = read_modtran(file.path(data_dir, ""modtran_baseline.txt""))

# Here is how you extract the various values from modtran_baseline:
baseline_i_out <- modtran_baseline$i_out
baseline_t_trop <- modtran_baseline$t_trop
```

The baseline MODTRAN run has 
$I_{\text{out}} = `r format_md(baseline_i_out, digits = 2, output_format = ""latex"")`$
and 
$T_{\text{tropopause}} = `r format_md(baseline_t_trop, digits=1, output_format = ""latex"")`$.

```{r plot_modtran_baseline, include=TRUE, message=FALSE, warning=FALSE}
plot_modtran(file.path(data_dir, ""modtran_baseline.txt""))
```

```{r double_co2, include=TRUE, message=FALSE, warning=FALSE}
run_modtran(filename = file.path(data_dir, ""modtran_double_co2.txt""), 
            co2_ppm = 800)
plot_modtran(file.path(data_dir, ""modtran_double_co2.txt""), 
             i_out_ref = baseline_i_out, delta_t = 0)
```

```{r double_co2_warming, include=TRUE, message=FALSE, warning=FALSE}
run_modtran(filename = file.path(data_dir, ""modtran_double_co2_warming.txt""), 
            co2_ppm = 800, delta_t = 0.76)
plot_modtran(file.path(data_dir, ""modtran_double_co2_warming.txt""), 
             i_out_ref = baseline_i_out, delta_t = 0.76)
```

# A few new R functions that we will use in this lab:

## Iterating over a series

Sometimes you want to repeat something in R, executing the same commands for
many different values of a variable. We can do this with the `for` command:

```{r loop, include = TRUE, fig.height=3, fig.width=3}
df = tibble(x = 1:10)

for (i in 1:4) {
  p = ggplot(df, aes(x = x, y = x^i)) + 
    geom_point() + geom_line() +
    labs(x = ""x"", y = str_c(""x to the power "", i))
  plot(p)
}
```

## Combining character variables

R has many functions for manipulating text. When R stores text, it stores it
in character variables (these are also sometimes called ""strings"" because
text is like a string of characters).
For instance, we might want to make a label or a filename by combining several 
variables. 
Three functions that we can use are `str_c`, from the `stringr` package and 
`paste` and `paste0`,
from basic R. All of these work pretty much the same way:

```{r combining_strings, include=TRUE}
print(paste(""mail"", ""box""))
print(paste(""mail"", ""box"", sep = """"))
print(paste0(""infra"", ""red""))
print(str_c(""infra"", ""red""))
print(str_c(""infra"", ""red"", sep = ""-""))
print(str_c(""one"", ""two"", ""three"", ""four"", sep = "", ""))
print(str_c(10, "" km""))

x = 50

print(str_c(x, "" Watts""))
print(str_c(x, ""Watts"", sep = "" ""))
```
Notice how `paste` puts spaces between the strings when it combines them unless
you specify that `sep` (the separator) should be something different.
`paste0` works just like `paste`, except that it doesn't have a separator, so
the variables are combined without an extra space.
`str_c` is like `paste0`, except that you can specify a separator if you do
want something in between the different variables.

## Calculating with leads and lags

Sometimes, when we are using `mutate` with a data tibble, we might want to
look at differences between a row and the row before or after it in the
tibble. We can do this with the `lead` and `lag` functions:

In the examnple below, the column `u` gets the value of the current row of
`y` minus the previous row of `y`, and the column `v` gets the value of the
next row of `y` minus the current row of `y`. Note that where there isn't a
previous row, `lag` returns `NA` (missing value), and similarly for `lead`
when there isn't a next row.

```{r lead.lag, include=TRUE}
tbl = tibble(x = 0:5, y = x^2)

tbl = tbl %>% mutate(u = y - lag(y), v = lead(y) - y)
tbl
```

If you want to lead or lag by more than one row, you can just say, `lag(y, 5)`
to get the value of `y` 5 rows before the current one.

```{r lead.lag.2, include = TRUE}

tbl = tibble(x = 1:10)

tbl = tbl %>% mutate(before = lag(x), after = lead(x), 
                     before.2 = lag(x, 2), after.3 = lead(x, 3))

tbl
```

## Modifying _x_ and _y_ axes in `ggplot`

It is easy to modify the _x_ or _y_ axis in `ggplot`. For instance, if
you want to put specific limits on the axis, or change where the labels
go, you can use `scale_x_continuous` or `scale_y_continuous`:

```{r simple_plot, include=TRUE, fig.height=3, fig.width=4}
tbl = tibble(x = 1:200, y = (x / 100)^5)

ggplot(tbl, aes(x = x, y = y)) + geom_line()
```

```{r scale_x, include=TRUE, warning=FALSE, fig.height=3, fig.width=4}
ggplot(tbl, aes(x = x, y = y)) + geom_line() +
  scale_x_continuous(limits = c(0,150), breaks = seq(0, 150, 25)) +
  scale_y_continuous(limits = c(0,10))
```

```{r log_plot, include=TRUE, fig.height=3, fig.width=4}
tbl = tibble(x = 1:200, y = 5 - 2 * x + 3 * x^2)

# Note that in R when we are typing numbers, we can express scientific notation 
# as 1E6 for 1,000,000 2.67E-3 for 0.00267

ggplot(tbl, aes(x = x, y = y)) + geom_line() +
  scale_x_log10(limits = c(1,1000)) +
  scale_y_log10(limits = c(1,1E6))
```

# Exercises for Lab #3

## Chapter 4 Exercises

### Exercise 4.1: Methane

Methane has a current concentration of 1.7 ppm in the atmosphere and
is doubling at a faster rate than CO~2~.

a) Would an additional 10 ppm of methane in the atmosphere have a larger or smaller
  impact on the outgoing IR flux than an additional 10 ppm of CO~2~ at current
  concentrations?

b) Where in the spectrum does methane absorb? What concentration does it take to
  begin to saturate the absorption in this band? Explain what you are looking 
  at to judge when the gas is saturated.

    **Suggestion:**
    
    * Run MODTRAN with no greenhouse gases, except 0.4 ppm of methane.
    * Run MODTRAN several times, successively doubling the amount of methane:
      0.4 ppm, 0.8 ppm, 1.6 ppm, ... 102.4 ppm.
      
        Hint: You can use the following R commands to do this:
        
        ```
        methane_data = tibble() # create a blank data tibble
    
        for (x in 0:11) {
          # Repeat everything between the braces ""{}"" for x taking on
          # each value in 0, 1, 2, ..., 11.
          
          p_methane = 0.4 * (2^x) # methane concentration is 0.4 times 2 to the 
                                  # power of x.
          
          # Create a character variable that will be a file name of the form
          # file.path(data_dir, ""methane_xx_x.txt""), where xx_x is the methane 
          # concentration, with an underscore for the decimal point.
          file_name = formatC(p_methane, digits = 1, decimal.mark = ""_"", 
                              format = ""f"") %>%
                      str_c('methane_', ., "".txt"") %>%
                      file.path(data_dir, .)
    
          # Now run MODTRAN
          run_modtran(file_name, co2_ppm = 0, ch4_ppm = p_methane, 
                      trop_o3_ppb = 0, strat_o3_scale = 0, h2o_scale = 0, 
                      freon_scale = 0, 
                      delta_t = 0, h2o_fixed = ""vapor pressure"",
                      atmosphere = ""tropical"", clouds = ""none"", 
                      altitude_km = 70, looking = ""down"")
    
          # Read the MODTRAN results into R
          results = read_modtran(file_name)
    
          # Create a data tibble with columns for the methane concentration
          # and I out, and append it to the end of the tibble methane_data
          df = tibble(methane = results$ch4, i_out = results$i_out)
          methane_data = bind_rows(methane_data, df)
        }
        ```
        
        This will run MODTRAN for the different values of methane concentration and
        save them in the ""_data"" folder as ""methane_0_4.txt"", ""methane_0_8.txt"", 
        ""methane_1_6.txt"", and so forth, up to ""methane_819_2.txt"", and also
        create a data tibble `methane_data` with a list of methane concentrations
        and the corresponding $I_{\text{out}}$.
    
    *   Use `mutate` to add a new column `change`, which contains the change in 
        $I_{\text{out}}$ between the previous row and this one. You can use the
        `lag` command to calculate this, as described above in the ""new R
        functions"" section.
        ```
        methane_data = methane_data %>% mutate(change = i_out - lag(i_out))
        ```
        
        Now plot `i_out` versus the methane concentration several ways:
        
        * First, just plot `i_out` versus `methane`:
          ```
          ggplot(methane_data, aes(x = methane, y = i_out)) + 
            geom_point(size = 2) +
            geom_line(size = 1) +
            labs() # add parameters to labs to label your axes.
          ```
    
        * Next, plot the same data, but with a logarithmic _x_-axis (use
          `scale_x_log10`, as described above in the ""New R Functions"" section)
    
        * Next, plot `methane_concentration`, but assign the column `change`
          to the _y_ axis, instead of the column `i_out`.
      
        * Think back to the slides I showed in class #6 about identifying band
          saturation. Do you see a place where the successive changes in `i_out` 
          flatten out?  Estimate the concentration of methane where absorption
          saturates.

c) Would a doubling of methane have as great an impact on the heat balance as a 
   doubling of CO~2~?

    **Suggestion:** 
    
    * Run MODTRAN in its default configuration (400 ppm CO~2~ and 1.7 ppm methane)
    * Run it again with 10 ppm of extra methane
    * Run it again with the default methane (1.7 ppm) but 10 ppm extra CO~2~.
    * Compare $I_{\text{out}}$ for the three runs.

d) What is the ""equivalent CO~2~"" of doubling atmospheric methane? That is to say,
   how many ppm of CO~2~ would lead to the same change in outgoing IR radiation
   energy flux as doubling methane? What is the ratio of ppm CO~2~ change to 
   ppm methane change?

    **Suggestion:** This is easier to do interactively with the web-based 
    interface to MODTRAN than by running it in R.
    
    * Run MODTRAN in its default configuration (400 ppm CO~2~ and 1.7 ppm methane)
    * Run MODTRAN again with the methane doubled. Note $I_{\text{out}}$. 
    * Return methane to the default value (1.7 ppm), and adjust CO~2~ until
      $I_{\text{out}}$ is the same as it was for the doubled methane.
      Note what concentration of CO~2~ does this.
    * Now you can use R to run MODTRAN with doubled methane and with the equivalent 
      concentration of CO~2~, and save these runs to the disk.

### Exercise 4.2: CO~2~ (Graduate students only)

a) Is the direct effect of increasing CO~2~ on the energy output at the top of
   the atmosphere larger in high latitudes or in the tropics?
   Compare the change in $I_{\text{out}}$ from doubling CO~2~ with
   the atmosphere set to `tropical`, `midlatitude summer`, and 
   `subartcic summer`.
   
    For each atmosphere, first record $I_{\text{out}}$ with CO~2~ at 400 ppm
    and then record the change when you increase CO~2~ to 800 ppm.

b) Set pCO~2~ to an absurdly high value of 10,000 ppm. You will see a spike
   in the CO~2~ absorption band. What temperature is this light coming from? 
   Where in the atmosphere do you think this comes from?

    Now turn on clouds and run the model again. Explain what you see.
    Why are night-time temperatures warmer when there are clouds?

### Exercise 4.3: Water vapor

Our theory of climate presumes that an increase in the temperature at ground
level will lead to an increase in the outgoing IR energy flux at the top of the
atmosphere.


a) How much extra outgoing IR would you get by raising the temperature of the 
   ground by 5&deg;C? What effect does the ground temperature have on the 
   shape of the outgoing IR spectrum and why?
   
    * Note the $I_{\text{out}}$ for the default conditions. Then
      set `delta_t` to 5 and run MODTRAN again, and note the new value
      of $I_{\text{out}}$.
      
    * Plot the spectrum for both runs and compare.

b) More water can evaporate into warm air than into cool air. Change the
   model settings to hold the water vapor at constant relative humidity 
   rather than constant vapor pressure (the default), calculate the change
   in outgoing IR energy flux for a 5&deg;C temperature increase.
   Is it higher or lower? Does water vapor make the Earth more sensitive to
   CO~2~ increases or less sensitive?

c) Now see this effect in another way. 

    * Starting from the default base case, record the total outgoing 
      IR flux. 

    * Now double pCO2. The temperature in the model stays the
      same (that's how the model is written), but the outgoing IR flux
      goes down.

    * Using constant water vapor pressure, adjust the temperature offset
      until you get the original IR flux back again. Record the change in
      temperature
    
    * Now repeat the exercise, but holding the relative humidity fixed
      instead of the water vapor pressure.
    
    * The ratio of the warming when you hold relative humidity fixed
      to the warming when you hold water vapor pressure fixed is the 
      feedback factor for water vapor. What is it?

## Chapter 5 Exercise

### Exercise 5.2: Skin Height

a) Run the MODTRAN model in using the ""Tropical"" atmosphere, without clouds, and with
   present-day pCO~2~ (400 ppm). Use the ground temperature reported by the model to calculate
   $\varepsilon \sigma T_{\text{ground}}^4$, the heat flux emitted by the ground.
   Assume $\varepsilon = 1$, and I have already provided the value of the 
   Stefan-Boltzmann constant $\sigma$, as the R variable `sigma_sb`,
   which equals `r format_md(sigma_sb, digits = 3)`.
   (I defined it in the script ""utils.R"", which I loaded in the ""setup"" chunk
   in the RMarkdown document).

    Next, look at the outgoing heat flux at the top of the atmosphere (70 km) 
    reported by the MODTRAN model. Is it greater or less than the heat flux
    that you calculated was emitted by the ground?

b) Use the outgoing heat flux at the top of the atmosphere to calcuate the
   skin temperature (use the equation 
   $I_{\text{out}} = \varepsilon \sigma T_{\text{skin}}^4)$).
   What is the skin temperature, and how does it compare to the ground 
   temperature and the temperature at the tropopause, as reported by the 
   MODTRAN model?
   
    Assuming an environmental lapse rate of 6K/km, and using the 
    skin temperature that you calculated above, and the ground temperature
    from the model, what altitude would you expect the skin height to be?

c) Double the CO~2~ concentration and run MODTRAN again. Do not adjust the
   ground temperature. Repeat the calculations from (b) of the skin
   temperature and the estimated skin height.

    What is the new skin temperature? What is the new skin height?

d) Put the CO~2~ back to today's value, but add cirrus clouds, using the
   ""standard cirrus"" value for the clouds. Repeat the calculations from (b) of
   the skin temperature and the skin height.
   
    What is the new skin temperature? What is the new skin height?
    Did the clouds or the doubled CO~2~ have a greater effect on the
    skin height?
"
401,CLOUD_LAPSE_LAB,Instructions for Lab #4,,instructions,/files/lab_docs/lab_04/lab_04_instructions.pdf,,"```{r setup, include=FALSE}
knitr::knit_hooks$set(inline = function(x) { knitr:::format_sci(x, 'md')})
knitr::opts_chunk$set(echo = TRUE, include = TRUE, cache = FALSE)

# This section loads necessary R libraries and sources scripts that define 
# useful functions format_md.
# 
data_dir = ""_data""
script_dir = ""_scripts""

library(pacman)
p_load(tidyverse, jsonlite, httr, magrittr, scales, xml2)

theme_set(theme_bw(base_size = 10))

source(file.path(script_dir, ""utils.R""), chdir = T)
source(file.path(script_dir, ""format_md.R""), chdir = T)
source(file.path(script_dir, ""modtran.R""), chdir = T)
source(file.path(script_dir, ""rrtm.R""), chdir = T)
```
# Exercises with lapse rate, clouds, and water-vapor feedback.

In this lab, we will continue using MODTRAN and will introduce a new
climate model, RRTM. RRTM, which stands for ""Rapid Radiative Transfer Model,""
is a radiative-convective model that uses code
from the radiative-transfer portion of a state-of-the-art globam climate model
called the ""Community Climate System Model,"" developed at the National Center
for Atmospheric Research in Boulder CO.

The entire CCSM model runs on giant supercomputers, but this radiative transfer
module can run efficiently on an ordinary computer. In order to speed up the
calculations, RRTM does not calculate the entire longwave spectrum the way
MODTRAN does, but uses a simplified approximation that is much faster when 
the big climate models need to run this radiative transfer calculation 
roughly 52 quadrillion ($5.2 \times 10^{15}$) times in a simulation of 100 years
of the earth's climate.

An advantage that RRTM has over MODTRAN, is that MODTRAN assumes that the 
atmosphere is static (none of the air moves), whereas RRTM allows for 
convective heat flow. This makes RRTM more realistic, even though it sacrifices
detail in its treatment of longwave radiation.

We will use RRTM to explore the role of convection in the earth system
and to examine the water-vapor feedback in the presence of convection.

You can run the RRTM model interactively on the web at 
<http://climatemodels.uchicago.edu/rrtm/index.html>
and I have also written a script that allows you to run it from R.

To run the model interactively, you can adjust various parameters, such as the
brightness of the sun, the albedo (it gives you a choice of many natural and
human-made surfaces, such as asphalt, concrete, forest, grassland, snow, ocean,
and the average for the earth) the concentrations of CO~2~ and methane,
the relative humidity, and the amount and type of high (cirrus) and 
low (stratus) clouds.

You can also introduce aerosols typical of different parts of the earth, such as
cities (with soot, sulfates, and other pollution), deserts (with blowing dust),
oceans (with sea spray and salt), and a Pinatubo-like volcanic eruption.

Like MODTRAN, the model does not automatically adjust the surface temperature. 
Instead, it calculates the upward and downward flux of longwave and shortwave 
radiation at 51 different levels of the atmosphere and reports whether the heat
flow is balanced (heat in = heat out) at the top of the atmosphere.

If the earth is _gaining_ heat, you can manually _raise_ the surface temperature 
until you balance the heat flow, and if the earth is _losing_ heat, you can
manually _lower_ the temperature.

## R Interface to RRTM

I have written an R function `run_rrtm` that allows you to manually run 
RRTM from R. To use this function, you need to include the line
`source(""_scripts/rrtm.R"")` or `source(file.path(script_dir, ""rrtm.R""))`
to load it.

* `run_rrtm()` allows you to automatically download a file
  with the data from a MODTRAN run. You call it with the following arguments:
  
    * `filename` is the name of the file to save the data to. The function
      returns the output data, so it's optional to specify a filename.
      
    * `co2_ppm` is the amount of CO~2~ in parts per million. The default is 400.
    
    * `ch4_ppm` is the amount of methane in parts per million. The default is 1.7.
    
    * `relative_humidity` is the relative humidity, in percent. The default is
      80%.
      
    * `T_surface` is the surface temperature, in Kelvin.
      The default (for 400 ppm CO~2~, etc.) is 284.42.
      You adjust this to restore radiative equilibrium after you change the
      parameters (amount of CO~2~, lapse rate, etc.).
      
    * `I_solar` is the brightness of the sun, in Watts per square meter.
      The default value is 1360.
      
    * `surface_type` is the type of surface (this is used to calcualte the 
      albedo). The default is `earth average. The options are:
      
      * ""`earth average`"": The average albedo of the earth (0.30)
      * ""`asphalt`"": Dark asphalt (0.08)
      * ""`concrete`"": Concrete (0.55)
      * ""`desert`"": Typical desert (0.40)
      * ""`forest`"": Typical forest (0.15)
      * ""`grass`"": Typical grassland (0.25)
      * ""`ocean`"": Ocean (0.10)
      * ""`snow`"": Typical snow (0.85)
      * ""`ice`"": Large ice masses covering ocean or land (0.60)
      * ""`soil`"": Bare soil (0.17)
      * ""`custom"": Custom albedo (if you choose this, you need to also supply 
        a value for `albedo`)
      
    * `tropopause_km` is the altitude of the tropopause, in kilometers above 
      sea level. The default value is 15. On the earth, the tropopause varies 
      from around 9 km at the poles to around 17 km near the equator.
        
    * `lapse_rate` is the lapse rate, in Kelvin per kilometer. The default is
      6. The dry adiabatic lapse rate is 10, so it's physically impossible to
      have a lapse rate greater than 10 and results with `lapse_rate` 
      greater than 10 won't make sense.
      
    * `low_cloud_frac` is the fraction (from 0--1) of the sky covered by 
      low (stratus) clouds. The default is 0.
      
    * `high_cloud_frac` is the fraction (from 0--1) of the sky covered by 
      high (cirrus) clouds. The default is 0.
    
    * `cloud_drop_radius` is the size of the water droplets in the clouds,
      in microns. The default is 10. (For reference, 10 microns is about the
      size of a red blood cell).
      You can reduce this to simulate the indirect aerosol effect.
    
    * `aerosols` allows you to set up the atmosphere with the kinds and 
      quantities of aerosols typical of a number of different environments.
      Options are:
      
        * ""`none`"": No aerosols
        * ""`ocean`"": Typical ocean aerosols (sea-spray, salt, etc.)
        * ""`desert`"": Typical desert aerosols (dust, sand)
        * ""`city`"": Typical city with soot (black carbon) and sulfate aerosols.
        * ""`city just sulfates`"": Just sulfate aerosols typical of a city.
        * ""`city just soot`"": Just soot (black carbon) aerosols typical of a 
          city.
        * ""`land`"": Typical rural land (dust, etc.)
        * ""`polluted land`"": Typical rural land suffering from pollution 
          (e.g., from farming)
        * ""`antarctic`"": Typical aerosols for Antarctica
        * ""`volcano`"": Similar sulfate and dust to the Mt. Pinatubo volcanic 
          eruption.

    Any arguments you don't specify explicitly take on their default value.
    Thus, `run_rrtm(co2_ppm = 800, relative humidity = 10, T_surface = 300)`
    would run with all the default values, except for 800 ppm CO~2~, 
    relative humidity of 10%, and a surface temperature of 300 Kelvin.

    `run_rrtm` returns a list of data containing:
    
    * Basic parameters of the model run:
      * `T_surface`
      * `co2_ppm`
      * `ch4_ppm`
      * `I_solar`
      * `albedo`
      * `lapse_rate`
      * `tropopause_km`
      * `relative_humidity`
      * `aerosols`,
      * `low_cloud_frac`
      * `high_cloud_frac`
      * `cloud_drop_radius`
    * Results of the model calculations:
      * `Q`: The heat imbalance $I_{\text{in}} - I_{\text{out}}$
      * `i_in`: The net solar radiation absorbed by the earth 
        ($(1 - \alpha) I_{\text{solar}} / 4$)
      * `i_out`: The net longwave radiation emitted to space from the top 
        of the atmosphere
      * `profile`: A tibble containing a profile of the atmosphere 
        (altitude in km, pressure in millibar, and temperature in Kelvin)
      * `fluxes`: A tibble containing the fluxes (in Watts per square 
        meter) of longwave, shortwave, and total radiation going up and 
        down at 52 levels from the surface to the top of the atmosphere.
        The columns are `altitude` (km), `T` (temperature in K), 
        `P` (pressure in millibar), `sw_up` (upward shortwave), `sw_down`
        (downward shortwave), `lw_up` (upward longwave), `lw_down`
        (downward longwave), `total_up` (`sw_up` + `lw_up`), and
        `total_down (`sw_down` + `lw_down`).

There are also functions for reading RRTM data files and plotting RRTM data:

* `read_rrtm(file)` reads an RRTM file saved by `run_rrtm` and returns a 
  list of data just like the one returned by `run_rrtm`.

* `plot_heat_flows()`: plots the upward and downward fluxes of radiation from
  an RRTM file or data structure. You can call it either with 
  `plot_rrtm(file = ""filename"")` or `plot_rrtm(data = rrtm_data)`, where
  ""`filename`"" and ""`rrtm_data`"" stand for your own filename or rrtm data
  structure returned by `run_rrtm` or `read_rrtm`.
  
    You can also specify which wavelengths to plot. By default, it plots
    shortwave (SW), longwave (LW), and total (SW + LW), but you can specify
    one or more of `sw = FALSE`, `lw = FALSE`, or `total = FALSE` to 
    omit wavelengths.

## Example of running RRTM

Here is an example of running RRTM:

```{r rrtm_example}
default_rrtm = run_rrtm()

# Surface temperature:
default_rrtm$T_surface

# Heat imbalance:
default_rrtm$Q
```

This run has surface temperature 
`r format_md(default_rrtm$T_surface, digits = 2)` K and a heat imbalance of 
`r format_md(default_rrtm$Q, digits = 2)` Watts per square meter.

### Interpreting RRTM Results

We can plot the heat flows as a function of altitude:
```{r plot_rrtm_example}
plot_heat_flows(default_rrtm)
```

What you see in this plot are thick lines representing downward heat flow
and thin lines representing upward flow. The different colors represent
shortwave, longwave, and total (shortwave + longwave).

A few things to notice: At the top of the atmospere, at 
`r format_md(max(default_rrtm$fluxes$altitude), digits = 0)` km, 
there is very little longwave going down, but a lot of shortwave going down 
(around `r format_md(tail(default_rrtm$fluxes$sw_down, 1), digits = 0)` W/m^2^).
Conversely, there is a modest amount of shortwave going up
(around `r format_md(tail(default_rrtm$fluxes$sw_up, 1), digits = 0)` W/m^2^),
but a lot of longwave going up (around 
`r format_md(tail(default_rrtm$fluxes$lw_up, 1), digits = 0)` W/m^2^).

The upward shortwave radiation is sunlight reflected from the atmosphere and 
the earth's surface.

The upward longwave radiation is emitted from the surface and the atmosphere.
You can see that the longwave radiation, both up and down, is greater closer to 
the surface, where temperatures are warmer, and smaller at higher altitudes, 
where the atmosphere is cooler.

If we look at the total radiation, we see that there is a good balance near the
top of the atmosphere (the upward and downward lines come together), but 
in the lower atmosphere, there is a serious imbalance with downward fluxes 
significantly larger than the upward ones.

This is a consequence of convection: The difference between the downward and 
upward radiative fluxes is taken up by convection, which moves heat upward
when warm air rises and cool air sinks.

### Determining Climate Sensitivity with RRTM

We can also use the RRTM model to study what happens when we double CO~2~:
```{r rrtm_warming}
rrtm_double_co2 = run_rrtm(co2_ppm = 800)
```
```{r rrtm_doubled_co2_balanced, echo=FALSE, include=FALSE}
new_ts = 286.9 # Kelvin
rrtm_double_co2_balanced = run_rrtm(co2_ppm = 800, T_surface = new_ts)
```
When we double CO~2~ without changing the surface temperature
(T~surface = `r format_md(rrtm_double_co2$T_surface, digits = 2)` K), 
this creates a heat imbalance of `r format_md(rrtm_double_co2$Q, digits = 2)`
W/m^2^. We can use the online interactive version of RRTM to adjust surface
temperature until the heat flows balance. The surface temperature where this
happens is `r format_md(new_ts, digits = 2)` K and we can paste it into our
R code:
```{r rrtm_sensitivity, echo = TRUE, include = TRUE, ref.label=""rrtm_doubled_co2_balanced""}
```

When we set T~surface~ to 
`r format_md(rrtm_double_co2_balanced$T_surface, digits = 2)` K, the heat
imbalance becomes `r format_md(rrtm_double_co2_balanced$Q, digits = 2)`
Watts/m^2^. The climate sensitivity is the change in equilibrium 
T~surface~ when you double CO~2~: 
$\Delta T_{2\times \text{CO}_2} = `r format_md(rrtm_double_co2_balanced$T_surface, digits = 2)` \mathrm{K} - 
`r format_md(default_rrtm$T_surface, digits = 2)` \mathrm{K} = 
`r format_md(rrtm_double_co2_balanced$T_surface - default_rrtm$T_surface, digits = 2)`$ K.
You may remember that when we calculated the climate sensitivity with MODTRAN 
(using constant relative humidity to enable water-vapor feedback) we got 
$\Delta T_{2\times\text{CO}_2} = 1.21$ K for the tropical atmosphere (it's 
smaller for the other atmospheres), so this shows that including convection in 
our calculations roughly doubles the climate sensivity.

## Enhancements to `read_modtran`

I have enhanced the MODTRAN functions:

* In addition to saving data to a file, `read_modtran` also returns
  data in a form that you can work with directly, without needing to
  use `read_modtran`:
  
```{r modtran_examples}
modtran_baseline = run_modtran(file = ""_data/modtran_baseline.txt"")
modtran_baseline$i_out
plot_modtran(modtran_baseline)
```

You can also leave out the `file` argument to `read_modtran` if you don't need
to save the MODTRAN output to a file.
```{r modtran_example_2}
modtran_doubled_co2 = run_modtran(co2_ppm = 800)
modtran_doubled_co2$i_out
plot_modtran(modtran_doubled_co2, i_out_ref = modtran_baseline$i_out)
```

# New R and RMarkdown tricks

Sometimes you may want to use different text into your document, depending
on what the result of a calculation is.

For instance, I might have a function called `foo` that returns a number
and I want to write something different if `foo(x)` > x than if 
`foo(x)` < x. Here, the function `ifelse` can come in handy.
```{r define_foo}
foo = function(x) {
  x^2
}
```
Now I can write `ifelse(foo(x) < x, ""less than"", ""greater than"")`:
When x = 0.5, `foo(x)` is 
`r x = 0.5; ifelse(foo(x) < x, ""less than"", ""greater than"")` x,
but when x = 2.0, `foo(x)` is 
`r x = 2.0; ifelse(foo(x) < x, ""less than"", ""greater than"")` x.

You may have spotted a problem with the code above: What if `foo(x)` = x?
Then I need another `ifelse`: 
`ifelse(foo(x) < x, ""less than"", ifelse(foo(x) > x, ""greater than"", ""equal to""))`.
This is cumbersome to type into your text, so you might want to write a function:
```{r compare_foo}
compare_f = function(f, x) {
  # f is a function
  # x is a number or a numeric variable
  result = f(x)
  ifelse(result < x, ""less than"",
         ifelse(result > x, ""greater than"", 
                ""equal to""))
}
```
Now I can just write `compare_f(foo, x)`: 
When x = 0.5, `foo(x)` is `r compare_f(foo, 0.5)` x, but 
when x = 2.0, `foo(x)` is `r compare_f(foo, 2.0)` x and 
when x = 1.0, `foo(x)` is `r compare_f(foo, 1.0)` x.

This may seem kind of strange, but it can be helpful if you have a report
that you prepare regularly with different data and want to be able to 
update by running RMarkdown with different data sets, and have the text
adjust automatically to what the new numbers are.

This approach is used when businesses need to generate monthly reports 
(e.g., sales, finances, etc.) and want to automate the process with an
RMarkdown report template that can be used over and over with each new
month's data.

It is also applicable to climate science, where many laboratories like to 
update their reports every month or every year with the latest climate data.

# Exercises

These are the exercises you will work for the lab this week.

### General Instructions

In the past three weeks, we focused on mastering many of the basics of using 
R and RMarkdown. For this week's lab, when you write up the answers, I would
like you to think about integrating your R code chunks with your text.

For instance, you can describe what you're going to do to answer the question, 
and then for each step, after you describe what you're going to do in that
step, you can include an R code chunk to do what you just described, and then
the subsequent text can either discuss the results of what you just did
or describe what the next step of the analysis will do.

This way, your answer can have several small chunks of R code that build on
each other and follow the flow of your text.

## Chapter 5 Exercise

For this model, you will use the RRTM model, which includes both radiation and
convection.

### Exercise 5.1: Lapse Rate

Run the RRTM model in its default configuration and then vary the lapse rate 
from 0 to 10 K/km. For each value of the lapse rate, adjust the surface 
temperature until the earth loses as much heat as it gains (i.e., the value of
_Q_ in the `run_rrtm` model output is zero.)

It will probably be easier to do this with the interactive version of the RRTM
model at <http://climatemodels.uchicago.edu/rrtm/> than with the R interface
`run_rrtm`. 

a) Make a tibble containing the values of the lapse rate and the corresponding 
   equilibrium surface temperature, and make a plot with lapse rate on the 
   horizontal axis and surface temperature on the vertical axis.

b) Describe how the equilibrium surface temperature varies as the lapse rate 
   varies.

## Chapter 7 Exercises

### Exercise 7.2: Clouds and Infrared.

**Note:** this exercise only considers the effect of clouds on longwave
radiation and ignores the effect of clouds on albedo, which is also important.

a) Run the MODTRAN model with present-day CO~2~ (400 ppm) and a tropical atmosphere. 
   Plot the outgoing infrared spectrum. 
   
    Run MODTRAN four times: first with no clouds, and then with three different
    kinds of clouds: standard cirrus, altostratus, and stratus. These correspond
    to high, medium, and low-altitude clouds.

    Describe the important differences between the spectra for the four cases.
    Describe the differences in the intensity of outgoing infrared radiation
    $I_{\text{out}}$ for the four cases.

    How do the four spectra compare for the 700 cm^-1^ band (where CO~2~ absorbs
    strongly) and the 900 cm^-1^ band (in the atmospheric window)?

    Which kind of cloud has the greatest impact on outgoing infrared light?
    Why?

b) Now set `atmosphere` to `""midlatitude winter""`, set `clouds` to `""none""`,
   and set the sensor altitude to 0 km (`altitude_km = 0`) and make the sensor 
   look up (`looking = ""up""`). 
   This means your sensor is on the ground looking up at the longwave radiation 
   coming down from the atmosphere to the ground instead of looking down from 
   the top of the atmosphere at the longwave radiation going out to space.
   
    Run MODTRAN first with `h2o_scale = 1` (the default), and then with
    `h2o_scale = 0` (no water vapor).
    
    Plot the two spectra and compare them. Discuss why you see what you see:
    
    * For the atmosphere with no water vapor, compare the parts of the 
      spectrum corresponding to the strong CO~2~ absorption 
      (roughly 600--750 cm^-1^) and the infrared window 
      (roughly 800--1200 cm^-1^). 
      
        * Which corresponds to higher emission temperatures and which to lower temperatures? 
        * Why do you think this is?
    
    * For the atmosphere with normal water vapor (`h2o_scale = 1`), how does 
      water vapor change the spectrum you see from the ground?
      
        * Does it make the longwave radiation brighter (warmer) or dimmer
          (cooler)?
        * Why do you think this is?
    
c) Keeping the same settings for `atmosphere = ""midlatitude winter""`,
   `altitude_km = 0`, and `looking=""up""`, set `h2o_scale=1`
   and run MODTRAN first with no clouds, then with three kinds of clouds:
   standard cirrus, altostratus, and stratus (`clouds=""none""`, 
   `clouds=""standard cirrus""`, `clouds=""altostratus""`, and `clouds=""stratus""`).
   
    When we're looking up at the clouds, the base (bottom) of the clouds form
    a layer that is opaque to longwave radiation, with an emissivity of 1
    (i.e., a perfect black body).
    
    Cirrus clouds are very high (around 10 km above sea level), 
    altostratus clouds are at a medium height (with a base around 2.4 km), 
    and stratus clouds are very low (with a base around 0.33 km).

    For each run examine  
    $I_{\text{down}}$. (Remember that the variable `i_out` in the MODTRAN
    output measures the intensity of longwave radiation reaching the sensor.
    In this exercise, the sensor is on the ground looking up, so `i_out` 
    measures the downward radiation reaching the ground.)

    Describe how $I_{\text{down}}$ compares for the four conditions.
    
        * Do the clouds have a heating or cooling effect?
        * Which clouds have the greatest effect?
        * What does this suggest about how clouds affect the ground temperature?
        
    As you do this exercise, think about a winter night with clear skies 
    versus a winter night with cloudy skies.
    
d) Plot the longwave radiation spectra for the four MODTRAN runs from part (c).
   Which parts of the spectrum do the different clouds affect the most?
   (Compare the infrared window to the parts of the spectra where CO~2~ 
   absorbs.)
   
    * Look at two parts of the spectrum: the infrared window 
      (roughly 800--1200 cm^-1^) and the region where CO~2~ absorbs strongly 
      (roughly 600--750 cm^-1^).
   
      Why do you suppose the high, medium, and low clouds affect the two
      different spectral regions the way they do?
      
    * In which part of the spectrum do the clouds affect the downward longwave
      radiation the most?

### Exercise 7.3: Clouds and Visible Light.

For this exercise, you will use the RRTM model to examine climate sensitivity
and the water vapor feedback in a radiative-convective atmosphere.

a) First, run the RRTM model with its default parameters (400 ppm CO~2~) and 
   note the surface temperature (`T_surface`).
   
    Then run it again with doubled CO~2~ concentration (`co2 = 800`).
    Adjust the surface temperature to bring the heat imbalance `Q` to zero
    (it may be easier to do this with the interactive model at 
    <http://climatemodels.uchicago.edu/rrtm/> and then paste the new surface
    temperature into your R code).
    
    The change in surface temperature between the 400 ppm CO~2~ and 
    800 ppm CO~2~ ($\Delta T_{2 \times \text{CO}_2}$) runs is the 
    **climate sensitivity**. What is it?

b) Now run the RRTM model again, for 400 and 800 ppm CO~2~, but this time 
   setting `relative_humidity = 0` (this turns off the water vapor feedback).
   At each concentration of CO~2~, adjust `T_surface` to bring the heat into
   balance (so the output has `Q` equal to zero). Now what is the climate 
   sensitivity $\Delta T_{2 \times \text{CO}_2}$?
   
c) Compare the climate sensitivity ($\Delta T_{2 \times \text{CO}_2}$) in 
   part (a) (with water-vapor feedback) and part (b) (without water-vapor 
   feedback).
   The amplification factor for the water-vapor feedback is the ratio of the 
   climate sensitivity with water-vapor feedback to the sensitivity without the 
   feedback. 
   What is it?
  
"
501,GEOCARB_LAB,The Geochemical Carbon Cycle,,instructions,/files/lab_docs/lab_05/lab_05_instructions.pdf,,"```{r setup, include=FALSE}
knitr::knit_hooks$set(inline = function(x) { knitr:::format_sci(x, 'md')})
knitr::opts_chunk$set(echo = TRUE)

# This section loads necessary R libraries and sources scripts that define 
# useful functions format_md.
# 
data_dir = ""_data""
script_dir = ""_scripts""

if (!dir.exists(data_dir)) dir.create(data_dir)

library(pacman)
p_load(zoo, xml2, tidyverse, stringr)

theme_set(theme_bw(base_size = 15))

source(file.path(script_dir, ""utils.R""), chdir = T)
source(file.path(script_dir, ""format_md.R""), chdir = T)
source(file.path(script_dir, ""geocarb.R""), chdir = T)
```
# Carbon Cycle

For the following exercises, you will use the GEOCARB model, 
which simulates the earth's carbon cycle.

The GEOCARB model has two time periods: 

* First, it runs for 5 million years with the ""Spinup"" settings in order to 
bring the carbon cycle and climate into a steady state.

* Then, at time zero, it abruptly changes the parameters to the ""Simulation""
settings and also dumps a ""spike"" of CO~2~ into the atmosphere and runs for 
another 2 million years with the new parameters to see how the climate and
carbon cycle adjust to the new parameters and the CO~2~ spike.


The quantities that are graphed include:

pCO2
  ~ is the concentration of CO~2~ in the atmosphere, in parts per million.

WeatC
  ~ is the rate of CO~2~ being weathered from carbonate rocks and moved to the 
  oceans.

BurC
  ~ is the rate of carbonate being converted into limestone and buried on the 
  ocean floor.

WeatS
  ~ is the rate of SiO~2~ being weathered from silicate rocks and moved to the 
  oceans.

Degas
  ~ is the rate at which CO~2~ is released to the atmosphere by volcanic activity

tCO2
  ~ is the total amount of CO~2~ dissolved in the ocean, adding all of its forms:
  $$ \ce{\text{tco2} = [CO2] + [H2CO3] + [HCO3-] + [CO3^{2-}]}. $$

alk
  ~ is the ocean alkalinity: the total amount of acid ($\ce{H+}$) necessary to
  neutralize the carbonate and bicarbonate in the ocean. The detailed definition
  is complicated, but to a good approximation, 
  $\ce{\text{alk} = [HCO3-] + 2 [CO3^{2-}]}$. This is not crucial for this lab.

CO3
  ~ is the concentration of dissolved carbonate ($\ce{CO3^{2-}}$) in the ocean,
  in moles per cubic meter.

d13Cocn
  ~ is the change in the fraction of the carbon-13 ($\ce{^{13}C}$) isotope, 
  relative to the more common carbon-12  ($\ce{^{12}C}$) isotope, in the 
  various forms of carbon dissolved in the ocean water.

d13Catm
  ~ is the change in the fraction of $\ce{^{13}C}$, 
  relative to $\ce{^{12}C}$ in atmospheric CO~2~.

Tatm
  ~ is the average air temperature.

Tocn 
  ~ is the average temperature of ocean water.


### **Note:**

In this lab, you will mostly look at pCO2, but in exercise 8.2, you will also 
look at the weathering.

## Running the GEOCARB model from R

I have provided functions for running the GEOCARB model from R:

To run the model:
```
run_geocarb(filename, co2_spike, degas_spinup, degas_sim,
plants_spinup, plants_sim, land_area_spinup, land_area_sim,
delta_t2x, million_years_ago, mean_latitude_continents)
```

You need to specify `filename` (the file to save
the results in) and `co2_spike` (the spike in CO~2~ at time zero).

The other parameters will take default values if you don't specify them,
but you can override those defaults by giving the parameters a value.

`degas_spinup` and `degas_sim` are the rates of CO~2~ degassing from volcanoes 
for the spinup and simulation phases, in trillions of molecules per year.

`plants_spinup` and `plants_sim` are `TRUE/FALSE` values for whether to include 
the role of plants in weathering (their roots speed up weathering by making soil 
more permeable and by releasing CO~2~ into the soil), and `land_area` is the 
total area of dry land, relative to today. 
The default values are: `degas` = 7.5, `plants` = `TRUE`, and `land_area` = 1.

The geological configuration allows you to look into the distant past, where
the continents were in different locations and the sun was not as bright as
today.  
`delta_t2x` is the climate sensitivity (the amount of warming, in degrees 
Celsius, that results from doubling CO~2~).
`million_years_ago` is how many million years ago you want year zero to be and 
`mean_latitude_continents` is the mean latitude, in degrees, of the continents 
(today, with most of the continents in the Northern hemisphere, the mean 
latitude is 30 degrees).

After you run `run_geocarb`, you would read the data in with
`read_geocarb(filename)`. This function will return a data frame with the columns
`year`, `co2.total`, `co2.atmos`, `alkalinity.ocean`, 
`delta.13C.ocean`, `delta.13C.atmos`, `carbonate.ocean`, 
`carbonate.weathering`, `silicate.weathering`, `total.weathering`,
`carbon.burial`, `degassing.rate`, `temp.atmos`, and `temp.ocean`.

## Chapter 8 Exercises

### Exercise 8.1: Weathering as a function of CO~2~

In the steady state, the rate of weathering must balance the rate of CO~2~
degassing from the Earth, from volcanoes and deep-sea vents. 

Run a simulation with `co2_spike` set to zero, and set the model to increase
the degassing rate at time zero (i.e., set `degas_sim` to a higher value than
`degas_spinup`).

a)  Does an increase in CO~2~ degassing drive atmospheric CO~2~ up or down?
    How long does it take for CO~2~ to stabilize after the degassing increases 
    at time zero?

b)  How can you see that the model balances weathering against CO~2~ degassing
    (**Hint:** what variables would you graph with `ggplot`?)

c)  Repeat this run with a range of degassing values for the simulation phase
    and make a table or a graph of the equilibrium CO~2~ concentration versus 
    the degassing rate.

    Does the weathering rate always balance the degassing rate when the CO~2~
    concentration stabilizes?

d)  Plot the weathering as a function of atmospheric CO~2~ concentration, using
    the data from the model runs you did in part (c).


### Exercise 8.2: Effect of solar intensity on steady-state CO~2~ concentration

The rate of weathering is a function of CO~2~ concentration and sunlight, 
and increases when either of those variables increases.
The sun used to be less intense than it is today. 

Run GEOCARB with the spike set to zero, with the default values of
7.5 for both `degas_spinup` and `degas_sim`, and with the clock turned back 
500 million years to when the sun was cooler than today. 

What do you get for the steady state CO~2~? How does this compare to what
you get when you run GEOCARB for today's solar intensity? Explain why.

### Exercise 8.3: The role of plants  (**Graduate students only**)

The roots of plants accelerate weathering by two processes: First, as they 
grow, they open up the soil, making it more permeable to air and water.
Second, the roots pump CO~2~ down into the soil.


Run a simulation with no CO~2~ spike at the transition and with no plants in 
the spinup, but with plants present in the simulation.

a) What happens to the rate of weathering when plants are introduced in year zero? 
Does it go up or down right after the transition? WHat happens later on?

b) What happens to atmospheric CO~2~, and why?

c) When the CO~2~ concentration changes, where does the carbon go?


## Exercise from Chapter 10

### Exercise 10.1: Long-term fate of fossil fuel CO~2~

Use the GEOCARB model in its default configuration.

a) Run the model with no CO~2~ spike at the transition. What happens to 
   the weathering rates (Silicate, Carbonate, and Total) at the transition
   from spinup to simulation (i.e., year zero)?

b) Now set the CO~2~ spike at the transition to 1000 GTon. 

    * What happens to the weathering at the transition? How does weathering
      change over time after the transition?
    
    * How long does it take for CO~2~ to roughly stabilize (stop changing)?

c) In the experiment from (b), how do the rates of total weathering and 
   carbonate burial change over time? 
   
    * Plot what happens from shortly before the transition until 10,000
      years afterward (**Hint:** you may want to add the following to your
      `ggplot` command: `xlim(NA,1E4)` 
      to limit the range of the _x_-axis,
      or 
      `scale_x_continuous(limits = c(NA,1E4), labels = comma))` 
      if you also want to format the numbers on the  _x_-axis with commas to 
      indicate thousands and millions.)
      
      How do the two rates change? What do you think is happening to cause
      this?
    
    * Now plot the carbon burial and total weathering for the range
      1 million years to 2 million years. How do the two rates compare?
"
601,PROJECT_LAB,Independent Research Project,,instructions,/files/lab_docs/lab_06/lab_06_instructions.pdf,,"
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(lubridate)
library(stringr)

topic_approval_date = ymd(""2017-10-05"")
due_date = ymd_hms(""2017-10-17 23:59:00"", tz=""America/Chicago"")
presentation_due_date = ymd_hms(""2017-10-15 14:10:00"", tz=""America/Chicago"")

date_fmt = function(d) {
  strftime(d, format = ""%b. %e"") %>% str_trim() %>% str_replace_all("" +"", "" "")
}

time_fmt = function(d) {
  strftime(d, format = ""%l:%M %p"") %>% str_trim() %>% str_replace_all("" +"", "" "")
}
```

# Introduction

You have done several structured labs that provided opportunities to learn 
about R, RMarkdown, and several different aspects of climate science (data 
analysis and several computer models).  
For the next few weeks, you will have a chance to utilize the skills you have 
learned thus far in lab to explore a research question pertaining to your 
specific interests. 
This series of labs consists of a written lab report in RMarkdown and a 
presentation to the class. For this project, you will choose a question or idea 
pertaining to one of the previous labs and explore the topic a deeper. This may 
consit of exploring the ""why"" portion of previous questions that only asked you 
to describe patterns in the data or it may be evaluating relationships between 
other variables not assesed in prior labs. To answer your question, you may use 
the any of the data from lab 2, the MODTRAN model, the RRTM model, the GEOCARB 
model, or a combination thereof. Please get your topic approved by either 
Dr. Gilligan or Ms. Best by `r date_fmt(topic_approval_date)`. The final 
project is **due `r date_fmt(due_date)` at `r time_fmt(due_date)`**.

## Solo or Team Projects

You may choose to do this project individually or with a partner as a team 
effort. If you work with a partner, you may work together to design the 
question, obtain and analyze data, make a team presentation, and write a report
together. We expect teams to include a note in the report that indicates which
member contributed what to the report (this does not need to be super detailed
and can say, ""Alice and Bob designed the experiment together. Bob wrote the
code to run the models. Alice wrote the code for the data analysis. 
Alice and Bob contributed equally to writing the discussion and conclusions."")
This is similar to the requirement at many research journals that 
co-authored papers include a statement of what each author contributed.

## Choosing a Topic

For undergraduates, we recommend that you choose a topic from one of the 
exercises that you did in labs #2--5 and think of a new question along the same 
lines as the questions that exercise asked.
For graduate students, we expect you to try something more ambitious than just 
simple extensions of the questions from the lab exercises, but it is still fine 
to take one of the lab exercises as a starting point.

If you want to do something really different than what we have done previously 
in lab, that is fine. But check with one of us to make sure your plan is 
appropriate and feasible (we don't want you to bite off more than you can chew).

**Be CrEaTiVe!** Now is the time to really explore parts of the class that you 
have found interesting and present your findings in a unique, exciting way. 

# Writen Report (Due `r date_fmt(due_date)`)

Your report should be comprehensive, yet not overly verbose. One recommendation 
for achieving this is to create an outline to organize your thoughts before 
initializing writing and data analysis. The report needs to include the 
following components:

* Introduction

    * _Provide background information that frames the problem you are 
      addressing. At the end of the introduction, the reader should understand 
      exactly **what the problem is** that you are addressing and why that 
      problem is **interesting** and **relevant** to the climate system._
    
* Methods

    * _Describe the methods for answering your question. The methods section 
      should be written such that someone completely unfamiliar with your 
      project could follow your steps and recreate your results._

    * This section should contain the R code you use to do the analysis:

        + Getting data into R: download from the internet, read it in from 
          files on your computer, run models, etc.

        + Process data to clean it up: use functions like `mutate`, `gather`, 
          `summarize`, etc. to convert the data into a useful form.

        + Analyze data: anything you do to analyze the data, such as generating 
          descriptive statistics like the mean or standard deviation, fitting 
          linear models to get slopes (rates of change), etc.

* Results

    * _Describe the results of your analyses. Include apropriate charts, 
      tables, graphs, and other quantitative representations of data._ 

    * This section should have R code for making graphs, tables, etc.

* Conclusions/Discussion

    * _Discuss the implications for the results you found using data from your 
      results section._
    
         * Why are these results significant or interesting? 
 
         * What data supports these conclusions? 
 
         * What are the broader implications of these results? 
 
         * From the results that you have found, what are the next steps to 
           take in this line of research? What other questions have arisen as a 
           result of your analyses?

* Works Cited

    * _Include a works cited section to credit the research and thoughts that 
      are not your own. Be sure to use citations throughout your report where 
      necessary._

    * We will post a separate document that explains how to do citations and 
      bibliographies in RMarkdown.

Final reports are to be pushed to your Lab 6 Github repository 
_no later than `r time_fmt(due_date)` on `r date_fmt(due_date)`_. 
**You must push the .Rmd file *and* the knitted PDF** to Github. 
A portion of your final report grade will reflect effective use of 
R/RMarkdown/Github, the clarity and succinctness of your writing, 
visual representations of data, appropriate discussion of results, 
and insights into future analyses. 

# Presentation (`r date_fmt(presentation_due_date)`)

Presentations will be limited to five minutes per student. Allot approximately 
3.5 to 4 minutes for the presentation of your project and 
approximately 1 minute for questions. 
Team projects will get five minutes for each team member, and we expect you to 
coordinate so that each member speaks for roughly equal time.

* You can use Powerpoint to make your presentation, or if you are adventurous, 
  you can use RMarkdown to make a presentation. I will post a separate document 
  about how to use RMarkdown to make presentation slides.

* A Powerpoint or PDF of your final presentation needs to be pushed to Github 
  _no later than `r time_fmt(presentation_due_date)` on 
  `r date_fmt(presentation_due_date)`_. 
"
602,PROJECT_LAB,Citations and Bibliographies,Jonathan Gilligan,citations_bibliographies,/files/lab_docs/lab_06/citations_bibliographies.pdf,lab_06.bib,"
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Introduction


The simplest way to do citations and bibliographies in RMarkdown is simply
to use author-date citation like this:

> The Urey reaction (Archer 2011, p. 98) shows how silicate weathering can
> convert atmospheric carbon (in the form of CO~2~) into rocks at the bottom
> of the ocean (in the form of CaCO~3~).

And then just manually enter a bibliography at the end of the document:

> * Archer, David (2011). _Global Warming: Understanding the Forecast_
>   (Wiley)
>
> * Rogelj, Joeri, McCollum, David L., Reisinger, Andy, Meinshausen, Malte,
>   and Riahi, Keywan (2013). 
>   Probabilistic cost estimates for climate change mitigation,
>   _Nature_ **493**, 79--83.

## Getting fancy with citations and bibliographies

However, if you want to experiment with something fancier, 
RMarkdown has the ability to automatically format bibliographic information and
manage citations. Using this is a little complicated because you need to have
file with your bibliographic information entered in a certain format. 

Everything that follows describes how to do citations and bibliographies
with RMarkdown using bibliography files. If you wish, you may ignore the
rest of this document and manage your citations and bibliographies manually,
as described at the top of this file.

RMarkdown can work with a wide variety of file formats for bibliographies, 
including those produced by many common software packages, such as 
EndNote, Zotero, and Mendeley.
You can find further documentation about bibliographies and citations
at <http://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html>


# Bibliography Files

RMarkdown reads bibliography entries from an external bibliography file.
To specify the bibloography file, you would 
just add a line to the header of your RMarkdown document: for instance,

```
---
title: ""My Lab Report""
subtitle: ""EES 3310: Global Climate Change""
author: ""Jonathan Gilligan""
date: ""Sept. 27, 2018""
bibliography: ""my_bibliography.bib""
---
```

RMarkdown can figure out the kind of bibliography file it is from the file 
extension (`.bib`, etc.). I have provided an example BibTeX file, `lab_05.bib` 
that has examples of entries.

For the examples here, I will work with a format called BibTeX because most
common software packages can export bibliographies in BibTeX format, and 
Google Scholar can also provide citations in BibTeX format that you can copy 
and past from a browser. BibTeX files are plain text and you can edit them in
RStudio.[^1]

[^1]: If you will use BibTeX files often, I recommend a free tool
called JabRef, which lets you edit the information in a BibTeX file
in a graphical interface without having to worry about the details of the 
BibTeX format. You can get JabRef from <http://www.jabref.org/>.
It runs on Windows, MacOS, and Linux.

If you prefer, you can also use bibliographic software, such as 
Zotero,[^2] Mendeley, or EndNote, and export your bibliography in
RIS format (as a `.ris` file), BibTeX format (as a `.bib` file),
or BibLaTeX format (as a `.biblatex` file).
RMarkdown claims to support the following file types as well, but I 
have not tried them, so I would not be able to provide much help if you
try them and run into trouble.

[^2]: I recommend Zotero, which is a free, easy to use, and very powerful
bibliography tool that supports Windows, MacOS, and Linux, and has
add-ins that integrate very nicely with most browsers (Chrome, Firefox, Safari, 
and Opera) and also has a good add-in to let you use it in 
Microsoft Word. Zotero has hundreds of bibliography styles for different 
uses, including standard ones, such as Chicago, APA, and MLS.
You can get Zotero from <https://zotero.org>. One reason I like Zotero 
enormously is that the web browser add-in creates an icon in the browser's 
toolbar that recognizes when you are reading an article in a scholarly journal,
a newspaper, a magazine, a blog, etc., and you can just click on the icon to 
import whatever you're reading in your browser into your bibliography database.
You can also sign up for a free account on 
<https://www.zotero.org/user/register>
that will let you back up your bibliography database to the cloud, synchronize
your bibliographies across multiple computers, and share bibliographies with 
other people.

\clearpage

| Format | File extension |
|:-------|:--------------|
| EndNote | `.enl` |
| EndNote XML | `.xml` |
| RIS | `.ris` |
| BibTeX | `.bib` |
| ISI | `.wos` |
| MEDLINE | `.medline` |
| MODS | `.mods` |
| Copac | `.copac` |
| JSON citeproc | `.json` |

You can export files in many of these formats (especially RIS and BibTeX) from
most bibliographic software packages, such as EndNote, Mendeley, and Zotero.

# Citations

In your document, you can cite books, articles, etc. by the identifiers, or 
_keys_, that appear in the database.[^3] 
If you open the bibliography file in RStudio, you will see that all references
begin with a reference type, starting with `@` (e.g., `@book` for a book,
`@article` for an article in a journal, etc.) and then all the data for the 
reference is contained between a pair of braces `{...}`.
The first thing after the opening brace will be the citation key,
followed by a comma:
```
@book{archer.forecast.2011,
  title = {Global Warming: Understanding the Forecast},
  author = {Archer, David},
  publisher = {Wiley},
  address = {Hoboken, NJ},
  year = {2011},
  edition = {2nd}
}

@article{rogelj2013probabilistic,
  title={Probabilistic cost estimates for climate change mitigation},
  author={Rogelj, Joeri and McCollum, David L and Reisinger, Andy and 
          Meinshausen, Malte and Riahi, Keywan},
  journal={Nature},
  volume={493},
  pages={79--83},
  year={2013},
}
```
The citation keys for these two references are 
`archer.forecast.2011` and `rogelj2013probabilistic`.

[^3]: In BibTeX files, the identifiers can
be any sequence of letters, numbers, and any of the following punctuation:
'`_`', '`.`', '`:`', and '`;`'.


I can insert citations in an RMarkdown document by putting them inside
square brackets (`[...]`) and putting an `@` in front of the citation key:
`[@archer.forecast.2011]` will become [@archer.forecast.2011].
I can cite a specific page with 
`[@archer.forecast.2011, p. 143]`, which becomes [@archer.forecast.2011, p. 143].
I can also add some preceding text: 

```
[See, e.g., @archer.forecast.2011, pp. 75--78]
```
becomes
[See, e.g., @archer.forecast.2011, pp. 75--78].

I can cite multiple authors: 
```
[@archer.forecast.2011, @nordhaus.casino.2013, and @pielke.climate.fix.2010].
```
becomes

> [@archer.forecast.2011, @nordhaus.casino.2013, and @pielke.climate.fix.2010].

If I want to omit the name of the author (for instance, if I have named him or her
earlier in the text), I can put a minus sign in front of the `@`:
```
Archer describes the water-vapor feedback [-@archer.forecast.2011].
```
becomes

> Archer describes the water-vapor feedback [-@archer.forecast.2011].
    
I can also put citations in-line by omitting the square brackets:

```
@jaeger.adams.fallacy.2008 argue that focusing only on the economic 
impacts of climate change is misleading.
```
becomes

> @jaeger.adams.fallacy.2008 argue that focusing only on the economic 
> impacts of climate change is misleading.

and I can also add page numbers or other text to follow the year in the citation
by putting them in square brackets after an in-text citation:
```
@rogelj2013probabilistic [Fig. 2, p. 81] show that if political inaction
causes even modest delays in reducing carbon emissions, it can dramatically 
increase the cost of mitigating climate change.
```
becomes

> @rogelj2013probabilistic [Fig. 2, p. 81] show that if political inaction
> causes even modest delays in reducing carbon emissions, it can dramatically 
> increase the cost of mitigating climate change.

# The Bibliography

RMarkdown will insert a formatted bibliography at the end of your document.
It will not automatically put a section heading, so you probably want to put
a section heading called ""Bibliography"" or ""Works Cited"" to set this off
from your text. 

At the bottom of this `.Rmd` file, I have inserted the line
```
# References
```

## Adding entries to the bibliography file

If you want to add new references to the bibliography file, you can 
open `lab_05.bib` in RStudio and edit it by hand, but that may be difficult
and confusing, so I would recommend one of three options:

* From Google Scholar, you can export a BibTeX entry: 

    1. Open `lab_05.bib` in RStudio and then open a web browser, go to 
       <https://scholar.google.com> and search for your reference.

    2. When you find the reference you want in Google Scholar, and click on the 
       ""Cite"" button, as shown below:

        ![Google Scholar search page. Click on the ""Cite"" button, as shown here.](images/scholar_1.png)\

    3. Click on ""BibTeX"" at the bottom of the citation page, as shown below:

        ![Google Scholar citation page: Click on ""BibTeX""](images/scholar_2.png)\

    4. You will see something like this:

        ```
         @article{rogelj2012global,
          title={Global warming under old and new scenarios using IPCC climate 
                 sensitivity range estimates},
          author={Rogelj, Joeri and Meinshausen, Malte and Knutti, Reto},
          journal={Nature climate change},
          volume={2},
          number={4},
          pages={248},
          year={2012},
          publisher={Nature Publishing Group}
        }
        ```
        
         Copy the entry and paste it into `lab_05.bib` in RStudio.
         
         In this entry, `rogelj2012global` is the key you would use
         for citing and the other fields are pretty much self-explanatory.

* If you have a bibliographic program like Zotero, Mendeley, or EndNote, 
  export the references you want as BibTeX files.
   
     For instance, in Zotero,[^5] highlight the references, right click, and from 
     the context menu choose ""Export Items...""; then select ""BibTeX"" format, 
     click ""OK"" and save the exported items to a file.)
    
     Next, open the file you just exported and `lab_05.bib` in RStudio and
     copy and past the references from the exported file into `lab_05.bib`.
     
* If you have JabRef[^6] installed on your computer, you open `lab_05.bib`,
  manually create a new bibliographic entry and fill in the relevant data.
  
[^5]: Zotero is free software that runs on Windows, MacOS, and Linux, and you
can download it from <https://zotero.org>

[^6]: JabRef is free software that runs on Windows, MacOS, and Linux and you can
download it from <https://jabref.org>.


# References

"
701,DECARB_BOTTOM_UP,New Tools for Data Analysis,Jonathan Gilligan,new_tools,/files/lab_docs/decarb_lab_bottom_up/new_tools.pdf,,"```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(broom)
library(magrittr)
library(kayadata)

theme_set(theme_bw())
```
# Some New Tools for Working with Data

There are two new tools we will use in working with data for this lab:

## Getting fancy with the pipe operator (`%>%`)

With the tidyverse, we often use the ""pipe"" operator `%>%` to send the
output from one function into the input of another function. In most cases,
this works very simply and it uses the output from the function on the
left as the first argument of the function on the right. 

For instance, the `filter` function selects certain rows in a data frame or
tibble. I will work here with examples from the built-in `storms` data
frame that lists a subset of hurricanes from the National Oceanic and 
Atmospheric Administration's Atlantic hurricane database.
The data includes the positions and attributes of 198 tropical storms, measured
every six hours during the lifetime of the storm.

Let's select Hurricane Katrina from 2005 (I specify the year because there
were two earlier Katrinas in 1981 and 1999; storm names are re-used 
periodically unless they are retired because a storm with that name becomes 
historically important).
```r
data(storms)
katrina <- filter(storms, name == ""Katrina"", year == 2005)
```
and we can also do the same thing using the pipe operator like this:
```r
storms %>% filter(name == ""Katrina"", year == 2005)
```
What the pipe operator `%>%` does behind the scenes is take that output of
whatever is on the left (the data frame `storms`) and stick it in front
of the arguemnts for the function on the right so the two examples given
above are equivalent.

We have been usuing the pipe operator throughout the semester to combine 
functions. 
Suppose we want to `mutate` the `storms` data frame to add a new 
column `ts_area`
that is equal to `pi * (ts_diameter / 2)^2`, where `ts_diameter` is a column
in `storms` that corresponds to the diameter (in nautical miles) of the area 
in which winds are tropical storm strength or greater (36 knots), 
and then we want to select only the data where the tropical storm area is 
greater than 10,000 square nautical miles, and finally sort the data so the
rows of the data frame go from largest to smallest tropical storm area.
We can do that as follows:
```r
# First way
df1 = arrange(filter(mutate(storms, ts_area = pi * (ts_diameter / 2.0)^2), 
     ts_area > 10000), desc(ts_area))

# Second way, using a temporary variable
df2 = mutate(storms, ts_area = pi * (ts_diameter / 2.0)^2)
df2 = filter(df2, ts_area > 10000)
df2 = arrange(df2, desc(ts_area))

# Third way, using the pipe operator
df3 = storms %>% mutate(storms, ts_area = pi * (ts_diameter / 2.0)^2) %>%
     filter(ts_area > 10000) %>% arrange(desc(ts_area))
```
We use pipes because often they are simpler to understand. If we use the first
way in the example above, if we are combining many functions, nesting functions
inside other functions becomes very difficult to read and understand.
The second way, of using temporary variables is fine, but it can get confusing 
to have lots of variable names that we use only one time to pass data from
one function to another. The third way, using the pipe operator makes it easier
to understand how the data is moving through the processing pipeline.

But the way pipe operators work can sometimes get in the way of things we want
to do.

First, suppose that I want to send the results of a function into the second
or third argument of a subsequent function instead of the first argument?
We use the `lm` function to calculate the slope and intercept of a trend line
that best fits some data. The lm function expects the formula for the line
to fit to be the first argument and the data to be the second.

Suppose I want to examine hurricanes from  the year 2005 and look for a 
relationship between the air pressure in the eye and the wind speed,
but only during the time when they are classified as hurricanes.

I can do this as follows:
```r
hurricanes_2005 = storms %>% filter(status == ""hurricane"", year == 2005)
lin_fit = lm(wind ~ pressure, data = hurricanes_2005)
```
The `lm` function looks at the data given by the `data` argument and
then finds the best line that characterizes the linear relationship
`wind ~ pressure`, which means that the wind is a linear function of the
pressure (this would be the slope of a graph where `wind` is on the y-axis
and `pressure` is on the x-axis).

Suppose that I would like to pipe data from the `filter` function to the
`lm` function. I have a problem because the pipe operator would put the
output from `filter` as the first argument of the `lm` function, but in
`lm` the first argument is the formula and `data` is the second.
I can do this as follows:
```r
lin_fit2 = storms %>% filter(status == ""hurricane"", year == 2005) %>%
           lm(wind ~ pressure, data = .)
```
The pipe operator creates a new variable `.` (the period), which gets the
value of whatever is on the left of the operator. If I don't use the `.`
in the function on the right, the pipe operator invisibly puts `.` in
the first argument so `foo(x) %>% bar(y, z)` is re-written behind the 
scenes to become
```r
. = foo(x)
bar(., y, z)
```
However, if I want to use the `.` somewhere else, I can do that explicitly
and when the pipe operator looks at the function on its right, it knows
not to stick the `.` in the first argument if I use `.` in the function
call. This, I could write `foo(y) %>% bar(x, ., z)` if I want to substitute
the output of `foo(y)` for the second argument of `bar`.

## Extracting columns from a data frame in a pipeline

If I just want to look at the names of the hurricanes from the `storms` data
frame, I can use the `$` operator:
```r
names = storms$name
```
But suppose that I want to filter the storms and then extract the names.
What are the names of category 5 hurricanes?
```r
cat_5 = storms %>% filter(category == 5)
names = cat_5$name
```
Suppose I want to do this without creating a variable `cat_5`?
I could try 
```r
# This will give an error
names = storms %>% filter(category == 5)$name
```
but it turns out that this doesn't work because of the way the pipe operators
work.
To do this, I need to load the `magrittr` library and use the `%$%` operator
instead of `$`:
```r
# This will work
names = storms %>% filter(category == 5) %$% name
```
The `%$%` operator works like the pipe operator, but instead of calling a function,
it extracts the specified column (in this case, `name`).

This will be useful for what comes next.

## Tidying up results of statistical modeling

`lm` is one example of a lot of powerful functions in R that let you analyze
data by fitting models. A statistical model is an idealized functional relationship
between different attributes of the data (attributes are represented by columns
in a data frame). 
For instance, a linear model idealizes two columns in the data frame in terms
of a linear relationship. For instance `y~x` represents a model in which
$y = a + b x$. In fact, the data rarely lie exactly along a line, but 
the `lm` function will find the values for `a` and `b` that define the line
that best  relates _y_ to _x_ (specifically this will be the line such that
the minimizes the sum of the square of the errors between the actual _y_
and the line:
$\sum_i (y_i - (a + b x_i))^2$.

We will be using `lm` in this lab to find the rates of change of different
parameters in the Kaya identity. For that we want to examine the slopes of the
lines identified by `lm`. We can do that as follows for the slope that relates
wind speed to pressure for a hurricane.

First, let's plot the two variables to see wether there's an apparent 
relationship. 
Let's look at hurricanes from 2005:

```{r plot_pressure_wind, echo=TRUE, eval=TRUE, include=TRUE, warning=FALSE, message=FALSE}
storms %>% filter(status == ""hurricane"", year == 2005) %>%
  ggplot(aes(x = pressure, y = wind)) + geom_point(na.rm = T)
```

The data clearly doesn't lie on a straight line, but there is a clear
relationship that lower pressures tend to go along with higher wind
speeds and higher pressures with lower wind speeds.

First, let's review how we can fit a relationship between wind speed and 
pressure for hurricanes from 2005:
```r
lin_fit = storms %>% filter(status == ""hurricane"", year == 2005) %>%
           lm(wind ~ pressure, data = .)
```
But how can we get the slope out of the variable `lin_fit`?
Here we use the `broom` package, which gives us a `tidy` function
that will tidy up the results of statistical models and put them
into a nice data frame:

```{r tidy_fit_1, echo=TRUE, eval=TRUE, include=TRUE, warning=FALSE, message=FALSE}
lin_fit = storms %>% filter(status == ""hurricane"", year == 2005) %>%
           lm(wind ~ pressure, data = .)
tidy(lin_fit)
```

Here, `term` tells us the parameter: `(Intercept)` is the intercept 
(_a_ in the formula above), and `pressure` is the slope with respect to the
pressure (_b_ in the formula above).
`estimate` is the estimate of the parameter (intercept or slope),
`std.error` is the uncertainty about the estimate of the parameter,
`statistic` and `p.value` are used for statistical hypothesis testing and we
will not use them so you can ignore them.

If I just want the slope, I can get that as follows:
```{r tidy_fit_2, echo=TRUE, eval=TRUE, include=TRUE, warning=FALSE, message=FALSE}
lin_fit = storms %>% filter(status == ""hurricane"", year == 2005) %>%
           lm(wind ~ pressure, data = .)
slope = tidy(lin_fit) %>% filter(term == ""pressure"") %$% estimate
slope
```

Here, `filter` selects the row where term equals ""pressure"" and `%$% estimate`
then selects the `estimate` column.

We can write this more compactly as follows:
```{r tidy_fit_3, echo=TRUE, eval=TRUE, include=TRUE, warning=FALSE, message=FALSE}
slope = storms %>% 
           filter(status == ""hurricane"", year == 2005) %>%
           lm(wind ~ pressure, data = .) %>% 
           tidy() %>% 
           filter(term == ""pressure"") %$% estimate
slope
```

# Writing functions

If you are going to do the same calculation many times in R it is often useful
to define a function that performs that calculation and then you can simply
call the function whenever you want to do the calculation with new values.

For instance, in the ""Growth Rates and Trends"" section of the assignment
for the bottom-up decarbonization lab, we see that if we have a quantity
_q_ that is growing at a steady percentage rate $r_q$ each year and we know
the value in one year $y_0$ and want to predict what the value will be at
a future year $y$, we can calculate this as follows:
$$ q(y) = q(y_0) \times \exp(r_q \times (y - y_0)). $$
We can write this as an R function that we can call for different quantities and
years:
```{r growth_function, echo=TRUE, include=TRUE}
growth = function(q_0, rate, start_year, end_year) {
  q_0 * exp(rate * (end_year - start_year))
}
```
A function definition has three parts:
First we have the name of the function (`growth`), then we have the word
`function`, followed by the arguments that the user must supply to the function
`(q_0, rate, start_year, end_year)`, and then the body of the function, between
curly braces `{ ... }`.
When you call the function, it executes all of the code between the curly braces
and returns the last value calculated before the closing brace.
Thus, we can call the function like this to estimate the world population 
in 2050
```{r call_function, echo=TRUE, include=TRUE}
P_2017 = 7.53 # world population was 7.53 billion
r_P = 0.0141  # growth rate is 1.41% per yer
P_2050 = growth(q_0 = P_2017, rate = r_P, start_year = 2017, end_year = 2050)
P_2050
```
If you don't specify the name of the parameter, the function fills them out in
order, so we could also write 
```{r call_function_2, echo=TRUE, include=TRUE}
P_2017 = 7.53 # world population was 7.53 billion
r_P = 0.0141  # growth rate is 1.41% per yer
P_2050 = growth(P_2017, r_P, 2017, 2050)
P_2050
```

# `kayadata`: an R package for analyzing Kaya-identity data

You can install the `kayadata` package as follows:

```r
library(devtools)
install_github(""jonathan-g/kayadata"")
```
or
```r
library(pacman)
p_load_current_gh(""jonathan-g/kayadata"")
```

Once you have installed the `kayadata` package, you load it as follows:
```r
library(kayadata)
```

This package has several useful functions:

* `get_kaya_data(region_name)`: Get Kaya-identity data for a country or region.
    * **Argument:** `region_name` is the name of the country or region you want 
      data for (e.g., ""United States"", ""World"", ""China"", etc.).
      
        You can inspect a list of countries and regions by running 
        `kaya_region_list()`.
    * **Results:** The function returns a data frame with the following columns:
        * `region`: The country or region.
        * `year`: The year.
        * `P`: The population (in billions).
        * `G`: The gross domestic product (in trillions).
        * `E`: The primary energy consumption (in quads).
        * `F`: The energy-related CO~2~ emissions (in millions of metric tons 
          of CO~2~).
        * `g`: Per-capita GDP (in thousands of dollars per person).
        * `e`: The energy intensity of the economy (in quads per trillion 
          dollars of GDP).
        * `f`: The emissions intensity of the energy supply (in millions of 
          metric tons of CO~2~ per quad).
        * `ef`: The emissions intensity of the economy (in millions of metric 
          tons of CO~2~ per trillion dollars of GDP).
* `get_top_down_values(region_name)`: Get projections of the future
  values of Kaya-identity variables for a country, from the top-down 
  International Energy Outlook analysis by the U.S. Energy Information 
  Administration.
    * **Argument:** `region_name` is the name of the country or region you want 
      data for (e.g., ""United States"", ""World"", ""China"", etc.).
    * **Results:** The function returns a data frame with the following columns:
        * `region`: The country or region.
        * `year`: The year (five-year intervals from 2015--2050).
        * `P`: The population (in billions).
        * `G`: The gross domestic product (in trillions).
        * `E`: The primary energy consumption (in quads).
        * `F`: The energy-related CO~2~ emissions (in millions of metric tons 
          of CO~2~).
        * `g`: Per-capita GDP (in thousands of dollars per person).
        * `e`: The energy intensity of the economy (in quads per trillion 
          dollars of GDP).
        * `f`: The emissions intensity of the energy supply (in millions of 
          metric tons of CO~2~ per quad).
        * `ef`: The emissions intensity of the economy (in millions of metric 
          tons of CO~2~ per trillion dollars of GDP).
* `get_top_down_trends(region_name)`: Get the trends for Kaya-identity variables 
  for a country, from the top-down International Energy OUtlook analysis by the 
  U.S. Energy Information Administration.
    * **Argument:** `region_name` is the name of the country or region you 
      want data for (e.g., ""United States"", ""World"", ""China"", etc.).
    * **Results:** The function returns a data frame with the following columns:
        * `region`: The country or region
        * `P`: The rate of change of the population (multiply by 100 to get 
          the rate in percent per year).
        * `G`: The rate of change of GDP.
        * `E`: The rate of change of primary energy consumption.
        * `F`: The rate of change of CO~2~ emissions.
        * `g`: The rate of change of per-capita GDP.
        * `e`: The rate of change of the energy intensity of the economy.
        * `f`: The rate of change of the emissions intensity of the energy 
          supply.
        * `ef`: The rate of change of the emissions intensity of the economy.
* `project_top_down(region_name, year)`: Calculate projected values for 
  Kaya identity variables at a future year from the present through 2050.
    * **Arguments:** 
        * `region_name` is the name of the country or region you want 
          data for.
        * `year`: The year you want projected data for (through 2050).
    * **Results:** A data frame with one row and columns for `region`,
      `year`, and the Kaya variables: _P_, _G_, _E_, _F_, _g_, _e_, _f_, and
      _ef_.
* `get_fuel_mix(region_name)`: Get the mixture of primary energy sources used
  by a country or region.
      * **Argument:** `region_name` is the name of the country or region you 
      want data for (e.g., ""United States"", ""World"", ""China"", etc.).
    * **Results:** The function returns a data frame with the following columns:
        * `region`: The country or region
        * `region_code`: A three-letter code for the country or region.
        * `geography`: The kind of region (""nation"", ""region"", or ""world"").
        * `year`: The year the data represents.
        * `fuel`: The kind of energy source (""Oil"", ""Natural Gas"", ""Coal"", 
          ""Nuclear"", ""Hydro"", or ""Renewables"")
        * `quads`: The number of quads of that kind of energy consumed in that 
          year.
        * `pct`: The percent of total primary energy consumption that came from
          that kind of energy source.
* `plot_kaya(kaya_data, variable, start_year, stop_year, y_lab, log_scale,
   trend_line, points, font_size)`: Plot a time series showing trends in a 
   Kaya variable.
    * **Arguments:**
        * `kaya_data`: A data frame, like those returned by `get_kaya_data`.
        * `variable`: A quoted Kaya variable name (""P"", ""G"", ""E"", ""F"", ""g"", 
          ""e"", ""f"", or ""ef"").
        * `start_year` (optional): If you want to do trend analysis, you can
          specify the year to start calculating the trend.
          The graph will color the years from `start_year` to `stop_year`
          in a darker color.
          
          If you don't specify `start_year`, the default value is 1980.
          You can eliminate the special coloring and trend analysis 
          by setting `start_year = NULL`.
        * `stop_year` (optional): The year to stop the trend analysis. 
          If you don't specify `stop_year`, it defaults to the last year
          in the data frame.
        * `y_lab` (optional): The label for the y-axis. If you don't specify 
          this, R will pick an appropriate label based on which variable you
          choose.
        * `log_scale` (optional): Set this to `TRUE` or `FALSE` to specify 
          whether to plot the y-axis on a logarithmic scale. 
          The default is to set `log_scale = FALSE` if you don't specify a 
          value.
        * `trend_line` (optional): Set this to `TRUE` or `FALSE` to specify 
          whether to plot a trend line. 
          The default is to set `trend_line = FALSE` if you don't specify a 
          value.
        * `points` (optional): Set this to `TRUE` or `FALSE` to specify 
          whether to plot points on the line. If you are making a small
          graph, the points may make the graph too busy.
          The default is to set `points = TRUE` if you don't specify a 
          value.
        * `font_size` (optional): This lets you adjust the size of the fonts
          used in labeling the axes on the graph.
          If you are making small plots and the y-axis label won't fit on
          the side of the graph, you might want to use a smaller font size.
          The default is to set `font_size = 20` if you don't specify a value.
    * **Result:** This function returns a plot object.
    * **Example:**
      ```{r example_kaya_plot, echo=TRUE, include=TRUE}
      kd <- get_kaya_data(""World"")
      plot_kaya(kd, ""F"")
      ```
      ```{r example_kaya_plot_2, echo=TRUE, include=TRUE}
      plot_kaya(kd, ""F"", log_scale = TRUE, font_size = 12, trend_line = TRUE)
      ```
      ```{r example_kaya_plot_3, echo=TRUE, include=TRUE}
      plot_kaya(kd, ""F"", points = FALSE, start_year = NULL, font_size = 12)
      ```
* `plot_fuel_mix(fuel_mix, collapse_renewables)`: Make a ""donut"" chart showing
  the amount of energy the country got from different energy sources.
    * **Arguments:**
        * `fuel_mix`: A data frame with the fuel mix for the country, like the
          ones returned by `get_fuel_mix()`
        * `collapse_renewables` (optional): Set this to `TRUE` or `FALSE` to
          specify whether to combine ""Hydro"" with other renewables to make one
          ""Renewables"" category (`TRUE`), or plot ""Hydro"" separately from the
          other renewables (`FALSE`).
          
          If you don't specify `collapse_renewables`, the default is to set
          `collapse_renewables = TRUE`.
    * **Results:** A plot object with a donut chart.
    * **Example:**
      ```{r example_fuel_plot, echo=TRUE, include=TRUE}
      fm <- get_fuel_mix(""World"")
      plot_fuel_mix(fm)
      ```
      ```{r example_fuel_plot_2, echo=TRUE, include=TRUE}
      plot_fuel_mix(fm, collapse_renewables = FALSE)
      ```
* `kaya_region_list()`: Get a list of regions and countries for which you can
  get Kaya data.
* `emissions_factors()`: Get a data frame with columns for 
    * `fuel`: The energy source
    * `emission_factor`: The emissions from that energy source, in million
      metric tons of CO~2~ per quad of energy.
* `generation_capacity()`: Get a data frame with generation capacity of a 
  typical power plant for different kinds of energy sources. 
  This data frame has columns:
    * `fuel`: The energy source.
    * `description`: A description of the kind of power plant.
    * `nameplate_capacity`: The ""nameplate capacity"" of a typical power plant
      of that type. The nameplate capacity is the power output when the
      plant is running at its maximum capacity.
    * `capacity_factor`: The capacity factor. This is the fraction of the 
      nameplate capacity that the power plant can produce over an average 
      year.
      
        The average power output from a power plant is the nameplate capacity
        times the capacity factor:
        $$ \text{average power} = \text{nameplate capacity} \times 
           \text{capacity factor} $$

"
702,DECARB_BOTTOM_UP,Instructions for Bottom-Up Decarbonization Policy Analysis,,instructions,/files/lab_docs/decarb_lab_bottom_up/decarb_bottom_up_instructions.pdf,,"```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = FALSE)

is_output_html <- function() {
  is_html <- TRUE
  header <- rmarkdown:::parse_yaml_front_matter(
    readLines(knitr::current_input())
  )
  if (""output.blogdown::html_page"" %in% names(header)) {
    is_html <-  TRUE
  } else if (""output"" %in% names(header)) {
    if (is.list(header$output)) {
      output <- names(header$output)[1]
    } else {
      output <- header$output[1]
    }
    if (str_detect(output, regex(""(pdf|word)_document"", ignore_case = TRUE))) {
      is_html <- FALSE
    }
  }
  is_html
}


getOutputFormat <- function() {
  output <- rmarkdown:::parse_yaml_front_matter(
    readLines(knitr::current_input())
    )$output
  if (is.list(output)){
    return(names(output)[1])
  } else {
    return(output[1])
  }
}

data_dir = ""_data""
script_dir = ""_scripts""

library(pacman)
p_load(magrittr, tidyverse, lubridate, knitr, broom)
p_load_gh(""jonathan-g/kayadata"")

theme_set(theme_bw(base_size = 15))

if (!dir.exists(data_dir)) dir.create(data_dir)

source(file.path(script_dir, ""utils.R""), chdir = T)

due_date_1 <- date(""2018-10-30"")
due_date_2 <- date(""2018-11-05"")

long_due_date_1 <- stamp(""Monday, March 1"")(due_date_1)
short_due_date_1 <- stamp(""Mar. 1"")(due_date_1)

long_due_date_2 <- stamp(""Monday, March 1"")(due_date_2)
short_due_date_2 <- stamp(""Mar. 1"")(due_date_2)

current_year <- max(get_kaya_data(""United States"")$year, na.rm = T)
after_current_year <- current_year + 1

ref_year <- 2005
target_year  <- 2050
target_reduction <- 0.80

eia_report_year <- max(get_top_down_values(""United States"")$year, na.rm = T)
donut_year <- max(kayadata::get_fuel_mix(""United States"")$year, na.rm = T)

IEO_url <- ""http://www.eia.gov/forecasts/ieo/""
EIA_table_url <- ""http://www.eia.gov/oiaf/aeo/tablebrowser""

bottom_up_country_assignments <- bind_rows(
  tibble(class = ""Undergraduates"", 
         region = c(""World"", ""United States"", ""China"")),
  tibble(class = ""Grad Students"", 
         region = c(""World"", ""United States"", ""China"",
                     ""India"", ""Brazil""))
)

rcp_26 <- tribble(
  ~region, ~target_year, ~ref_year, ~reduction,
  ""Australia/New Zealand"", 2050, 2005,  0.82,
  ""Canada"",                2050, 2005,  0.72,
  ""China"",                 2050, 2005,  0.78,
  ""India"",                 2050, 2005,  0.73,
  ""Japan"",                 2050, 2005,  0.66,
  ""South Korea"",           2050, 2005,  0.67,
  ""United States"",         2050, 2005,  0.73,
  ""Africa"",                2050, 2005,  0.28,
  ""Latin America"",         2050, 2005,  0.40,
  ""Middle East"",           2050, 2005,  0.32,
  ""Southeast Asia"",        2050, 2005, -0.17,
  ""Western Europe"",        2050, 2005,  0.74,
  ""World"",                 2050, 2005,  0.36
)

```
# Note

This is a revised and corrected version of the instructions. I corrected the
error in step 6 that a student called to my attention during lab today.

I also added a concise outline of the steps before the detailed step-by-step
instructions.

Nothing has changed about the substance of the lab. All I have done is to 
clarify and correct the original instructions.

# Introduction

The purpose of this homework is to get a sense of the challenges to cutting 
emissions significantly by analyzing several representative emissions-reduction
policies.
These policy analyses will follow the methods Roger Pielke used in 
Chapters 3--4 of _The Climate Fix_.

I encourage you to work with a partner on this lab, but you should write up
your own lab report individually.

## Data Resources

To make things simple for you, I have prepared an interactive web application, 
available at <https://ees3310.jgilligan.org/decarbonization/}, with almost all 
the data you will need for this project. 
It contains historical data on population, GDP, energy consumption, and 
CO~2~ emissions for many countries and regions of the world. 

I have also provided an R package that you can install on your own computer 
through R Studio:
```r
library(pacman)
p_load_gh(""jonathan-g/kayadata"")
```

Finally, there is an experimental version of the interactive web application
that you can install and run on your computer using RStudio, but it is still
experimental and may not work perfectly.
You can install it in RStudio like this:
```r
library(pacman)
p_load_gh(""jonathan-g/kayadata"")
```
and then you can run the application like this:
```r
library(kayatool)
launch_kayatool()
```
**Note:** you should not put `launch_kaya_tool()` in RMarkdown documents,
like your lab report, because launching an interactive web application 
when you knit your report will prevent the report from knitting correctly.

### Using the interactive web application:

To use the decarbonization web app, start by selecting a country on the 
left-hand control panel. Then you can set the parameters for your policy goals: 
The target year for accomplishing the emissions reductions, the reductions you 
hope to achieve for the country, and the reference year. 

For instance, if your 
goal is for emissions in `r target_year` to be 
`r round(100 * target_reduction)`% less than they were in 
`r ref_year`, you would 
put 
`r target_year` for the target year, 
`r round(100 * target_reduction)`% for the emissions reduction, 
and `r ref_year` for the reference year.  
If you want to indicate a growth in emissions, rather than a reduction, 
just enter a negative number for the emissions reduction.

You can also select what year to use for starting the calculation of bottom-up 
trends in the Kaya-identity parameters 
population _P_, 
per-capita gross-domestic product _g_, 
energy intensity of the economy _e_, 
and carbon intensity of the energy supply _f_. 
When you calculate decarbonization rates in this homework project, you will be 
focusing on the carbon intensity of the economy, which is given by the 
product _ef_.

After you have set the parameters you want, the bottom of the left panel will 
show a ""Bottom-up Analysis"" table that shows the average percentage growth 
rates for the Kaya parameters, their actual values in 2017, and the bottom-up 
projections for what their values will be in the target year (2050 by default).

The tabs on the right-hand side of the web page show:

* ""**Trends:**"" shows historical trends and the calculated growth rate for the 
  Kaya parameters. You select a variable (_P_, _g_, _e_, _f_, or various 
  multiples _ef_, $G = Pg$, $E = Pge$, or $F = Pgef$) The app shows two graphs: 
  on the right, the value of the parameter and on the left, 
  the natural logarithm of the parameter, which we use to calculate 
  percentage growth rates. 
  The graphs show the points that are used in calculating the trends in 
  darker red and the points not used in the trend calculation in lighter red. 
  If you change the starting year on the left-hand panel, you will see the 
  colors of the dots change to reflect this.

    The trend is shown in black on the left-hand graph. If the quantity is 
    changing at a steady rate, the data points will follow a straight line 
    (the trend line). Sometimes you will see that the variables _e_ and _f_ do 
    not seem to be changing at a steady rate, but the product _ef_ is. 
    Explore the trends for the different variables and notice which seem to be 
    following a steady growth or reduction and which do not.

    If you hold the mouse pointer over a data point on either graph, a tool-tip 
    will pop up showing the value of that variable in that year.

* ""**Calculations**"" shows the steps for you to follow for each country in this 
  homework exercise.

* ""**Implied Decarbonization**"" shows the historical trend in the carbon 
  intensity of the economy (_ef_) and the implied future changes in order to 
  meet the policy goal that you set.

* ""**Energy Mix**"" shows the mixture of energy sources (coal, natural gas, oil, 
  nuclear, and renewables) that provide the country or region's energy supply. 
  From this page, you can download the energy mix for the country you're 
  looking at as a text file, using comma-separated value (csv) format, which 
  you can read into R, Excel, or any other common data anlysis program.

* ""**Historical**"" shows a table of historical values for the different Kaya 
  parameters. This is a convenient place to look up the exact numbers for your 
  country in a particular year.
  This sheet also has a download button that lets you download the data in a 
  `.csv` file.

# Background and Context

The basic framework for your analysis will be the Kaya identity:
$$
F = P \times g \times e \times f,
$$
where 
_F_ is the CO~2~ emissions (in million metric tons of carbon per year), 
_P_ is the population (in billion people), 
_g_ is the per-capita GDP (in thousands of dollars per person per year), 
_e_ is the energy intensity of the economy (in quads per trillion dollars of GDP), 
and _f_ is the carbon intensity of the energy supply (in million metric tons of 
carbon dioxide per quad).^[One metric ton = 1000 kg = 1.1 English tons = 2200 pounds] 
A quad means one quadrillion British thermal units (BTU) of energy. 
One quad is approximately equal to 8 billion gallons of gasoline or 
36 million tons of coal. 
It is roughly equal to the electricity used by 
26 million homes in a year, 
or the amount of electricity generated by 
15 nuclear power plants in a year.

We will also focus on the carbon intensity of the economy (in metric tons of 
CO~2~ emissions per million dollars of GDP), which 
equals $e \times f$.^[Note that _e_ is in units of quads per trillion dollars 
of GDP and _f_ is in units of million metric tons of CO~2~ per quad, 
so if you multiply the units you get million metric tons of CO~2~ per 
trillion dollars of GDP, which equals metric tons of CO~2~ per 
million dollars of GDP.]

## Growth Rates and Trends

We will assume that all of the rates of change in the growth and 
decarbonization trends we are studying will be constant from year to year.
A constant percentage rate of change implies that the quantity follows
an exponential growth function, 
so if you know the values for _P_, _g_, _e_, and _f_ in `r current_year`, 
then at some future year _y_:

```{r lefteqn_for_mathjax, echo=FALSE, eval=TRUE, results=""asis"", message=FALSE, warning=FALSE}
# Insert missing definition of \lefteqn for MathJax if the output is
# a variant on HTML.
message(""Output format = "", getOutputFormat())
if(is_output_html()) {
  message(""Defining lefteqn"")
  cat(""$\\def\\lefteqn#1{\\rlap{\\displaystyle{#1}}}$\n"")
} else {
  message(""Not defining lefteqn"")
  cat("""")
}
```
$$
\begin{aligned}
  P(y) &= P(`r current_year`) \times \exp(r_P (y - `r current_year`)),\\
  g(y) &= g(`r current_year`) \times \exp(r_g (y - `r current_year`)),\\
  e(y) &= e(`r current_year`) \times \exp(r_e (y - `r current_year`)),\\
  \lefteqn{\text{and}}\\
  f(y) &= f(`r current_year`) \times \exp(r_f (y - `r current_year`)),\\
\end{aligned}
$$
where _r~P~_ is the growth rate of the population, _r~g~_ is the growth rate of the per-capita GDP, etc.
Increasing energy efficiency and/or decarbonization of the energy supply mean that _r~e~_ and/or _r~f~_ are negative.

------

**Remember that you have to divide percentages by 100 to get the rates for 
these equations: if _r_ is 3%, you use 0.03, not 3.0 in the equations.**

**In your math classes and on your calculator, you have probably seen the 
exponential function exp(_x_) written as _e^x^_, where _e_ is the base of the 
natural logarithm (_2.718..._). 
But since I am using the letter _e_ to represent the energy intensity of the 
economy 
(the energy consumption divided by the GDP), 
I am writing it as exp(_x_) so you won't get confused by two different 
meanings of ""_e_."" Also, in R the exponential function is `exp()`.**

------

Because of the properties of the exponential function, when you multiply two or 
more quantities together, the rate of change of the product is the sum of the 
rates of change of each of the quantities:
$$
\begin{aligned}
  \mathrm{GDP}(y) &= P(y)\times g(y)\\
    &= P(`r current_year`) \times \exp(r_p (y - `r current_year`)) 
    \times g(`r current_year`) \times \exp(r_g (y - `r current_year`))\\
    &= P(`r current_year`)\times g(`r current_year`)
    \times \exp((r_P + r_g) (y - `r current_year`))\\
    \lefteqn{\text{so}}\\
  r_{\mathrm{GDP}} &= r_{P\times g} = r_P + r_g.
\end{aligned}
$$

The web app does these calculations so you can check your results.
So that errors in the first parts of a problem don't cascade through the whole 
exercise, you should work the problems exercises with RMarkdown and compare 
your work to the ""Bottom-up Analysis"" table to make sure you know how to do it.


# The Assignment

For this assignment, analyze the economy and carbon emissions from the whole 
world, and then for individual countries.

## Decarbonization Lab #1, due `r long_due_date_1`

For this lab, you will do a bottom-up analysis of the following countries/regions:

```{r country_table}
bottom_up_country_assignments %>% group_by(class) %>%
  summarize(regions = str_c(region, collapse = "", "")) %>%
  kable(align = ""ll"")
```

For the bottom-up analysis, use the Kaya Identity to make reasonable 
extrapolations of the population and per-capita GDP through `r target_year`. 

Repeat the steps below for each country or region:

### Outline:

To analyze the policy for each country:

1. Get the Kaya identity data for the country
2. Figure out appropriate starting years for calculating the historical trends 
   for the Kaya variables _P_, _g_, _e_, and _f_.
3. Calculate the _historical trends_ for the Kaya variables from the starting
   year you determined in step (2).
4. Use the _historical trends_ to extrapolate projected values for _P_, _g_, 
   _e_, and _f_ in `r target_year`.
5. Calculate the policy goal for emissions _F_ in `r target_year`.
   This uses the policy criteria (target emissions reduction) and the 
   measured emissions _F_ in `r ref_year`, from the Kaya data for your
   country.
6. Calculate the _implied rate of change_ of _F_ between `r current_year`
   and `r target_year`, in order to reduce emissions to the policy goal that
   you calculated in step (5).
7. Combine the _implied rate of change_ of _F_ with the _historical trends_ of
   _P_ and _g_ to calculate the _implied rate of change_ of _ef_ that you 
   calculated in step (3) in order to meet the policy goal from step (5).
8. Compare the _implied rate of change_ of _ef_ that you calculated in step (7)
   to the _historical trend_ of _ef_ that you can determine from the
   _historical trends_ of _e_ and _f_ that you calculated in step (3).

### Detailed steps:

Each step has two alternative methods: using the interactive web application or 
using the `kayadata` library in RStudio:

1. Open the web app at <https://ees3310.jgilligan.org/decarbonization>, 
   select the country you want to analyze, to start, leave the 
   ""Calculate trends starting in"" at its default value (1980), 
   and write down the most current (`r current_year`) values for 
   _P_, _g_, _e_, _f_, _ef_, and _F_. 
   
     Alternately, use the `kayadata` package in RStudio to load the data for
     your country or region. Below is an example of looking up the data
     for the OECD:
     ```{r get_oecd_data, echo=TRUE, eval=TRUE, include=TRUE, warning=FALSE, message=FALSE}
     library(kayadata)
     oecd_data = get_kaya_data(""OECD"")
     oecd_latest_year = oecd_data %>% filter(year == 2017)
     ```
     You can get a list of all the countries and regions that are available 
     from `kaya_region_list()`.
   
     **Start with the whole world first and do the whole analysis for the 
     world before doing it for the individual countries.**

2. Next, go to the ""Trends"" tab and look at the graphs of 
   ln(_P_), ln(_g_), ln(_e_), ln(_f_), and ln(_ef_).
   
    * Write down the rate of change for each variable.
    * For each graph compare the real data (in red) to the trend line 
      (the straight blue line).
    * Does the trend line look a like a good description of the data?
    * Is there a better starting year for calculating trends? 
      If so, adjust ``Calculate trends starting in'' to this year
    * Do you anticipate a problem if we make policy by assuming that 
      the Kaya identity variables will follow the trend line for the next 
      several decades?

     You should also plot these in your report using RMarkdown. 
     Following from the example above, you can use the `plot_kaya` function:
     ```{r plot_oecd_data, echo=TRUE, eval=TRUE, include=TRUE, warning=FALSE, message=FALSE}
     plot_kaya(oecd_data, ""e"", log_scale = TRUE, font_size = 12)
     ```
     
     Be sure to set `log_scale = TRUE` in the `plot_kaya` function because 
     a constant percentage rate of
     change corresponds to a linear trend in the logarithm of the variable.

3. Next, calculate the rates of change of 
   _P_, _g_, _e_, and _f_ (the Population, per-capita GDP, energy intensity of 
   the economy, and carbon-intensity of the energy supply) from your starting 
   year through `r current_year`, using the `lm` function in R.

    A constant rate of change is represented by a linear relationship between
    the natural logarithm of the kaya variable and time: for the variable
    `P` (population), we would write this formula in R as `log(P)~year`.
    
    Here is an example of calculating the rate of change of _e_ 
    (the energy intensity of the economy) for the OECD, using the variable 
    `oecd_data` that you calculated above:
    ```{r get_oecd_e_trend, echo=TRUE, include=TRUE, eval=TRUE, warning=FALSE, message=FALSE}
    # Load the broom library for organizing lm results
    library(broom)
    # Load the magrittr library with helper functions for piping data
    library(magrittr)
    
    e_trend = oecd_data %>% filter(year >= 1980) %>%
      lm(log(e) ~ year, data = .) %>%
      tidy() %>% filter(term == ""year"") %$% estimate
    ```

    For more detailed explanation of the code above, see the handout
    ""New Tools for Data Analysis.""
    
    Here, we find that `e_trend` = `r round(e_trend, 4)` 
    (`r round(100 * e_trend, 2)`% per year).
    
    You can check your results against the interactive web application by
    looking at the rates of change reported on the ""Trends"" tab.
    Be sure to set the start year on the web app to the same values that you
    used for calculating the slopes in RMarkdown.

    These numbers are the slopes of the trend lines that you looked at in part 2.

4. Using the rates of change that you determined in part 3, 
   use the formulas from the ""Growth Rates and Trends"" section to compute the 
   values for 
   _P_, _g_, _e_, and _f_ 
   in the year `r target_year`.
   
    Next, use the growth rates of _P_, _g_, _e_, and _f_ to calculate the 
    growth rate of the total emissions _F_. 
    Calculate the total CO~2~ emissions (_F_) from the country in 
    `r target_year`, assuming that emissions continue to grow at 
    historical rates. 
    
    I recommend that you write R chunks in your report in a way that you
    can copy and paste the chunks from one country or region into the analysis
    for the other countries or regions.
    
    It may also be useful to define functions for frequently used 
    (e.g., see the example `growth` function in the handout on ""New Tools for
    Data Analysis"")

    Check your work against the bottom-up numbers in the ``Bottom-Up Analysis'' 
    table on the bottom of the left-hand pane of the web application.

5. Calculate the emissions target for each country: Set the reference year for 
   emissions reduction to `r ref_year`, and set the target emissions
   reduction using the table below:
   
    The IPCC developed many representative concentration pathways (RCPs) 
    using a top-down approach, for hitting various targets of radiative forcing 
    from greenhouse gases. The only RCP that has at least a two-thirds 
    probability of keeping warming below 2 degrees Celsius is RCP~2.6. 
    This concentration pathway calls for emissions reductions (relative to 2005) 
    for different parts of the world listed in the table below:

    ```{r rcp_26_table}
    rcp_26 %>% 
      mutate(reduction = map_chr(reduction, ~sprintf(""%.0f%%"", 100 * .x))) %>%
      select(Region = region, ""Emissions reduction"" = reduction) %>%
      kable(caption = ""Percent reduction in CO~2~ emissions in 2050, relative to 2005."",
            align = ""lr"")
    ```

    Note that Southeast Asia has a negative reduction. 
    This means that countries in this region are allowed a 17% _increase_
    in CO~2~ emissions (_F_).
    
    Set the target year in the web app to `r target_year`;
    set the reference year to `r ref_year`;
    set the emissions reduction to the emissions reduction you are trying to 
    achieve.

    For each country, how much CO~2~ (_F_) would each country emit in
    `r target_year` in order to meet your policy goal? 
    (Remember to work this whole exercise for whole world before starting on 
    the individual countries.)
    
    Let's work an example using the Middle East:
    ```{r middle_east_example, echo = TRUE, eval=TRUE, include=TRUE, warning=FALSE, message=FALSE}
    F_2005_middle_east = get_kaya_data(""Middle East"") %>% 
      filter(year == 2005) %$% F
    F_2005_middle_east
    ```
    ```{r middle_east_example_2, echo = TRUE, eval=TRUE, include=TRUE, warning=FALSE, message=FALSE}
    middle_east_reduction = 0.32
    F_goal_middle_east = F_2005_middle_east * (1 - middle_east_reduction)
    F_goal_middle_east
    ```
    
    Check this result against the interactive web application.

6. Look up what the CO~2~ emission is in `r current_year` and calculate the 
   rate of change in _F_ that would be necessary to achieve your policy target. 
   For the `r target_year`\ calculation:
   
    a. Calculate the ratio of $F_{`r target_year`}/F_{`r current_year`}$.
    b. Take the natural logarithm of this ratio 
       (in R, the natural logarithm function is `log()`; on your calculator
       it is ""LN"").
    c. Divide the logarithm by the number of years 
       ($`r target_year` - `r current_year`$). 
       This is the rate of change of _F_. A positive number means growth and a 
       negative number means a reduction.
    The percentage rate of change per year is 100 times this number.
    
    For our Middle East example:
    ```{r middle_east_example_3, echo = TRUE, eval=TRUE, include=TRUE, warning=FALSE, message=FALSE}
    F_2017_middle_east = get_kaya_data(""Middle East"") %>% 
      filter(year == 2017) %$% F
    r_F_middle_east = log(F_goal_middle_east / F_2017_middle_east) / 
      (2050 - 2017)
    r_F_middle_east
    ```
    so total emissions for the Middle East would need to drop by 
    `r round(100 * r_F_middle_east, 2)`% per year between `r current_year`
    and `r target_year`

7. Now calculate the decarbonization rate implied by the policy goal. This is 
   the rate of reduction of _ef_, the carbon intensity of the economy. 
   $F = Pgef$, so $r_F = r_P + r_g + r_e + r_f$. Subtract the projected _r~P~_ 
   and _r~g~_ 
   (look them up in the ``Bottom up Analysis'' table) 
   from _r~F~_, which you just calculated in step~7, 
   to get the rate of change of _ef_. 
   Multiply the rate of change of _ef_ by -1 to get the rate of decarbonization
   (because negative rate of change is a positive rate of decarbonization and
   vice-versa). 
   Multiply by 100 to get the percent implied rate of decarbonization.

8. How does the implied rate of decarbonization for each nation compare to the 
   historical rate of decarbonization (i.e., the trend in _ef_ reported in the 
   ""Bottom up Analysis"" table)? 
   Which nation will have the hardest time meeting this emission goal without 
   damaging its economy?
"
801,DECARB_TOP_DOWN,Instructions for Top-Down Decarbonization Policy Analysis,,instructions,/files/lab_docs/decarb_lab_top_down/decarb_top_down_instructions.pdf,,"```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)

is_output_html <- function() {
  is_html <- TRUE
  header <- rmarkdown:::parse_yaml_front_matter(
    readLines(knitr::current_input())
  )
  if (""output.blogdown::html_page"" %in% names(header)) {
    is_html <-  TRUE
  } else if (""output"" %in% names(header)) {
    if (is.list(header$output)) {
      output <- names(header$output)[1]
    } else {
      output <- header$output[1]
    }
    if (str_detect(output, regex(""(pdf|word)_document"", ignore_case = TRUE))) {
      is_html <- FALSE
    }
  }
  is_html
}


getOutputFormat <- function() {
  output <- rmarkdown:::parse_yaml_front_matter(
    readLines(knitr::current_input())
    )$output
  if (is.list(output)){
    return(names(output)[1])
  } else {
    return(output[1])
  }
}

data_dir = ""_data""
script_dir = ""_scripts""

library(pacman)
p_load(magrittr, tidyverse, lubridate, knitr, broom)
p_load_gh(""jonathan-g/kayadata"")

theme_set(theme_bw(base_size = 15))

if (!dir.exists(data_dir)) dir.create(data_dir)

source(file.path(script_dir, ""format_md.R""), chdir = T)

due_date_1 <- date(""2018-10-30"")
due_date_2 <- date(""2018-11-12"")

long_due_date_1 <- stamp(""Monday, March 1"")(due_date_1)
short_due_date_1 <- stamp(""Mar. 1"")(due_date_1)

long_due_date_2 <- stamp(""Monday, March 1"")(due_date_2)
short_due_date_2 <- stamp(""Mar. 1"")(due_date_2)

start_year <- max(get_kaya_data(""United States"")$year, na.rm = T)
after_start_year <- start_year + 1

eia_report_year <- max(get_top_down_values(""United States"")$year, na.rm = T)
donut_year <- max(kayadata::get_fuel_mix(""United States"")$year, na.rm = T)

IEO_url <- ""http://www.eia.gov/forecasts/ieo/""
EIA_table_url <- ""http://www.eia.gov/oiaf/aeo/tablebrowser""

top_down_country_assignments <- bind_rows(
  tibble(class = ""Undergraduates"", 
         region = c(""World"", ""United States"", ""China"")),
  tibble(class = ""Grad Students"", 
         region = c(""World"", ""United States"", ""China"",
                     ""India"", ""Brazil""))
)

rcp_26 <- tribble(
  ~region, ~target_year, ~ref_year, ~reduction,
  ""Australia/New Zealand"", 2050, 2005,  0.82,
  ""Canada"",                2050, 2005,  0.72,
  ""China"",                 2050, 2005,  0.78,
  ""India"",                 2050, 2005,  0.73,
  ""Japan"",                 2050, 2005,  0.66,
  ""South Korea"",           2050, 2005,  0.67,
  ""United States"",         2050, 2005,  0.73,
  ""Africa"",                2050, 2005,  0.28,
  ""Latin America"",         2050, 2005,  0.40,
  ""Middle East"",           2050, 2005,  0.32,
  ""Southeast Asia"",        2050, 2005, -0.17,
  ""Western Europe"",        2050, 2005,  0.74,
  ""World"",                 2050, 2005,  0.36
)

target_year <- rcp_26$target_year[1]
ref_year <- rcp_26$ref_year[1]
current_year <- get_fuel_mix(""World"")$year[1]
```
# Introduction

In this lab, you will use a top-down approach to figure out how much new energy 
infrastructure you would need to install for each country or region in order
to meet the emissions-reduction goals for `r target_year`.


You will analyze the same countries and regions you did in the bottom-up lab last 
week:
```{r country_assignments, echo=FALSE}
top_down_country_assignments %>% group_by(class) %>%
  summarize(region = str_c(region, collapse = "", "")) %>%
  ungroup() %>% 
  mutate(class = ordered(class, 
                         levels = c(""Undergraduates"", ""Grad Students""))) %>%
  arrange(class) %>%
  kable()
```

The lab report is due (knitted to PDF or Word document, committed, and pushed
to GitHub) by Monday Nov.\ 12.

# The Assignment:

## Outline:

1. For each country, examine the top-down projections for the Kaya 
   variables for the year `r target_year`. How many quads of energy does the 
   Energy Information Administration preduct that the country will use
   in `r target_year`?
2. Examine the mix of energy sources that the country used in `r current_year`. 
3. If the  country uses the same percentages of each energy source in 
   `r target_year` that it did in `r current_year`, 
   calculate how much energy the country would use from each energy source in
   `r target_year` and how much CO~2~ each energy source would emit.
   Add these up to get the total CO~2~ emissions _F_ in `r target_year` under a
   ""business-as-usual"" scenario.
4. Calculate the policy target for _F_ in the year `r target_year`.
5. Calculate how much the country would have to reduce its CO~2~ emissions  in 
   `r target_year` below business-as-usual to meet its policy goal.
6. Allocate the emissions reduction from step 5 across the different fuels and 
   calculate the amount of fossil fuel energy the country would need to replace 
   with clean energy by `r target_year` to meet its policy goal.
7. Calculate how many clean power plants the country would need to build 
   between now and `r target_year` to produce this clean energy. 
   
    Report this as the total number, the number per year, and  the number per 
    week or per day if appropriate.
  
    Start by doing this calculation for nuclear energy plants, then repeat the 
    calculation for solar, and then for wind.
8. Suppose instead that the country or region supplied its growing energy 
   demand with coal. Calculate how many new coal power plants it would have
   to build between now and `r target_year` to supply the growing energy 
   demand. 
   
    Calculate the number of new coal power plants the country would have to 
    build and how much additional CO~2~ that would put into the atmosphere
    every year.

## Detailed Instructions:

For each country:

1. Use the function `get_top_down_values()` from the 
   `kayadata` package to examine the top-down projections for the Kaya 
   variables for the year `r target_year`. How many quads of energy does the 
   Energy Information Administration preduct that the country will use
   in `r target_year`?
2. Use the `get_fuel_mix()` function to get the mix of energy sources that the
   country used in `r current_year`. 
3. If the  country uses the same percentages of each energy source in 
   `r target_year` that it did in `r current_year`, calculate the number of
   quads and the greenhouse gas emissions from each energy source 
   in `r target_year`, if the total energy consumed in that year
   is the value for _E_ that you looked up in step (1).
   
    First, multiply the total energy demand _E_ in `r target_year` by the 
    percentages in the fuel mix to get the number of quads for that energy
    source. Then multiply the number of quads for each energy source by the 
    emissions factor for that fuel to get the emissions from that energy
    source.
    
    Finally, add up the emissions from all of the fuel sources to get the total
    emissions for `r target_year`, under a ""business-as-usual"" scenario, where
    the mix of energy sources does not change.
    
    **Hint:** You can combine data frames using `join` functions to make this
    easier. Here is an example, which I have worked for Mexico. You can
    do the same thing for other countries or regions:
    ```{r example_fuel_mix}
    E_2050 <- get_top_down_values(""Mexico"") %>% 
      filter(year == 2050) %$% E
    fm <- get_fuel_mix(""Mexico"") %>% select(-region_code, -geography)
    # Calculate the mix of energy sources for 2050
    fm_2050 <- fm %>% mutate(quads = E_2050 * pct)
    kable(fm, digits = 2)
    ```
    
    ```{r example_fuel_mix_2}
    # Get the emission factors
    factors <- emissions_factors()
    kable(factors, digits = 2)
    ```
    
    ```{r example_fuel_mix_3}
    # Combine the emission factors data frame with the 
    # fuel mix data frame:
    fm_2050 <- fm_2050 %>% left_join(factors, by = ""fuel"") %>%
      # This line is to fix a problem with the kaya_data package,
      # because I forgot to put an emissions factor for Hydro power.
      mutate(emission_factor = replace_na(emission_factor, 0))
    kable(fm_2050, digits = 2)
    ```
    
    ```{r example_fuel_mix_4}
    fm_2050 <- fm_2050 %>% mutate(emissions = quads * emission_factor)
    
    # Summarize by adding up the emissions from each fuel source.
    total_emissions_2050 <- fm_2050 %>% 
      summarize(emissions = sum(emissions)) %$% emissions
    ```
    
4. Get the Kaya identity data for the country from `get_kaya_data()` and look
   up the CO~2~ emissions _F_ in `r ref_year`. Then use the policy goal for 
   emissions reduction from the data frame `rcp_26` to calculate the policy 
   target for _F_ in the year `r target_year`. You did this calculation last
   week as part of the bottom-up analysis.
5. Compare the projected ""business as usual"" emissions from step (3) to the 
   policy target from step (4) to figure the total amount of emissions you 
   would need to cut by converting fossil fuel energy sources to clean sources.
6. Use the top-down procedure I presented in class last week to allocate the
   emissions reduction that you calculated in step (5) across the different 
   energy sources for `r target_year`, that you calculated in step (3). 
   
    How many million metric tons of CO~2~ would you cut from coal, natural gas, 
    and oil? How many quads of energy from that fuel would you need to replace
    with new clean energy?
    
    Add up all of the clean energy requirements to calculate the number of quads
    of clean energy that the country would need to add between now and 
    `r target_year` to meet the policy goal.
7. Calculate how many clean generating plants you would need to build between
   now and `r target_year` to produce this clean energy. Report this as the
   total number, the number per year, and  the number per week or per day if
   the number is large.
  
    Start with nuclear, then do the calculation for solar and then for wind.
    
    You can look up the nameplate capacity and the capacity factor for 
    different kinds of power plants (clean and dirty) from the function 
    `generating_capacity()`.  
    Remember that the average number of megawatts a power source supplies over 
    a year is the nameplate capacity times the capacity factor.
    Remember that one quad equals 
    `r format_md(megawatts_per_quad(), comma = TRUE)` megawatts.
8. Suppose instead that the country or region supplied its growing energy 
   demand with coal. Calculate how many new coal power plants it would have
   to build between now and `r target_year` to supply the growing energy 
   demand. 
   
    Calculate the number of new coal power plants the country would have to 
    build and how much additional CO~2~ that would put into the atmosphere
    every year.
"
901,ROLE_PLAY,Preparation for Lab on Regulating Greenhouse Gas Emissions,,preparation,/files/lab_docs/regulation_lab/preparation_for_regulation_lab.pdf,,"```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(tidyverse)
library(lubridate)
# library(xtable)
library(knitr)
library(kableExtra)

game_day = ymd(""2018-11-05"")
long_game_day = format(game_day, ""%A %B %e"")
short_game_day = format(game_day, ""%b.\ %e"")

```
# Introduction

For the laboratory period on `r long_game_day`, we will do a role-playing 
exercise explore the effectiveness of three different approaches to 
regulating pollution, such as greenhouse gas emissions: command-and-control, 
cap-and-trade, and emissions taxes. 
This reading should serve to prepare you for the exercise.

# Zero-Emissions Baseline

The handout ""The economics of regulating greenhouse gas emissions"" uses a 
baseline scenario of business-as-usual emissions and calculates the costs and 
benefits of an alternate policy that cuts emissions below this baseline. 
This is the usual way policy analysts examine regulations, but if we did the 
emissions-trading game this way, it would be more confusing. To simplify things, 
we'll do the emissions-trading game a different way: with a zero-emissions 
baseline that will allow us to calculate costs and benefits of emitting CO~2~. 
In this case, the costs to the players will be the cost of buying permits to 
emit CO~2~ and the benefits will be the profits they earn. The costs to society 
will be the social cost of carbon (the damage caused by global warming) and the 
benefit will be the combination of profit earned by the companies (this benefits 
the companies, but it's also part of the gross domestic product of the nation) 
and money received from the players as taxes or payment for emissions permits.

# The Exercise



## The Players

To keep groups small, I will divide the class in half and each half will perform 
the same role-playing exercise. For the exercise, I will divide the students 
into three groups:

1. One group will play The Environmental Protections Agency (EPA) and will 
   decide how much to reduce pollution, how many permits to issue, or how 
   much to charge as a pollution tax. 
   
    The EPA's motivation is to produce the best net benefit for society by 
    balancing the costs of reducing greenhouse gas emissions against the 
    benefits of limiting global warming.
2. A second group will play Alpha Electricity: a large power company with a 
   varied portfolio of generating plants including coal, natural gas, and 
   nuclear. 
   
     Alpha's motives are purely to produce the greatest profit for its 
     shareholders regardless of the cost to society of greenhouse gas emissions.
3. A third group will play Beta Industries: a large heavy-industrial 
   conglomerate with many large factories producing steel, aluminum, and 
   petrochemicals such as plastics, paints, and pharmaceuticals. 
   
     Beta's motives are purely to produce the greatest profit for its 
     shareholders regardless of the cost to society of greenhouse gas 
     emissions.

For the purposes of this exercise, we will assume that the EPA can accurately 
estimate the damage that would be caused by global warming, and thus, that it 
can also accurately estimate the social benefit of reducing greenhouse gas 
emissions. However, the EPA cannot accurately assess the costs individual 
companies will incur when they reduce their emissions. This means that the 
EPA's estimates of net benefits (benefits minus costs) is limited by its 
uncertainty about the cost of reducing emissions.

Only the EPA will know the social cost of greenhouse gas emissions, only Alpha 
will know Alpha's cost for reducing emissions, and only Beta will know Beta's 
cost for reducing emissions.

Without regulation, Alpha and Beta would each emit 150 million tons of CO~2~ 
per year. The goal of the exercise is for the EPA to reduce pollution to achieve 
the socially optimal balance between the benefits of economic activity 
(jobs, wealth, etc.) and the harms of pollution. The goal of each firm is to 
maximize their profit regardless of the costs or benefits to society or to its 
competitor.

To keep things simple, emissions cuts will be figured in blocks of 
10 million tons, so a firm can cut emissions by zero, 10 million tons, 
20 million tons, ..., up to a maximum of 150 million tons (cutting emissions 
by 150 million tons means the firm reduces its emissions to zero).

\clearpage
## The Game

The game will have six stages:

1. First, the EPA will gather information on the cost of abating pollution. 
   A representative of the EPA can ask four questions about each company's 
   costs (I recommend asking questions about the marginal profit for the 
   $n^{\text{th}}$ million tons of pollution, or how much it would cost to 
   reduce emissions by $x$ million tons.). A representative of each company 
   will answer the question. The representatives may answer strategically, 
   meaning they may exaggerate the costs. The EPA may take this into account 
   in deciding how to use the answers to estimate the true cost of reducing 
   emissions.
2. Second, the EPA will determine three possible courses of action:
    a) A command and control regulation, which mandates a specific emissions 
       reduction. Because the Constitution guarantees equality before the law, 
       this regulation must impose the same emissions cut for each 
       firm.^[The real world is a bit more complex, because the government 
       would be allowed to regulate different industries differently, but would 
       face real obstacles if it tried to impose different emissions cuts on 
       different companies within the same industry. Because this exercise is 
       limited to just two companies, we simplify by forcing the EPA to impose 
       identical emissions cuts on all firms.} Based on what it knows about the 
       costs (to the firms) and benefits (to society) of reducing emissions, the 
       EPA will determine how many million tons of total emissions to cut 
       (to keep things simple, the EPA should make the amount an even multiple 
       of 20 million tons). It will then divide those emissions cuts equally 
       between the two firms.]
    b) A cap-and-trade. Under this program, the total emissions cuts would be 
       the same and the EPA will issue permits to emit CO~2~. Each permit will 
       allow the owner to emit 10 tons of CO~2~. Total CO~2~ emissions are 
       300 million tons minus the emissions cuts the EPA wants to impose. 
       If the EPA wants to cut emissions by 200 million tons, it would issue 
       10 permits 
       ($300~\text{million} - 20 \times 10~\text{million} = 100~\text{million}$; 
       Each permit allows 10 million tons of emissions so 
       $100~\text{million}/10~\text{million} = 10~\text{permits}$). 
       Again, to keep things simple, make the number of permits a multiple of 2. 
       The EPA will give each company an equal number of permits (if the EPA 
       issues 10 permits, it gives 5 to each company).
	  c) An emissions-tax program. Under this program, the EPA determines the tax 
		   a firm must pay for every million tons of CO~2~ it emits. The firms will 
		   then decide on their own how much to cut their emissions.
3. Third, the firms determine how much CO~2~ they will emit under the 
   command-and-control program (this is easy). At the end of this round, the 
   EPA will publish the total benefit to society of the emissions reduction, 
   the firms will calculate their total costs (but keep these secret), and the 
   instructor will announce the total deadweight loss due to inefficiencies in 
   the regulation.
4. Fourth, permits will be distributed. The class will vote whether to auction 
   the permits and distribute them to the highest payer or give equal numbers 
   of permits to each firm and allow the firms to trade permits with each other. 
   After the permits are allocated, we will again announce the total benefits to 
   society and the deadweight loss.
5. Fifth, we will impose an carbon tax, at the price set by the EPA in stage 2. 
   The two firms will be free to cut emissions by as much or as little as they 
   want (between 0 and 150 million tons each). After each firm determines its 
   final emissions, we will once again announce the total benefits to society 
   and the deadweight loss.
6. Sixth and finally, we will reveal the details at each step, including the 
   private information each firm has about its costs. This will let us discuss 
   and analyze the strengths and limitations of each regulatory program.

## Acknowledgements

This exercise was adapted from The Pollution Game, an interactive exercise 
developed by Jay R. Corrigan, Associate Professor of Economics, Kenyon College.  
See J.R. Corrigan, ``The Pollution Game: A Classroom Exercise Demonstrating the 
Relative Effectiveness of Emissions Taxes and Tradable Permits,'' 
_The Journal of Economic Education_ **42**, 70--78 (2011) 
[doi: 10.1080/00220485.2011.536491](https://doi.org/10.1080/00220485.2011.536491)

\clearpage
## Homework

Do the following homework (turn it in on `r long_game_day` at the beginning of 
lab):

1. Table 1 lists the marginal profit a company earns by emitting CO~2~. 
   Fill in the blanks in the table to show the total profits it earns at each 
   amount of CO~2~ emissions.

2. What level of CO~2~ emissions would produce the maximum total profit? 
   How much profit would this be?
   \vspace{2cm}

3. If the company is emitting the amount of CO~2~ that would maximize its 
   profits, and then the Environmental Protection Agency requires the company 
   to reduce its emissions by 5 million tons, what is the total cost for the 
   company to comply with the regulation?
   \vspace{2cm}

4. What is the marginal cost to comply with the regulation? 
   (Be careful and consider, if the company cuts 1 million tons, then a second 
   million tons, and so forth, what did it cost the company to make the 
   fifth million-ton cut?)
   \vspace{2cm}

5. If the EPA imposed a tax of \$30 per ton on CO~2~ emissions, complete the 
   table to indicate the new marginal profit and total profit at each level of 
   emission.
   \vspace{2cm}

6. Under a \$30 per ton tax, what level of CO~2~ emissions would produce the 
   maximum total profit? What would its total profit be? How much less is this 
   than the total profit you reported in question 2?
   \vspace{2cm}

7. Table 2 shows the economic value of the marginal 
   environmental harm caused by each additional million tons of CO~2~ emissions. 
   Fill in the blanks to indicate the total environmental harm.

8. The marginal net economic impact is the marginal profit generated by emitting 
   each million tons of CO~2~ minus the marginal environmental harm. This number 
   is the net benefit to society from emitting an additional million tons of 
   CO~2~. If the number is positive, society benefits. If it is negative, society 
   suffers.

    Fill in the marginal and total economic impacts, using the information on 
    environmental harm from Table 2 and the information 
    on marginal profits from Table 1.

9. What is the optimum amount of CO~2~ to emit if we consider the net benefit to 
   society (i.e., the net economic impact).
   \vspace{2cm}

10. If you were going to set a cap on emissions, how many tons would you set the 
    cap at?
   \vspace{2cm}

11. If you were going to set a tax on emissions, how many dollars per ton would 
    you set the tax at?
   \vspace{2cm}

### Table 1: Profits vs. Emissions

```{r profit_table}
tax = 30
profits = tibble(
  emissions = seq(1,20), 
  marginal_profit = 91 - 6 * emissions,
  total_profit = str_c(""$"", marginal_profit, 
                       ifelse(is.na(lag(marginal_profit)), """", 
                              str_c("" + $"", lag(cumsum(marginal_profit)), 
                                    "" = $"", cumsum(marginal_profit))
                              )
                       ),
  marginal_taxed = str_c(""$"", marginal_profit, "" - $"", tax, "" = $"",
                         marginal_profit - tax),
  total_taxed = str_c(""$"", marginal_profit - 30,
                       ifelse(is.na(lag(marginal_profit)), """", 
                              str_c("" + $"", lag(cumsum(marginal_profit - 30)), 
                                    "" = $"", cumsum(marginal_profit - 30))
                              )
                      ),
) %>% mutate_at(vars(total_profit:total_taxed),
                funs(ifelse(emissions <= 3, ., """")))

column_names <- c(""CO~2~ Emissions\n(million tons)"",
                    ""Marginal profit\n(million dollars)"", 
                    ""Total profit\n(million dollars)"",
                    ""Marginal profit\n(million dollars)"", 
                    ""Total profit\n(million dollars)"")

output_format <- ifelse(is_html_output(), ""html"", ""latex"")
# message(""Output format = "", output_format)

if (output_format == ""latex"") {
   escape = FALSE
   column_names <- linebreak(column_names) %>%
      str_replace_all(""CO~2~"", ""$\\\\text{CO}_2$"")
   profits <- profits %>% mutate_all(funs(str_replace_all(., fixed(""$""), ""\\$"")))
} else {
   lb <- function(x) x
   escape = TRUE
}

kable(profits, format = output_format, align = ""crrrr"", digits = 0, 
      col.names = column_names,
      booktabs = T, escape = FALSE) %>% 
  add_header_above(c("" "" = 1, ""No Emissions Tax"" = 2, 
                     ""$30/ton Emissions Tax"" = 2)) %>%
  kable_styling(c(""bordered"",""hover""), c(""striped"", ""scale_down""))
```

**Table 1:** Profits versus emissions: The table lists the marginal profit, in 
millions of dollars, for each million tons of CO~2~ it emits. For instance, if 
it emits one million tons, it earns a marginal profit of $85 million. 
The total profit for _x_ million tons is the sum of the marginal profits for 
each million tons from 1 to _x_.

Fill in the blanks with the total profit the company earns by for each amount 
of CO~2~ emission.""

### Table 2: Environmental Harm vs. Emissions

```{r cost_table}
costs = tibble(
  emissions = seq(1,20), 
  marginal_profit = 91 - 6 * emissions,
  marginal_harm = 5 * emissions,
  impact = marginal_profit - marginal_harm,
  total_harm = str_c(""$"", marginal_harm, 
                       ifelse(is.na(lag(marginal_harm)), """", 
                              str_c("" + $"", lag(cumsum(marginal_harm)), 
                                    "" = $"", cumsum(marginal_harm))
                              )
                       ),
  marginal_impact = str_c(""$"", marginal_profit, "" - $"", marginal_harm, 
                          "" = $"", impact),
  total_impact = str_c(""$"", cumsum(impact))
) %>% mutate_at(vars(total_harm:total_impact),
                funs(ifelse(emissions <= 3, ., """"))) %>%
  select(-marginal_profit, -impact)

column_names <- c(""CO~2~ Emissions\n(million tons)"",
                    ""Marginal harm\n(million dollars)"", 
                    ""Total harm\n(million dollars)"",
                    ""Marginal impact\n(million dollars)"", 
                    ""Total impact\n(million dollars)"")

output_format <- ifelse(is_html_output(), ""html"", ""latex"")
# message(""Output format = "", output_format)

if (output_format == ""latex"") {
   column_names <- linebreak(column_names) %>%
      str_replace_all(""CO~2~"", ""$\\\\text{CO}_2$"")
   escape = FALSE
   costs <- costs %>% mutate_all(funs(str_replace_all(., fixed(""$""), ""\\$"")))
} else {
   escape = TRUE
}


kable(costs, format = output_format, align = ""crrrr"", digits = 0,
      col.names = column_names, booktabs = T, escape = escape) %>% 
  add_header_above(c("" "" = 1, ""Environmental Harm"" = 2, 
                     ""Net Economic Impact"" = 2)) %>%
  kable_styling(c(""bordered"",""hover""), c(""striped"", ""scale_down""))
```

**Table 2**: Economic impacts of CO~2~ emissions. The table lists the marginal 
environmental harm, in millions of dollars, for each million tons of CO~2~ 
released into the environment. For instance, the first million tons of 
emissions cause $5 million in damage.

The right pair of columns shows the net economic impact of the profits 
generated from the emissions (see Table 1) minus the environmental harm.

Fill in the blanks with the total environmental harm from CO~2~ emission and 
compute the net economic impact.
"
1201,DECARB_PROJECT,Project: Decarbonization Policy for One Country,,assignment,/files/lab_docs/decarb_project/country_decarbonization_assignment.pdf,decarbonization.bib,"```{r setup, include=FALSE, cache = FALSE}
knitr::knit_hooks$set(inline = function(x) { knitr:::format_sci(x, 'md')})
knitr::opts_chunk$set(cache = FALSE, echo = TRUE, fig.height=4, fig.width=7)

# This section loads necessary R libraries and sources scripts that define
# useful functions format_md.
#
data_dir = ""_data""
script_dir = ""_scripts""

if (!dir.exists('data')) dir.create('data')

library(pacman)

# Load all of the following packages.
# Install any missing packages.
p_load(scales, tidyverse, stringr, janitor, knitr)
p_load_gh(""jonathan-g/kayadata"")

# This sets the default style for ggplot
theme_set(theme_bw(base_size = 15))

# Load basic utility scripts
source('_scripts/format_md.R', chdir = T)
```
# Important Revision

The original assignment called for you to analyze **both** the country's 
2030 nationally determined commitment under the Paris agreement **and**
the 2050 goal that you choose. 

I decided that this would be too much work, so **you only need to analyze the
2050 goal** (you **do not** need to analyze the difficulty of meeting the
2030 goal).

# Introduction

This assignment will be more free-form than the lab assignments we have done
earlier in the semester.
For this assignment, you will build on the work you did for the bottom-up and
top-down decarbonization analysis and investigate how decarbonization might
play out in one specific country.
You can work on your own or in a team of two or three for this assignment.
You will choose one country, decide what that country's greenhouse gas
emissions target should be for the year 2050, and then sketch out a plan for
how that country could achieve its goal.

You and your team will give a short (five minutes per person) presentation in
the final lab on Monday, December 3 and turn in a written report (one report
from the whole team is acceptable)
before midnight on Wednesday December 5.
The report is not meant to be an exhaustive research report. I expect a length
of around 5 pages of doublespaced
text (or equivalently, around 3 pages singlespaced)
per team
member, plus appropriate figures, tables, and references.

Unlike previous assignments, you may use RMarkdown, Word, or any other
word-processing mode that you wish when writing your report, but I would like
to you produce PDF output (e.g., if you're writing in Word, save a copy in PDF
format).

# Establishing a Goal

Briefly, look at what your country has pledged to do for emissions reduction
for the 2015 Paris Agreements
(<http://spappssecext.worldbank.org/sites/indc/Pages/INDCHome.aspx> and the
individual Country Briefs at 
<http://spappssecext.worldbank.org/sites/indc/Pages/Content_Brief.aspx>).
Consider what the country is planning to do by 2030 or whichever alternative
target year it specifies in its Nationally Determined Contribution and
then consider how much more the country might reasonably plan to do by
2050 to make a fair contribution to reducing global greenhouse gas emissions
while also taking into account the need for less-developed nations to
grow their economies in order to lift people out of poverty.

For simplicity, I recommend that you focus on carbon dioxide emissions from
burning fossil fuels rather than on all different kinds of greenhouse gases.

There is no right answer, and it would be possible to write a 50-page
paper just on this part, but that is not what I want you to do.
Rather, just present a simple overview of the issues your country faces and how
you would choose a goal.

For instance, you could look back to 1965 (when the data on energy and emissions
in the `kayadata` package begins) and add up the total emissions from your
country and compare it to the total emissions of the world during that period
and then consider how your country's fraction of total emissions compare to your
country's fraction of the total world population or the total world Gross
Domestic Product (GDP).

You may want to refer to the table below, which lists emissions reductions from
2005--2050 for different parts of the world that would meet a global 36%
emissions target in 2050. This table comes from the IPCC's representative
concentration pathway (RCP) database and is based on the pathway called
""RCP 2.6,"""" which gives about a two-thirds probability of keeping global warming
below 2&deg; C.
However, this table was produced from an economic model and does not take
account of political and ethical considerations, such as fairness, so you are
not obliged to choose the same goal that this table lists for your country or
its region.

```{r rcp_26, echo=FALSE}
rcp_26 <- tribble(
  ~region, ~target_year, ~ref_year, ~reduction,
  ""Australia/New Zealand"", 2050, 2005,  0.82,
  ""Canada"",                2050, 2005,  0.72,
  ""China"",                 2050, 2005,  0.78,
  ""India"",                 2050, 2005,  0.73,
  ""Japan"",                 2050, 2005,  0.66,
  ""South Korea"",           2050, 2005,  0.67,
  ""United States"",         2050, 2005,  0.73,
  ""Africa"",                2050, 2005,  0.28,
  ""Latin America"",         2050, 2005,  0.40,
  ""Middle East"",           2050, 2005,  0.32,
  ""Southeast Asia"",        2050, 2005, -0.17,
  ""Western Europe"",        2050, 2005,  0.74,
  ""World"",                 2050, 2005,  0.36
)

rcp_26 %>% mutate(reduction = percent(reduction, 1)) %>% select(-ref_year) %>%
                  kable(digits = 0, align = c(""c"", ""r"", ""r""),
                        col.names = c(""Region"", ""Year"",
                                      str_c(""Reduction from "", rcp_26$ref_year[1])))
```

# Realizing the Goal

Once you have chosen a goal, you should apply the same kinds of methods that we
used in the bottom-up and top-down decarbonization analyses to estimate what
your conuntry's population and per-capita GDP may be ~~in 2030 and 2050~~ **in 2050**, and what
the implications are for energy efficiency and the mixture of fuels that should
supply its energy needs. This is a place to research the country's natural
resources, current energy supply, and opportunities to decarbonize.

For instance, if your country is mountainous with many rivers, you may want to
expand hydroelectricity.
On the other hand, if it is relatively flat, has few rivers, or if all the
major rivers already have dams and generators, then you may want to look at
other sources of energy.
If your country receives a lot of sunshine, you may want to emphasize solar
energy.
Don't forget to think about nuclear energy as well as renewables like
hydroelectricity, wind, and solar.

I am not asking you to give a thorough engineering and economic assessment of
the energy transition, but to give a brief overview of the major opportunities
and obstacles to clean energy in your country and what you think would be the
best strategy for reducing emissions.

# Format

This is a lab report, not a formal research paper, so you do not need to
structure it as a formal paper.
My grading rubric for the written report will be:

* Thoughtful and sensible analysis of goals [30%]
* Thoughtful and sensible analysis of how to realize the goal [30%]
* Good use of the data for your country [20%]
* Organization of the report (do the parts fit together well and tell a clear
  story) [10%]
* Quality of writing [5%]
* Appropriate use of citations and references [5%]

You may choose to turn this in as an RMarkdown document (knitted to PDF, of
course) or a Word document (saved as PDF), or any other format that you have
rendered as PDF.

The key is that you explain clearly how you do your analysis.
You can either use RMarkdown or you can turn in a document written with a
word-processor and refer the reader to R scripts or an RMarkdown document
with ""supporting information"" that contains the details of your calculations
and quantitative analysis.

You will turn this assignment in by accepting the assignment on
GitHub Classroom, cloning the repository to your own computer, saving your
final products in the repository and committing and pushing them to GitHub.

# Data Sources
The `kayadata` package has data on the Kaya identity variables for 81 countries
and top-down projections for 78 countries.

If you are working on a country that's not one of these, you may want to look
at additional data, such as the International Energy Agency's energy and
emissions analyses.
These reports are extremely expensive to buy, but the Vanderbilt Library 
provides free online access to the World Energy Outlook reports from 1999--2017 
through the ACORN library catalog and this link
<https://www.oecd-ilibrary.org/energy/world-energy-outlook-2017_weo-2017-en>
if you are on campus or this link if you are off-campus
<https://www-oecd-ilibrary-org.proxy.library.vanderbilt.edu/energy/world-energy-outlook-2017_weo-2017-en>.
You can also get detailed data for energy in the the OECD nations at
<https://www.oecd-ilibrary.org/energy/energy/indicator-group/english_379b6cdc-en>)
or 
<https://www-oecd-ilibrary-org.proxy.library.vanderbilt.edu/energy/energy/indicator-group/english_379b6cdc-en>).

The U.S. Energy Information Administration
publishes detailed country profiles for a number of countries at 
<https://www.eia.gov/outlooks/ieo/>,
with data on many individual countries at
<https://www.eia.gov/beta/international/>
and brief country and regional analysis reports at
<https://www.eia.gov/beta/international/analysis.cfm>

This list of data sources is not exhaustive and you should feel free to explore
other sources of information, but don't feel that you have to spend hours in the
library or the internet chasing down data. It is fine for this assignment to use
""good enough"" data or to substitute estimates if you can't find exact numbers
for what you're looking for.
Just be clear about how you're doing your analysis.
"
